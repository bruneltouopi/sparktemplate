23:54:06.308 [main] WARN  org.apache.spark.util.Utils - Your hostname, fabrice-GL552VW resolves to a loopback address: 127.0.1.1; using 192.168.1.67 instead (on interface wlp2s0)
23:54:06.312 [main] WARN  org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
23:54:06.400 [main] INFO  org.apache.spark.SparkContext - Running Spark version 2.4.0
23:54:06.457 [main] DEBUG o.a.h.m.lib.MutableMetricsFactory - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
23:54:06.464 [main] DEBUG o.a.h.m.lib.MutableMetricsFactory - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
23:54:06.465 [main] DEBUG o.a.h.m.lib.MutableMetricsFactory - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
23:54:06.466 [main] DEBUG o.a.h.m.impl.MetricsSystemImpl - UgiMetrics, User and group related metrics
23:54:06.590 [main] DEBUG o.a.h.s.a.util.KerberosName - Kerberos krb5 configuration not found, setting default realm to empty
23:54:06.594 [main] DEBUG org.apache.hadoop.security.Groups -  Creating new Groups object
23:54:06.596 [main] DEBUG o.a.hadoop.util.NativeCodeLoader - Trying to load the custom-built native-hadoop library...
23:54:06.597 [main] DEBUG o.a.hadoop.util.NativeCodeLoader - Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
23:54:06.597 [main] DEBUG o.a.hadoop.util.NativeCodeLoader - java.library.path=/home/fabrice/IDE/idea-IU-171.4073.35/bin::/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
23:54:06.597 [main] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23:54:06.597 [main] DEBUG o.a.hadoop.util.PerformanceAdvisory - Falling back to shell based
23:54:06.598 [main] DEBUG o.a.h.s.JniBasedUnixGroupsMappingWithFallback - Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
23:54:06.663 [main] DEBUG org.apache.hadoop.util.Shell - Failed to detect a valid hadoop home directory
java.io.IOException: HADOOP_HOME or hadoop.home.dir are not set.
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:325) [hadoop-common-2.6.5.jar:na]
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:350) [hadoop-common-2.6.5.jar:na]
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79) [hadoop-common-2.6.5.jar:na]
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116) [hadoop-common-2.6.5.jar:na]
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93) [hadoop-common-2.6.5.jar:na]
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73) [hadoop-common-2.6.5.jar:na]
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293) [hadoop-common-2.6.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283) [hadoop-common-2.6.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260) [hadoop-common-2.6.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789) [hadoop-common-2.6.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774) [hadoop-common-2.6.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647) [hadoop-common-2.6.5.jar:na]
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2422) [spark-core_2.12-2.4.0.jar:2.4.0]
	at scala.Option.getOrElse(Option.scala:121) ~[scala-library-2.12.7.jar:na]
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2422) [spark-core_2.12-2.4.0.jar:2.4.0]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:293) ~[spark-core_2.12-2.4.0.jar:2.4.0]
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520) ~[spark-core_2.12-2.4.0.jar:2.4.0]
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$5(SparkSession.scala:935) ~[spark-sql_2.12-2.4.0.jar:2.4.0]
	at scala.Option.getOrElse(Option.scala:121) ~[scala-library-2.12.7.jar:na]
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926) ~[spark-sql_2.12-2.4.0.jar:2.4.0]
	at com.spark.examples.gradle.SparkInit.main(SparkInit.java:27) ~[main/:na]
23:54:06.674 [main] DEBUG org.apache.hadoop.util.Shell - setsid exited with exit code 0
23:54:06.696 [main] DEBUG org.apache.hadoop.security.Groups - Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
23:54:06.699 [main] DEBUG o.a.h.security.UserGroupInformation - hadoop login
23:54:06.699 [main] DEBUG o.a.h.security.UserGroupInformation - hadoop login commit
23:54:06.701 [main] DEBUG o.a.h.security.UserGroupInformation - using local user:UnixPrincipal: fabrice
23:54:06.701 [main] DEBUG o.a.h.security.UserGroupInformation - Using user: "UnixPrincipal: fabrice" with name fabrice
23:54:06.701 [main] DEBUG o.a.h.security.UserGroupInformation - User entry: "fabrice"
23:54:06.702 [main] DEBUG o.a.h.security.UserGroupInformation - UGI loginUser:fabrice (auth:SIMPLE)
23:54:06.717 [main] INFO  org.apache.spark.SparkContext - Submitted application: spark session example
23:54:06.790 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: fabrice
23:54:06.791 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: fabrice
23:54:06.791 [main] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
23:54:06.792 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
23:54:06.792 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fabrice); groups with view permissions: Set(); users  with modify permissions: Set(fabrice); groups with modify permissions: Set()
23:54:06.984 [main] DEBUG i.n.u.i.l.InternalLoggerFactory - Using SLF4J as the default logging framework
23:54:06.984 [main] DEBUG i.n.u.i.InternalThreadLocalMap - -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
23:54:06.984 [main] DEBUG i.n.u.i.InternalThreadLocalMap - -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
23:54:06.994 [main] DEBUG i.n.c.MultithreadEventLoopGroup - -Dio.netty.eventLoopThreads: 16
23:54:07.009 [main] DEBUG i.n.util.internal.PlatformDependent0 - -Dio.netty.noUnsafe: false
23:54:07.010 [main] DEBUG i.n.util.internal.PlatformDependent0 - Java version: 8
23:54:07.010 [main] DEBUG i.n.util.internal.PlatformDependent0 - sun.misc.Unsafe.theUnsafe: available
23:54:07.011 [main] DEBUG i.n.util.internal.PlatformDependent0 - sun.misc.Unsafe.copyMemory: available
23:54:07.011 [main] DEBUG i.n.util.internal.PlatformDependent0 - java.nio.Buffer.address: available
23:54:07.011 [main] DEBUG i.n.util.internal.PlatformDependent0 - direct buffer constructor: available
23:54:07.012 [main] DEBUG i.n.util.internal.PlatformDependent0 - java.nio.Bits.unaligned: available, true
23:54:07.012 [main] DEBUG i.n.util.internal.PlatformDependent0 - jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
23:54:07.012 [main] DEBUG i.n.util.internal.PlatformDependent0 - java.nio.DirectByteBuffer.<init>(long, int): available
23:54:07.012 [main] DEBUG i.n.util.internal.PlatformDependent - sun.misc.Unsafe: available
23:54:07.013 [main] DEBUG i.n.util.internal.PlatformDependent - -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
23:54:07.013 [main] DEBUG i.n.util.internal.PlatformDependent - -Dio.netty.bitMode: 64 (sun.arch.data.model)
23:54:07.014 [main] DEBUG i.n.util.internal.PlatformDependent - -Dio.netty.noPreferDirect: false
23:54:07.014 [main] DEBUG i.n.util.internal.PlatformDependent - -Dio.netty.maxDirectMemory: 3687317504 bytes
23:54:07.014 [main] DEBUG i.n.util.internal.PlatformDependent - -Dio.netty.uninitializedArrayAllocationThreshold: -1
23:54:07.014 [main] DEBUG io.netty.util.internal.CleanerJava6 - java.nio.ByteBuffer.cleaner(): available
23:54:07.025 [main] DEBUG io.netty.channel.nio.NioEventLoop - -Dio.netty.noKeySetOptimization: false
23:54:07.025 [main] DEBUG io.netty.channel.nio.NioEventLoop - -Dio.netty.selectorAutoRebuildThreshold: 512
23:54:07.030 [main] DEBUG i.n.util.internal.PlatformDependent - org.jctools-core.MpscChunkedArrayQueue: available
23:54:07.046 [main] DEBUG io.netty.util.ResourceLeakDetector - -Dio.netty.leakDetection.level: simple
23:54:07.046 [main] DEBUG io.netty.util.ResourceLeakDetector - -Dio.netty.leakDetection.targetRecords: 4
23:54:07.047 [main] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.numHeapArenas: 16
23:54:07.048 [main] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.numDirectArenas: 16
23:54:07.048 [main] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.pageSize: 8192
23:54:07.048 [main] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxOrder: 11
23:54:07.048 [main] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.chunkSize: 16777216
23:54:07.048 [main] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.tinyCacheSize: 512
23:54:07.048 [main] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.smallCacheSize: 256
23:54:07.048 [main] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.normalCacheSize: 64
23:54:07.048 [main] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxCachedBufferCapacity: 32768
23:54:07.048 [main] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.cacheTrimInterval: 8192
23:54:07.048 [main] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.useCacheForAllThreads: true
23:54:07.069 [main] DEBUG io.netty.channel.DefaultChannelId - -Dio.netty.processId: 25007 (auto-detected)
23:54:07.071 [main] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv4Stack: false
23:54:07.071 [main] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv6Addresses: false
23:54:07.072 [main] DEBUG io.netty.util.NetUtil - Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%lo)
23:54:07.072 [main] DEBUG io.netty.util.NetUtil - /proc/sys/net/core/somaxconn: 128
23:54:07.073 [main] DEBUG io.netty.channel.DefaultChannelId - -Dio.netty.machineId: d0:57:7b:ff:fe:12:4b:66 (auto-detected)
23:54:07.087 [main] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.allocator.type: pooled
23:54:07.087 [main] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.threadLocalDirectBufferSize: 65536
23:54:07.087 [main] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.maxThreadLocalCharBufferSize: 16384
23:54:07.094 [main] DEBUG o.a.s.network.server.TransportServer - Shuffle server started on port: 40325
23:54:07.097 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 40325.
23:54:07.098 [main] DEBUG org.apache.spark.SparkEnv - Using serializer: class org.apache.spark.serializer.JavaSerializer
23:54:07.121 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
23:54:07.121 [main] DEBUG o.a.s.MapOutputTrackerMasterEndpoint - init
23:54:07.139 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
23:54:07.142 [main] INFO  o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23:54:07.142 [main] INFO  o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
23:54:07.155 [main] INFO  o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-f098309a-64b5-4b84-803b-2d6aa4542783
23:54:07.156 [main] DEBUG o.a.spark.storage.DiskBlockManager - Adding shutdown hook
23:54:07.157 [main] DEBUG o.a.spark.util.ShutdownHookManager - Adding shutdown hook
23:54:07.178 [main] INFO  o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1929.9 MB
23:54:07.191 [main] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
23:54:07.192 [main] DEBUG o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - init
23:54:07.206 [main] DEBUG org.apache.spark.SecurityManager - Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
23:54:07.264 [main] DEBUG org.spark_project.jetty.util.log - Logging to Logger[org.spark_project.jetty.util.log] via org.spark_project.jetty.util.log.Slf4jLog
23:54:07.265 [main] INFO  org.spark_project.jetty.util.log - Logging initialized @2156ms
23:54:07.268 [main] DEBUG org.spark_project.jetty.util.Jetty - 
java.lang.NumberFormatException: For input string: "unknown"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[na:1.8.0_191]
	at java.lang.Long.parseLong(Long.java:589) ~[na:1.8.0_191]
	at java.lang.Long.valueOf(Long.java:803) ~[na:1.8.0_191]
	at org.spark_project.jetty.util.Jetty.formatTimestamp(Jetty.java:89) [spark-core_2.12-2.4.0.jar:2.4.0]
	at org.spark_project.jetty.util.Jetty.<clinit>(Jetty.java:61) [spark-core_2.12-2.4.0.jar:2.4.0]
	at org.spark_project.jetty.server.Server.getVersion(Server.java:159) [spark-core_2.12-2.4.0.jar:2.4.0]
	at org.spark_project.jetty.server.handler.ContextHandler.<clinit>(ContextHandler.java:128) [spark-core_2.12-2.4.0.jar:2.4.0]
	at org.apache.spark.ui.JettyUtils$.createServletHandler(JettyUtils.scala:143) [spark-core_2.12-2.4.0.jar:2.4.0]
	at org.apache.spark.ui.JettyUtils$.createServletHandler(JettyUtils.scala:130) [spark-core_2.12-2.4.0.jar:2.4.0]
	at org.apache.spark.ui.WebUI.attachPage(WebUI.scala:84) [spark-core_2.12-2.4.0.jar:2.4.0]
	at org.apache.spark.ui.WebUI.$anonfun$attachTab$1(WebUI.scala:65) [spark-core_2.12-2.4.0.jar:2.4.0]
	at org.apache.spark.ui.WebUI.$anonfun$attachTab$1$adapted(WebUI.scala:65) [spark-core_2.12-2.4.0.jar:2.4.0]
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58) ~[scala-library-2.12.7.jar:na]
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51) ~[scala-library-2.12.7.jar:na]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) ~[scala-library-2.12.7.jar:na]
	at org.apache.spark.ui.WebUI.attachTab(WebUI.scala:65) [spark-core_2.12-2.4.0.jar:2.4.0]
	at org.apache.spark.ui.SparkUI.initialize(SparkUI.scala:62) ~[spark-core_2.12-2.4.0.jar:2.4.0]
	at org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:80) ~[spark-core_2.12-2.4.0.jar:2.4.0]
	at org.apache.spark.ui.SparkUI$.create(SparkUI.scala:175) ~[spark-core_2.12-2.4.0.jar:2.4.0]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:444) ~[spark-core_2.12-2.4.0.jar:2.4.0]
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520) ~[spark-core_2.12-2.4.0.jar:2.4.0]
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$5(SparkSession.scala:935) ~[spark-sql_2.12-2.4.0.jar:2.4.0]
	at scala.Option.getOrElse(Option.scala:121) ~[scala-library-2.12.7.jar:na]
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926) ~[spark-sql_2.12-2.4.0.jar:2.4.0]
	at com.spark.examples.gradle.SparkInit.main(SparkInit.java:27) ~[main/:na]
23:54:07.272 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@9bd0fa6
23:54:07.276 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@18245eb0{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@7c7d3c46,MANAGED}
23:54:07.279 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@7c7d3c46 added {org.apache.spark.ui.JettyUtils$$anon$3-346a361@c09e0636==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.279 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@7c7d3c46 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-346a361,POJO}
23:54:07.280 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@297ea53a
23:54:07.281 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@acb0951{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@5bf22f18,MANAGED}
23:54:07.281 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@5bf22f18 added {org.apache.spark.ui.JettyUtils$$anon$3-267f474e@5709d133==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.281 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@5bf22f18 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-267f474e,POJO}
23:54:07.282 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@63648ee9
23:54:07.282 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@68d6972f{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@45be7cd5,MANAGED}
23:54:07.282 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@45be7cd5 added {org.apache.spark.ui.JettyUtils$$anon$3-7651218e@581a23d3==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.282 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@45be7cd5 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-7651218e,POJO}
23:54:07.283 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@3185fa6b
23:54:07.283 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@6d366c9b{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@5b58ed3c,MANAGED}
23:54:07.283 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@5b58ed3c added {org.apache.spark.ui.JettyUtils$$anon$3-24faea88@3d418e85==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.283 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@5b58ed3c added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-24faea88,POJO}
23:54:07.291 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@eb6449b
23:54:07.291 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@7c351808{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@180e6ac4,MANAGED}
23:54:07.292 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@180e6ac4 added {org.apache.spark.ui.JettyUtils$$anon$3-42b64ab8@99efdb81==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.292 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@180e6ac4 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-42b64ab8,POJO}
23:54:07.292 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@7e985ce9
23:54:07.292 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@2a39fe6a{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@410ae9a3,MANAGED}
23:54:07.292 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@410ae9a3 added {org.apache.spark.ui.JettyUtils$$anon$3-319988b0@b75d3ff8==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.292 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@410ae9a3 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-319988b0,POJO}
23:54:07.292 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@68759011
23:54:07.292 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@7e242b4d{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@305f031,MANAGED}
23:54:07.293 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@305f031 added {org.apache.spark.ui.JettyUtils$$anon$3-592e843a@24cb7d0d==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.293 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@305f031 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-592e843a,POJO}
23:54:07.293 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@1d1f7216
23:54:07.293 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@423e4cbb{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@6e16b8b5,MANAGED}
23:54:07.293 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@6e16b8b5 added {org.apache.spark.ui.JettyUtils$$anon$3-43b4fe19@ced0ab64==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.293 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@6e16b8b5 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-43b4fe19,POJO}
23:54:07.293 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@1536602f
23:54:07.293 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@4ebea12c{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@2a1edad4,MANAGED}
23:54:07.293 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@2a1edad4 added {org.apache.spark.ui.JettyUtils$$anon$3-6256ac4f@1cfc981d==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.294 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@2a1edad4 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-6256ac4f,POJO}
23:54:07.294 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@44c79f32
23:54:07.294 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@7fcbe147{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@235f4c10,MANAGED}
23:54:07.294 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@235f4c10 added {org.apache.spark.ui.JettyUtils$$anon$3-743cb8e0@edba849a==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.294 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@235f4c10 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-743cb8e0,POJO}
23:54:07.297 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@4c168660
23:54:07.297 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@52b56a3e{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@fd0e5b6,MANAGED}
23:54:07.297 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@fd0e5b6 added {org.apache.spark.ui.JettyUtils$$anon$3-4eed46ee@2b73cb9e==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.297 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@fd0e5b6 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-4eed46ee,POJO}
23:54:07.297 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@36b0fcd5
23:54:07.297 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@4fad94a7{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@475835b1,MANAGED}
23:54:07.297 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@475835b1 added {org.apache.spark.ui.JettyUtils$$anon$3-6326d182@4cc4eb34==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.297 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@475835b1 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-6326d182,POJO}
23:54:07.298 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@716a7124
23:54:07.298 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@77192705{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@226642a5,MANAGED}
23:54:07.298 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@226642a5 added {org.apache.spark.ui.JettyUtils$$anon$3-7e809b79@13735d5d==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.298 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@226642a5 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-7e809b79,POJO}
23:54:07.298 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@5cc126dc
23:54:07.298 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@625e134e{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@72bd06ca,MANAGED}
23:54:07.298 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@72bd06ca added {org.apache.spark.ui.JettyUtils$$anon$3-89c10b7@d409b38a==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.298 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@72bd06ca added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-89c10b7,POJO}
23:54:07.299 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@3d08f3f5
23:54:07.300 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@119f1f2a{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@1a1da881,MANAGED}
23:54:07.300 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@1a1da881 added {org.apache.spark.ui.JettyUtils$$anon$3-5b970f7@790092ca==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.300 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@1a1da881 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-5b970f7,POJO}
23:54:07.300 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@7fd4acee
23:54:07.300 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@165b8a71{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@6175619b,MANAGED}
23:54:07.300 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@6175619b added {org.apache.spark.ui.JettyUtils$$anon$3-2f058b8a@3392f4b8==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.300 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@6175619b added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-2f058b8a,POJO}
23:54:07.302 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@2cf92cc7
23:54:07.302 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@30ea8c23{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@7b139eab,MANAGED}
23:54:07.302 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@7b139eab added {org.apache.spark.ui.JettyUtils$$anon$3-4e76dac@491d00ca==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.302 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@7b139eab added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-4e76dac,POJO}
23:54:07.302 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@611df6e3
23:54:07.302 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@5f2f577{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@6273c5a4,MANAGED}
23:54:07.302 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@6273c5a4 added {org.apache.spark.ui.JettyUtils$$anon$3-5d465e4b@84a2c7b==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.302 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@6273c5a4 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-5d465e4b,POJO}
23:54:07.303 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@41a90fa8
23:54:07.303 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@3d8bbcdc{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@52500920,MANAGED}
23:54:07.303 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@52500920 added {org.apache.spark.ui.JettyUtils$$anon$3-117e0fe5@e69a4940==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.303 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@52500920 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-117e0fe5,POJO}
23:54:07.303 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@18a3962d
23:54:07.303 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@78aea4b9{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@2a65bb85,MANAGED}
23:54:07.303 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@2a65bb85 added {org.apache.spark.ui.JettyUtils$$anon$3-4b85880b@3d6287c9==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.303 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@2a65bb85 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-4b85880b,POJO}
23:54:07.303 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@4215838f
23:54:07.303 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@452ba1db{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@2289aca5,MANAGED}
23:54:07.308 [main] DEBUG o.s.jetty.http.PreEncodedHttpField - HttpField encoders loaded: []
23:54:07.312 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@2289aca5 added {org.spark_project.jetty.servlet.DefaultServlet-6a62689d@70045d03==org.spark_project.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true,AUTO}
23:54:07.313 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@2289aca5 added {[/]=>org.spark_project.jetty.servlet.DefaultServlet-6a62689d,POJO}
23:54:07.313 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@60fa3495
23:54:07.313 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@3e2822{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@79e18e38,MANAGED}
23:54:07.314 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@79e18e38 added {org.apache.spark.ui.JettyUtils$$anon$4-29a60c27@2f0be93f==org.apache.spark.ui.JettyUtils$$anon$4,jsp=null,order=-1,inst=true,AUTO}
23:54:07.314 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@79e18e38 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$4-29a60c27,POJO}
23:54:07.314 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@1ca25c47
23:54:07.314 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@5fcacc0{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@533b266e,MANAGED}
23:54:07.317 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@533b266e added {org.glassfish.jersey.servlet.ServletContainer-5f3b9c57@cadbfd1a==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=-1,inst=false,AUTO}
23:54:07.317 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@533b266e added {[/*]=>org.glassfish.jersey.servlet.ServletContainer-5f3b9c57,POJO}
23:54:07.318 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@94f6bfb
23:54:07.318 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@34645867{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@2484f433,MANAGED}
23:54:07.318 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@2484f433 added {org.apache.spark.ui.JettyUtils$$anon$4-60b71e8f@f43bf45c==org.apache.spark.ui.JettyUtils$$anon$4,jsp=null,order=-1,inst=true,AUTO}
23:54:07.318 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@2484f433 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$4-60b71e8f,POJO}
23:54:07.319 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@7c22d4f
23:54:07.319 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@5f59185e{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@60bdf15d,MANAGED}
23:54:07.319 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@60bdf15d added {org.apache.spark.ui.JettyUtils$$anon$4-47da3952@9c7f08ce==org.apache.spark.ui.JettyUtils$$anon$4,jsp=null,order=-1,inst=true,AUTO}
23:54:07.320 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@60bdf15d added {[/]=>org.apache.spark.ui.JettyUtils$$anon$4-47da3952,POJO}
23:54:07.327 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.Server@4ef27d66 added {SparkUI{STOPPED,8<=0<=200,i=0,q=0},AUTO}
23:54:07.328 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.Server@4ef27d66 added {org.spark_project.jetty.server.handler.ErrorHandler@5c48c0c0,AUTO}
23:54:07.330 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.Server@4ef27d66 added {org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[],MANAGED}
23:54:07.330 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.Server@4ef27d66
23:54:07.331 [main] INFO  o.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
23:54:07.342 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.Server@4ef27d66
23:54:07.342 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting SparkUI{STOPPED,8<=0<=200,i=0,q=0}
23:54:07.352 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2243ms SparkUI{STARTED,8<=8<=200,i=7,q=0}
23:54:07.352 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.ErrorHandler@5c48c0c0
23:54:07.352 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.ErrorHandler@5c48c0c0
23:54:07.352 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2243ms org.spark_project.jetty.server.handler.ErrorHandler@5c48c0c0
23:54:07.352 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[]
23:54:07.352 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[]
23:54:07.352 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2243ms org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[]
23:54:07.352 [main] INFO  o.spark_project.jetty.server.Server - Started @2243ms
23:54:07.353 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2243ms org.spark_project.jetty.server.Server@4ef27d66
23:54:07.359 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - HttpConnectionFactory@11dee337[HTTP/1.1] added {HttpConfiguration@460f76a6{32768/8192,8192/8192,https://:0,[]},POJO}
23:54:07.362 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - ServerConnector@a77614d{null,[]}{0.0.0.0:0} added {org.spark_project.jetty.server.Server@4ef27d66,UNMANAGED}
23:54:07.363 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - ServerConnector@a77614d{null,[]}{0.0.0.0:0} added {SparkUI{STARTED,8<=8<=200,i=8,q=0},UNMANAGED}
23:54:07.363 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - ServerConnector@a77614d{null,[]}{0.0.0.0:0} added {org.spark_project.jetty.util.thread.ScheduledExecutorScheduler@4fd4cae3,AUTO}
23:54:07.363 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - ServerConnector@a77614d{null,[]}{0.0.0.0:0} added {org.spark_project.jetty.io.ArrayByteBufferPool@4a067c25,POJO}
23:54:07.363 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - ServerConnector@a77614d{null,[http/1.1]}{0.0.0.0:0} added {HttpConnectionFactory@11dee337[HTTP/1.1],AUTO}
23:54:07.363 [main] DEBUG o.s.jetty.server.AbstractConnector - ServerConnector@a77614d{HTTP/1.1,[http/1.1]}{0.0.0.0:0} added HttpConnectionFactory@11dee337[HTTP/1.1]
23:54:07.365 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - ServerConnector@a77614d{HTTP/1.1,[http/1.1]}{0.0.0.0:0} added {org.spark_project.jetty.server.ServerConnector$ServerConnectorManager@4cc76301,MANAGED}
23:54:07.365 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting ServerConnector@a77614d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
23:54:07.366 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - ServerConnector@a77614d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040} added {sun.nio.ch.ServerSocketChannelImpl[/0:0:0:0:0:0:0:0:4040],POJO}
23:54:07.366 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.util.thread.ScheduledExecutorScheduler@4fd4cae3
23:54:07.366 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2257ms org.spark_project.jetty.util.thread.ScheduledExecutorScheduler@4fd4cae3
23:54:07.366 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting HttpConnectionFactory@11dee337[HTTP/1.1]
23:54:07.366 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2257ms HttpConnectionFactory@11dee337[HTTP/1.1]
23:54:07.366 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.ServerConnector$ServerConnectorManager@4cc76301
23:54:07.369 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.ServerConnector$ServerConnectorManager@4cc76301 added {org.spark_project.jetty.io.ManagedSelector@3bffddff id=0 keys=-1 selected=-1,AUTO}
23:54:07.369 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.ServerConnector$ServerConnectorManager@4cc76301 added {org.spark_project.jetty.io.ManagedSelector@66971f6b id=1 keys=-1 selected=-1,AUTO}
23:54:07.369 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.ServerConnector$ServerConnectorManager@4cc76301 added {org.spark_project.jetty.io.ManagedSelector@50687efb id=2 keys=-1 selected=-1,AUTO}
23:54:07.369 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.ServerConnector$ServerConnectorManager@4cc76301 added {org.spark_project.jetty.io.ManagedSelector@517bd097 id=3 keys=-1 selected=-1,AUTO}
23:54:07.369 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.io.ManagedSelector@3bffddff id=0 keys=-1 selected=-1
23:54:07.369 [main] DEBUG o.s.j.util.thread.QueuedThreadPool - queue org.spark_project.jetty.io.ManagedSelector@3bffddff id=0 keys=0 selected=0
23:54:07.369 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2260ms org.spark_project.jetty.io.ManagedSelector@3bffddff id=0 keys=0 selected=0
23:54:07.369 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.io.ManagedSelector@66971f6b id=1 keys=-1 selected=-1
23:54:07.369 [SparkUI-34] DEBUG o.s.j.util.thread.QueuedThreadPool - run org.spark_project.jetty.io.ManagedSelector@3bffddff id=0 keys=0 selected=0
23:54:07.369 [main] DEBUG o.s.j.util.thread.QueuedThreadPool - queue org.spark_project.jetty.io.ManagedSelector@66971f6b id=1 keys=0 selected=0
23:54:07.369 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2260ms org.spark_project.jetty.io.ManagedSelector@66971f6b id=1 keys=0 selected=0
23:54:07.370 [SparkUI-34] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Idle/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@3e4a1453 execute
23:54:07.370 [SparkUI-35] DEBUG o.s.j.util.thread.QueuedThreadPool - run org.spark_project.jetty.io.ManagedSelector@66971f6b id=1 keys=0 selected=0
23:54:07.370 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.io.ManagedSelector@50687efb id=2 keys=-1 selected=-1
23:54:07.370 [SparkUI-34] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Prod/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@3e4a1453 produce enter
23:54:07.370 [SparkUI-35] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Idle/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@58693520 execute
23:54:07.370 [SparkUI-34] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Prod/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@3e4a1453 producing
23:54:07.370 [main] DEBUG o.s.j.util.thread.QueuedThreadPool - queue org.spark_project.jetty.io.ManagedSelector@50687efb id=2 keys=0 selected=0
23:54:07.370 [SparkUI-35] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Prod/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@58693520 produce enter
23:54:07.370 [SparkUI-35] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Prod/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@58693520 producing
23:54:07.370 [SparkUI-34] DEBUG o.s.jetty.io.ManagedSelector - Selector loop waiting on select
23:54:07.370 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2261ms org.spark_project.jetty.io.ManagedSelector@50687efb id=2 keys=0 selected=0
23:54:07.370 [SparkUI-35] DEBUG o.s.jetty.io.ManagedSelector - Selector loop waiting on select
23:54:07.370 [SparkUI-36] DEBUG o.s.j.util.thread.QueuedThreadPool - run org.spark_project.jetty.io.ManagedSelector@50687efb id=2 keys=0 selected=0
23:54:07.370 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.io.ManagedSelector@517bd097 id=3 keys=-1 selected=-1
23:54:07.370 [SparkUI-36] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Idle/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@4f6583b6 execute
23:54:07.370 [SparkUI-36] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Prod/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@4f6583b6 produce enter
23:54:07.370 [SparkUI-36] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Prod/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@4f6583b6 producing
23:54:07.370 [main] DEBUG o.s.j.util.thread.QueuedThreadPool - queue org.spark_project.jetty.io.ManagedSelector@517bd097 id=3 keys=0 selected=0
23:54:07.370 [SparkUI-36] DEBUG o.s.jetty.io.ManagedSelector - Selector loop waiting on select
23:54:07.370 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2261ms org.spark_project.jetty.io.ManagedSelector@517bd097 id=3 keys=0 selected=0
23:54:07.370 [SparkUI-37] DEBUG o.s.j.util.thread.QueuedThreadPool - run org.spark_project.jetty.io.ManagedSelector@517bd097 id=3 keys=0 selected=0
23:54:07.370 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2261ms org.spark_project.jetty.server.ServerConnector$ServerConnectorManager@4cc76301
23:54:07.370 [SparkUI-37] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Idle/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@7b8d954b execute
23:54:07.370 [SparkUI-37] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Prod/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@7b8d954b produce enter
23:54:07.370 [SparkUI-37] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Prod/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@7b8d954b producing
23:54:07.370 [SparkUI-37] DEBUG o.s.jetty.io.ManagedSelector - Selector loop waiting on select
23:54:07.371 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - ServerConnector@a77614d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040} added {acceptor-0@5990e6c5,POJO}
23:54:07.371 [main] DEBUG o.s.j.util.thread.QueuedThreadPool - queue acceptor-0@5990e6c5
23:54:07.371 [main] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@a77614d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
23:54:07.371 [SparkUI-38] DEBUG o.s.j.util.thread.QueuedThreadPool - run acceptor-0@5990e6c5
23:54:07.371 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2262ms ServerConnector@a77614d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
23:54:07.372 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
23:54:07.372 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.Server@4ef27d66 added {Spark@a77614d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040},UNMANAGED}
23:54:07.385 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905 mime types IncludeExclude@24f360b2{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@60cf80e7,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@302fec27}
23:54:07.385 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905 added {o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,null,@Spark},MANAGED}
23:54:07.385 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,UNMANAGED}
23:54:07.386 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,null,@Spark}]}]
23:54:07.386 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905
23:54:07.387 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905
23:54:07.387 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,null,@Spark}
23:54:07.387 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,STARTING,@Spark}
23:54:07.387 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@7c7d3c46
23:54:07.388 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-346a361 from default=false
23:54:07.389 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.389 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.389 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.389 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-346a361@c09e0636==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.389 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-346a361=org.apache.spark.ui.JettyUtils$$anon$3-346a361@c09e0636==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.390 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@7c7d3c46
23:54:07.390 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2281ms org.spark_project.jetty.servlet.ServletHandler@7c7d3c46
23:54:07.390 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-346a361@c09e0636==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.392 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2282ms org.apache.spark.ui.JettyUtils$$anon$3-346a361@c09e0636==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.393 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@4efcf8a for org.apache.spark.ui.JettyUtils$$anon$3-346a361
23:54:07.393 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}
23:54:07.393 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2284ms o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}
23:54:07.393 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2284ms org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905
23:54:07.393 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5 mime types IncludeExclude@379ab47b{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@307765b4,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@4a9e6faf}
23:54:07.393 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5 added {o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,null,@Spark},MANAGED}
23:54:07.393 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,UNMANAGED}
23:54:07.393 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,null,@Spark}]}]
23:54:07.394 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.394 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5
23:54:07.394 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5
23:54:07.394 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,null,@Spark}
23:54:07.394 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,STARTING,@Spark}
23:54:07.394 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@5bf22f18
23:54:07.394 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-267f474e from default=false
23:54:07.394 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.394 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.394 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.394 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-267f474e@5709d133==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.394 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-267f474e=org.apache.spark.ui.JettyUtils$$anon$3-267f474e@5709d133==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.394 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@5bf22f18
23:54:07.394 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2285ms org.spark_project.jetty.servlet.ServletHandler@5bf22f18
23:54:07.394 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-267f474e@5709d133==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.394 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2285ms org.apache.spark.ui.JettyUtils$$anon$3-267f474e@5709d133==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.394 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@2c95ac9e for org.apache.spark.ui.JettyUtils$$anon$3-267f474e
23:54:07.394 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}
23:54:07.394 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2285ms o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}
23:54:07.394 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2285ms org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5
23:54:07.395 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b mime types IncludeExclude@459f7aa3{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@7cc586a8,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@7db534f2}
23:54:07.395 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b added {o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,null,@Spark},MANAGED}
23:54:07.395 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,UNMANAGED}
23:54:07.395 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.395 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,null,@Spark}]}]
23:54:07.395 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.396 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b
23:54:07.396 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b
23:54:07.396 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,null,@Spark}
23:54:07.396 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,STARTING,@Spark}
23:54:07.396 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@45be7cd5
23:54:07.396 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-7651218e from default=false
23:54:07.396 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.396 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.396 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.396 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-7651218e@581a23d3==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.396 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-7651218e=org.apache.spark.ui.JettyUtils$$anon$3-7651218e@581a23d3==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.396 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@45be7cd5
23:54:07.396 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2287ms org.spark_project.jetty.servlet.ServletHandler@45be7cd5
23:54:07.396 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-7651218e@581a23d3==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.396 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2287ms org.apache.spark.ui.JettyUtils$$anon$3-7651218e@581a23d3==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.396 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@44a2b17b for org.apache.spark.ui.JettyUtils$$anon$3-7651218e
23:54:07.396 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}
23:54:07.396 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2287ms o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}
23:54:07.396 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2287ms org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b
23:54:07.397 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a mime types IncludeExclude@7eb01b12{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@2f4854d6,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@61d9efe0}
23:54:07.397 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a added {o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,null,@Spark},MANAGED}
23:54:07.397 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,UNMANAGED}
23:54:07.397 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.398 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.398 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,null,@Spark}]}]
23:54:07.398 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.398 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a
23:54:07.398 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a
23:54:07.398 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,null,@Spark}
23:54:07.398 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,STARTING,@Spark}
23:54:07.398 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@5b58ed3c
23:54:07.398 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-24faea88 from default=false
23:54:07.398 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.398 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.398 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.398 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-24faea88@3d418e85==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.398 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-24faea88=org.apache.spark.ui.JettyUtils$$anon$3-24faea88@3d418e85==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.398 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@5b58ed3c
23:54:07.398 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2289ms org.spark_project.jetty.servlet.ServletHandler@5b58ed3c
23:54:07.398 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-24faea88@3d418e85==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.398 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2289ms org.apache.spark.ui.JettyUtils$$anon$3-24faea88@3d418e85==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.398 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@7e70bd39 for org.apache.spark.ui.JettyUtils$$anon$3-24faea88
23:54:07.398 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}
23:54:07.398 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2289ms o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}
23:54:07.398 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2289ms org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a
23:54:07.399 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e mime types IncludeExclude@6de54b40{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@43ed0ff3,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@388ffbc2}
23:54:07.399 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e added {o.s.j.s.ServletContextHandler@7c351808{/stages,null,null,@Spark},MANAGED}
23:54:07.399 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,UNMANAGED}
23:54:07.399 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.399 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.399 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.399 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.399 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,null,@Spark}]}]
23:54:07.399 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e
23:54:07.399 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e
23:54:07.399 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@7c351808{/stages,null,null,@Spark}
23:54:07.399 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@7c351808{/stages,null,STARTING,@Spark}
23:54:07.399 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@180e6ac4
23:54:07.400 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-42b64ab8 from default=false
23:54:07.400 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.400 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.400 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.400 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-42b64ab8@99efdb81==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.400 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-42b64ab8=org.apache.spark.ui.JettyUtils$$anon$3-42b64ab8@99efdb81==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.400 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@180e6ac4
23:54:07.400 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2291ms org.spark_project.jetty.servlet.ServletHandler@180e6ac4
23:54:07.400 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-42b64ab8@99efdb81==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.400 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2291ms org.apache.spark.ui.JettyUtils$$anon$3-42b64ab8@99efdb81==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.400 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@a50b09c for org.apache.spark.ui.JettyUtils$$anon$3-42b64ab8
23:54:07.400 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}
23:54:07.400 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2291ms o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}
23:54:07.400 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2291ms org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e
23:54:07.400 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd mime types IncludeExclude@6691490c{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@2187fff7,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@2e5c7f0b}
23:54:07.401 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd added {o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,null,@Spark},MANAGED}
23:54:07.401 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,UNMANAGED}
23:54:07.401 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.401 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.401 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.401 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,null,@Spark}]}]
23:54:07.401 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.401 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.401 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd
23:54:07.401 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd
23:54:07.401 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,null,@Spark}
23:54:07.402 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,STARTING,@Spark}
23:54:07.402 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@410ae9a3
23:54:07.402 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-319988b0 from default=false
23:54:07.402 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.402 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.402 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.402 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-319988b0@b75d3ff8==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.402 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-319988b0=org.apache.spark.ui.JettyUtils$$anon$3-319988b0@b75d3ff8==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.402 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@410ae9a3
23:54:07.402 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2293ms org.spark_project.jetty.servlet.ServletHandler@410ae9a3
23:54:07.402 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-319988b0@b75d3ff8==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.402 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2293ms org.apache.spark.ui.JettyUtils$$anon$3-319988b0@b75d3ff8==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.402 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@21d5c1a0 for org.apache.spark.ui.JettyUtils$$anon$3-319988b0
23:54:07.402 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}
23:54:07.402 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2293ms o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}
23:54:07.402 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2293ms org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd
23:54:07.403 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf mime types IncludeExclude@538613b3{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@1eef9aef,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@11389053}
23:54:07.403 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf added {o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,null,@Spark},MANAGED}
23:54:07.403 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,UNMANAGED}
23:54:07.403 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.403 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.404 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.404 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.404 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.404 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.404 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,null,@Spark}]}]
23:54:07.404 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf
23:54:07.404 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf
23:54:07.404 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,null,@Spark}
23:54:07.404 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,STARTING,@Spark}
23:54:07.404 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@305f031
23:54:07.404 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-592e843a from default=false
23:54:07.404 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.404 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.404 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.404 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-592e843a@24cb7d0d==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.404 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-592e843a=org.apache.spark.ui.JettyUtils$$anon$3-592e843a@24cb7d0d==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.404 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@305f031
23:54:07.404 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2295ms org.spark_project.jetty.servlet.ServletHandler@305f031
23:54:07.405 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-592e843a@24cb7d0d==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.405 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2295ms org.apache.spark.ui.JettyUtils$$anon$3-592e843a@24cb7d0d==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.405 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@5db99216 for org.apache.spark.ui.JettyUtils$$anon$3-592e843a
23:54:07.405 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}
23:54:07.405 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2296ms o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}
23:54:07.405 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2296ms org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf
23:54:07.406 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94 mime types IncludeExclude@18cc679e{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@2e77b8cf,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@2c4ca0f9}
23:54:07.406 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94 added {o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,null,@Spark},MANAGED}
23:54:07.406 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,UNMANAGED}
23:54:07.406 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.406 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.406 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,null,@Spark}]}]
23:54:07.406 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.407 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.407 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.407 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.407 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.407 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94
23:54:07.407 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94
23:54:07.407 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,null,@Spark}
23:54:07.407 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,STARTING,@Spark}
23:54:07.407 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@6e16b8b5
23:54:07.407 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-43b4fe19 from default=false
23:54:07.407 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.407 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.407 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.407 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-43b4fe19@ced0ab64==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.407 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-43b4fe19=org.apache.spark.ui.JettyUtils$$anon$3-43b4fe19@ced0ab64==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.407 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@6e16b8b5
23:54:07.407 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2298ms org.spark_project.jetty.servlet.ServletHandler@6e16b8b5
23:54:07.408 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-43b4fe19@ced0ab64==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.408 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2298ms org.apache.spark.ui.JettyUtils$$anon$3-43b4fe19@ced0ab64==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.408 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@67ef029 for org.apache.spark.ui.JettyUtils$$anon$3-43b4fe19
23:54:07.408 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}
23:54:07.408 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2299ms o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}
23:54:07.408 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2299ms org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94
23:54:07.408 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef mime types IncludeExclude@6e57e95e{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@2755d705,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@56db847e}
23:54:07.408 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef added {o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,null,@Spark},MANAGED}
23:54:07.408 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,UNMANAGED}
23:54:07.409 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.409 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.409 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.409 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.409 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.409 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.409 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.409 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.409 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,null,@Spark}]}]
23:54:07.409 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef
23:54:07.409 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef
23:54:07.409 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,null,@Spark}
23:54:07.410 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,STARTING,@Spark}
23:54:07.410 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@2a1edad4
23:54:07.410 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-6256ac4f from default=false
23:54:07.410 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.410 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.410 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.410 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-6256ac4f@1cfc981d==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.410 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-6256ac4f=org.apache.spark.ui.JettyUtils$$anon$3-6256ac4f@1cfc981d==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.410 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@2a1edad4
23:54:07.410 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2301ms org.spark_project.jetty.servlet.ServletHandler@2a1edad4
23:54:07.410 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-6256ac4f@1cfc981d==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.410 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2301ms org.apache.spark.ui.JettyUtils$$anon$3-6256ac4f@1cfc981d==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.410 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@740abb5 for org.apache.spark.ui.JettyUtils$$anon$3-6256ac4f
23:54:07.410 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}
23:54:07.410 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2301ms o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}
23:54:07.410 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2301ms org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef
23:54:07.411 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a mime types IncludeExclude@5fe8b721{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@551a20d6,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@578524c3}
23:54:07.411 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a added {o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,null,@Spark},MANAGED}
23:54:07.411 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,UNMANAGED}
23:54:07.411 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.411 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.412 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.412 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.412 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.412 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.412 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.412 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.412 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,null,@Spark}]}]
23:54:07.412 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.412 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a
23:54:07.412 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a
23:54:07.412 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,null,@Spark}
23:54:07.412 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,STARTING,@Spark}
23:54:07.412 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@235f4c10
23:54:07.412 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-743cb8e0 from default=false
23:54:07.412 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.412 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.412 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.413 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-743cb8e0@edba849a==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.413 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-743cb8e0=org.apache.spark.ui.JettyUtils$$anon$3-743cb8e0@edba849a==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.413 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@235f4c10
23:54:07.413 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2304ms org.spark_project.jetty.servlet.ServletHandler@235f4c10
23:54:07.413 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-743cb8e0@edba849a==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.413 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2304ms org.apache.spark.ui.JettyUtils$$anon$3-743cb8e0@edba849a==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.413 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@64c2b546 for org.apache.spark.ui.JettyUtils$$anon$3-743cb8e0
23:54:07.413 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}
23:54:07.413 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2304ms o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}
23:54:07.413 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2304ms org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a
23:54:07.413 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740 mime types IncludeExclude@7a11c4c7{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@4cc547a,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@7555b920}
23:54:07.414 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740 added {o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,null,@Spark},MANAGED}
23:54:07.414 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,UNMANAGED}
23:54:07.414 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.414 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.414 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.414 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.414 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.414 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.414 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.414 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.414 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,null,@Spark}]}]
23:54:07.414 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.415 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.415 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740
23:54:07.415 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740
23:54:07.415 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,null,@Spark}
23:54:07.415 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,STARTING,@Spark}
23:54:07.415 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@fd0e5b6
23:54:07.415 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-4eed46ee from default=false
23:54:07.415 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.415 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.415 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.415 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-4eed46ee@2b73cb9e==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.415 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-4eed46ee=org.apache.spark.ui.JettyUtils$$anon$3-4eed46ee@2b73cb9e==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.415 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@fd0e5b6
23:54:07.415 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2306ms org.spark_project.jetty.servlet.ServletHandler@fd0e5b6
23:54:07.415 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-4eed46ee@2b73cb9e==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.415 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2306ms org.apache.spark.ui.JettyUtils$$anon$3-4eed46ee@2b73cb9e==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.415 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@4152d38d for org.apache.spark.ui.JettyUtils$$anon$3-4eed46ee
23:54:07.415 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}
23:54:07.415 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2306ms o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}
23:54:07.415 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2306ms org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740
23:54:07.416 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c mime types IncludeExclude@5398edd0{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@b5cc23a,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@5cc5b667}
23:54:07.416 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c added {o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,null,@Spark},MANAGED}
23:54:07.416 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,UNMANAGED}
23:54:07.416 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.416 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.416 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.417 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.417 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.417 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.417 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.417 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.417 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,null,@Spark}]}]
23:54:07.417 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.417 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.417 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.417 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c
23:54:07.417 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c
23:54:07.417 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,null,@Spark}
23:54:07.417 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,STARTING,@Spark}
23:54:07.417 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@475835b1
23:54:07.417 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-6326d182 from default=false
23:54:07.417 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.417 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.417 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.417 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-6326d182@4cc4eb34==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.417 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-6326d182=org.apache.spark.ui.JettyUtils$$anon$3-6326d182@4cc4eb34==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.417 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@475835b1
23:54:07.417 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2308ms org.spark_project.jetty.servlet.ServletHandler@475835b1
23:54:07.418 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-6326d182@4cc4eb34==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.418 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2308ms org.apache.spark.ui.JettyUtils$$anon$3-6326d182@4cc4eb34==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.418 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@61edc883 for org.apache.spark.ui.JettyUtils$$anon$3-6326d182
23:54:07.418 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}
23:54:07.418 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2309ms o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}
23:54:07.418 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2309ms org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c
23:54:07.418 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03 mime types IncludeExclude@182f1e9a{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@6928f576,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@660e9100}
23:54:07.418 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03 added {o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,null,@Spark},MANAGED}
23:54:07.418 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,UNMANAGED}
23:54:07.419 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.419 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.419 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,null,@Spark}]}]
23:54:07.419 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.419 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.419 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.419 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.419 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.419 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.419 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.419 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.419 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.419 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.419 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03
23:54:07.419 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03
23:54:07.419 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,null,@Spark}
23:54:07.420 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,STARTING,@Spark}
23:54:07.420 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@226642a5
23:54:07.420 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-7e809b79 from default=false
23:54:07.420 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.420 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.420 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.420 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-7e809b79@13735d5d==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.420 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-7e809b79=org.apache.spark.ui.JettyUtils$$anon$3-7e809b79@13735d5d==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.420 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@226642a5
23:54:07.420 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2311ms org.spark_project.jetty.servlet.ServletHandler@226642a5
23:54:07.420 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-7e809b79@13735d5d==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.420 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2311ms org.apache.spark.ui.JettyUtils$$anon$3-7e809b79@13735d5d==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.420 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@69f63d95 for org.apache.spark.ui.JettyUtils$$anon$3-7e809b79
23:54:07.420 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}
23:54:07.420 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2311ms o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}
23:54:07.420 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2311ms org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03
23:54:07.421 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff mime types IncludeExclude@27e0f2f5{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@3574e198,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@6db66836}
23:54:07.421 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff added {o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,null,@Spark},MANAGED}
23:54:07.421 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,UNMANAGED}
23:54:07.421 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.421 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.421 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.421 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.421 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.421 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.421 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.422 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.422 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.422 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,null,@Spark}]}]
23:54:07.422 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.422 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.422 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.422 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.422 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff
23:54:07.422 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff
23:54:07.422 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,null,@Spark}
23:54:07.422 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,STARTING,@Spark}
23:54:07.422 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@72bd06ca
23:54:07.422 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-89c10b7 from default=false
23:54:07.422 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.422 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.422 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.422 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-89c10b7@d409b38a==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.422 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-89c10b7=org.apache.spark.ui.JettyUtils$$anon$3-89c10b7@d409b38a==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.422 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@72bd06ca
23:54:07.422 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2313ms org.spark_project.jetty.servlet.ServletHandler@72bd06ca
23:54:07.423 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-89c10b7@d409b38a==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.423 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2313ms org.apache.spark.ui.JettyUtils$$anon$3-89c10b7@d409b38a==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.423 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@db44aa2 for org.apache.spark.ui.JettyUtils$$anon$3-89c10b7
23:54:07.423 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}
23:54:07.423 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2314ms o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}
23:54:07.423 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2314ms org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff
23:54:07.423 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb mime types IncludeExclude@3f093abe{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@61a002b1,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@4eeea57d}
23:54:07.423 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb added {o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,null,@Spark},MANAGED}
23:54:07.423 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,UNMANAGED}
23:54:07.424 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.424 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.424 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.424 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.424 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.424 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.424 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.424 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.424 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.424 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
23:54:07.424 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.424 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,[o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,null,@Spark}]}]
23:54:07.424 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.424 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.424 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.424 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb
23:54:07.424 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb
23:54:07.425 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,null,@Spark}
23:54:07.425 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,STARTING,@Spark}
23:54:07.425 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@1a1da881
23:54:07.425 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-5b970f7 from default=false
23:54:07.425 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.425 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.425 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.425 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-5b970f7@790092ca==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.425 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-5b970f7=org.apache.spark.ui.JettyUtils$$anon$3-5b970f7@790092ca==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.425 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@1a1da881
23:54:07.425 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2316ms org.spark_project.jetty.servlet.ServletHandler@1a1da881
23:54:07.425 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-5b970f7@790092ca==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.425 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2316ms org.apache.spark.ui.JettyUtils$$anon$3-5b970f7@790092ca==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.425 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@780ec4a5 for org.apache.spark.ui.JettyUtils$$anon$3-5b970f7
23:54:07.425 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}
23:54:07.425 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2316ms o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}
23:54:07.425 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2316ms org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb
23:54:07.426 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0 mime types IncludeExclude@6f70f32f{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@548e76f1,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@5aabbb29}
23:54:07.426 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0 added {o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,null,@Spark},MANAGED}
23:54:07.426 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0,UNMANAGED}
23:54:07.426 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.426 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.426 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.426 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0,[o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,null,@Spark}]}]
23:54:07.426 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.426 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.426 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.426 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.427 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.427 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.427 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
23:54:07.427 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,[o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}]}]
23:54:07.427 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.427 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.427 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.427 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.427 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0
23:54:07.427 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0
23:54:07.427 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,null,@Spark}
23:54:07.427 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,STARTING,@Spark}
23:54:07.427 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@6175619b
23:54:07.427 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-2f058b8a from default=false
23:54:07.427 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.427 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.427 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.427 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-2f058b8a@3392f4b8==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.427 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-2f058b8a=org.apache.spark.ui.JettyUtils$$anon$3-2f058b8a@3392f4b8==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.427 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@6175619b
23:54:07.427 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2318ms org.spark_project.jetty.servlet.ServletHandler@6175619b
23:54:07.427 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-2f058b8a@3392f4b8==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.428 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2318ms org.apache.spark.ui.JettyUtils$$anon$3-2f058b8a@3392f4b8==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.428 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@72c927f1 for org.apache.spark.ui.JettyUtils$$anon$3-2f058b8a
23:54:07.428 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,AVAILABLE,@Spark}
23:54:07.428 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2318ms o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,AVAILABLE,@Spark}
23:54:07.428 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2319ms org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0
23:54:07.428 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c mime types IncludeExclude@3dd69f5a{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@3aa3193a,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@1ee4730}
23:54:07.428 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c added {o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,null,@Spark},MANAGED}
23:54:07.428 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c,UNMANAGED}
23:54:07.429 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.429 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.429 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.429 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0,[o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,AVAILABLE,@Spark}]}]
23:54:07.429 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.429 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.429 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.429 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.429 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.429 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.429 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
23:54:07.429 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,[o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}]}]
23:54:07.429 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.430 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.430 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c,[o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,null,@Spark}]}]
23:54:07.430 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.430 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.430 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c
23:54:07.430 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c
23:54:07.430 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,null,@Spark}
23:54:07.430 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,STARTING,@Spark}
23:54:07.430 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@7b139eab
23:54:07.430 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-4e76dac from default=false
23:54:07.430 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.430 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.430 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.430 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-4e76dac@491d00ca==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.430 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-4e76dac=org.apache.spark.ui.JettyUtils$$anon$3-4e76dac@491d00ca==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.430 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@7b139eab
23:54:07.430 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2321ms org.spark_project.jetty.servlet.ServletHandler@7b139eab
23:54:07.430 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-4e76dac@491d00ca==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.430 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2321ms org.apache.spark.ui.JettyUtils$$anon$3-4e76dac@491d00ca==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.430 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@59a67c3a for org.apache.spark.ui.JettyUtils$$anon$3-4e76dac
23:54:07.431 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}
23:54:07.431 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2321ms o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}
23:54:07.431 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2321ms org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c
23:54:07.431 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b mime types IncludeExclude@724bade8{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@16fb356,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@6bc248ed}
23:54:07.431 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b added {o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,null,@Spark},MANAGED}
23:54:07.431 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c, org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b,UNMANAGED}
23:54:07.431 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b,[o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,null,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0,[o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,AVAILABLE,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,[o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c,[o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.432 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b
23:54:07.432 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b
23:54:07.432 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,null,@Spark}
23:54:07.432 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,STARTING,@Spark}
23:54:07.433 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@6273c5a4
23:54:07.433 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-5d465e4b from default=false
23:54:07.433 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.433 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.433 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.433 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-5d465e4b@84a2c7b==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.433 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-5d465e4b=org.apache.spark.ui.JettyUtils$$anon$3-5d465e4b@84a2c7b==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.433 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@6273c5a4
23:54:07.433 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2324ms org.spark_project.jetty.servlet.ServletHandler@6273c5a4
23:54:07.433 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-5d465e4b@84a2c7b==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.433 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2324ms org.apache.spark.ui.JettyUtils$$anon$3-5d465e4b@84a2c7b==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.433 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@23a9ba52 for org.apache.spark.ui.JettyUtils$$anon$3-5d465e4b
23:54:07.433 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,AVAILABLE,@Spark}
23:54:07.433 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2324ms o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,AVAILABLE,@Spark}
23:54:07.433 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2324ms org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b
23:54:07.434 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722 mime types IncludeExclude@70ab80e3{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@9573b3b,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@67427b69}
23:54:07.434 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722 added {o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,null,@Spark},MANAGED}
23:54:07.434 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c, org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b, org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722,UNMANAGED}
23:54:07.434 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.434 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b,[o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,AVAILABLE,@Spark}]}]
23:54:07.434 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.434 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.434 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0,[o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,AVAILABLE,@Spark}]}]
23:54:07.434 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.434 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.435 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.435 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.435 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.435 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.435 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
23:54:07.435 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,[o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}]}]
23:54:07.435 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.435 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.435 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c,[o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}]}]
23:54:07.435 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.435 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.435 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722,[o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,null,@Spark}]}]
23:54:07.435 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722
23:54:07.435 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722
23:54:07.435 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,null,@Spark}
23:54:07.435 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,STARTING,@Spark}
23:54:07.435 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@52500920
23:54:07.435 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-117e0fe5 from default=false
23:54:07.435 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.435 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.435 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.435 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-117e0fe5@e69a4940==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.436 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-117e0fe5=org.apache.spark.ui.JettyUtils$$anon$3-117e0fe5@e69a4940==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.436 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@52500920
23:54:07.436 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2326ms org.spark_project.jetty.servlet.ServletHandler@52500920
23:54:07.436 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-117e0fe5@e69a4940==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.436 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2327ms org.apache.spark.ui.JettyUtils$$anon$3-117e0fe5@e69a4940==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.436 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@78461bc4 for org.apache.spark.ui.JettyUtils$$anon$3-117e0fe5
23:54:07.436 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,AVAILABLE,@Spark}
23:54:07.436 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2327ms o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,AVAILABLE,@Spark}
23:54:07.436 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2327ms org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722
23:54:07.436 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7 mime types IncludeExclude@64f857e7{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@1095f122,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@58c540cf}
23:54:07.436 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7 added {o.s.j.s.ServletContextHandler@78aea4b9{/executors/threadDump/json,null,null,@Spark},MANAGED}
23:54:07.437 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c, org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b, org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722, org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7,UNMANAGED}
23:54:07.437 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.437 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b,[o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,AVAILABLE,@Spark}]}]
23:54:07.437 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.437 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.437 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7,[o.s.j.s.ServletContextHandler@78aea4b9{/executors/threadDump/json,null,null,@Spark}]}]
23:54:07.437 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0,[o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,AVAILABLE,@Spark}]}]
23:54:07.437 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.437 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.437 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.437 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.437 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.437 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.437 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
23:54:07.437 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,[o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}]}]
23:54:07.438 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.438 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.438 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c,[o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}]}]
23:54:07.438 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.438 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.438 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722,[o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,AVAILABLE,@Spark}]}]
23:54:07.438 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7
23:54:07.438 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7
23:54:07.438 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@78aea4b9{/executors/threadDump/json,null,null,@Spark}
23:54:07.438 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@78aea4b9{/executors/threadDump/json,null,STARTING,@Spark}
23:54:07.438 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@2a65bb85
23:54:07.438 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-4b85880b from default=false
23:54:07.438 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.438 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.438 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.438 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-4b85880b@3d6287c9==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.438 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-4b85880b=org.apache.spark.ui.JettyUtils$$anon$3-4b85880b@3d6287c9==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.438 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@2a65bb85
23:54:07.438 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2329ms org.spark_project.jetty.servlet.ServletHandler@2a65bb85
23:54:07.438 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-4b85880b@3d6287c9==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.438 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2329ms org.apache.spark.ui.JettyUtils$$anon$3-4b85880b@3d6287c9==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.438 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@3d6300e8 for org.apache.spark.ui.JettyUtils$$anon$3-4b85880b
23:54:07.439 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78aea4b9{/executors/threadDump/json,null,AVAILABLE,@Spark}
23:54:07.439 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2329ms o.s.j.s.ServletContextHandler@78aea4b9{/executors/threadDump/json,null,AVAILABLE,@Spark}
23:54:07.439 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2329ms org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7
23:54:07.439 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc mime types IncludeExclude@24a1c17f{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@56102e1c,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@73511076}
23:54:07.439 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc added {o.s.j.s.ServletContextHandler@452ba1db{/static,null,null,@Spark},MANAGED}
23:54:07.439 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c, org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b, org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722, org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7, org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc,UNMANAGED}
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - static->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc,[o.s.j.s.ServletContextHandler@452ba1db{/static,null,null,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b,[o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7,[o.s.j.s.ServletContextHandler@78aea4b9{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0,[o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,[o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c,[o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722,[o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,AVAILABLE,@Spark}]}]
23:54:07.440 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc
23:54:07.440 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc
23:54:07.440 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@452ba1db{/static,null,null,@Spark}
23:54:07.441 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@452ba1db{/static,null,STARTING,@Spark}
23:54:07.441 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@2289aca5
23:54:07.441 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.spark_project.jetty.servlet.DefaultServlet-6a62689d from default=false
23:54:07.441 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.441 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.441 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.441 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.spark_project.jetty.servlet.DefaultServlet-6a62689d@70045d03==org.spark_project.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true}
23:54:07.441 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.spark_project.jetty.servlet.DefaultServlet-6a62689d=org.spark_project.jetty.servlet.DefaultServlet-6a62689d@70045d03==org.spark_project.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true}
23:54:07.441 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@2289aca5
23:54:07.441 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2332ms org.spark_project.jetty.servlet.ServletHandler@2289aca5
23:54:07.441 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.DefaultServlet-6a62689d@70045d03==org.spark_project.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true
23:54:07.441 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2332ms org.spark_project.jetty.servlet.DefaultServlet-6a62689d@70045d03==org.spark_project.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true
23:54:07.441 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.spark_project.jetty.servlet.DefaultServlet@7927bd9f for org.spark_project.jetty.servlet.DefaultServlet-6a62689d
23:54:07.447 [main] DEBUG o.s.jetty.servlet.DefaultServlet - resource base = jar:file:/home/fabrice/.gradle/caches/modules-2/files-2.1/org.apache.spark/spark-core_2.12/2.4.0/39394b518bba5f3b17aa395240cbe03db370ba0/spark-core_2.12-2.4.0.jar!/org/apache/spark/ui/static
23:54:07.447 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@452ba1db{/static,null,AVAILABLE,@Spark}
23:54:07.447 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2338ms o.s.j.s.ServletContextHandler@452ba1db{/static,null,AVAILABLE,@Spark}
23:54:07.447 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2338ms org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc
23:54:07.448 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2 mime types IncludeExclude@46cc127b{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@60094a13,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@5aceec94}
23:54:07.448 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2 added {o.s.j.s.ServletContextHandler@3e2822{/,null,null,@Spark},MANAGED}
23:54:07.448 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c, org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b, org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722, org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7, org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc, org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2,UNMANAGED}
23:54:07.448 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - ->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2,[o.s.j.s.ServletContextHandler@3e2822{/,null,null,@Spark}]}]
23:54:07.448 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.448 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - static->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc,[o.s.j.s.ServletContextHandler@452ba1db{/static,null,AVAILABLE,@Spark}]}]
23:54:07.448 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b,[o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,AVAILABLE,@Spark}]}]
23:54:07.448 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.448 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.448 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7,[o.s.j.s.ServletContextHandler@78aea4b9{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
23:54:07.448 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0,[o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,AVAILABLE,@Spark}]}]
23:54:07.448 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.448 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.448 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.448 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.448 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.449 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.449 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
23:54:07.449 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,[o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}]}]
23:54:07.449 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.449 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.449 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c,[o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}]}]
23:54:07.449 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.449 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.449 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722,[o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,AVAILABLE,@Spark}]}]
23:54:07.449 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2
23:54:07.449 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2
23:54:07.449 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@3e2822{/,null,null,@Spark}
23:54:07.449 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@3e2822{/,null,STARTING,@Spark}
23:54:07.449 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@79e18e38
23:54:07.449 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$4-29a60c27 from default=false
23:54:07.449 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.449 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.449 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.449 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$4-29a60c27@2f0be93f==org.apache.spark.ui.JettyUtils$$anon$4,jsp=null,order=-1,inst=true}
23:54:07.449 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$4-29a60c27=org.apache.spark.ui.JettyUtils$$anon$4-29a60c27@2f0be93f==org.apache.spark.ui.JettyUtils$$anon$4,jsp=null,order=-1,inst=true}
23:54:07.449 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@79e18e38
23:54:07.449 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2340ms org.spark_project.jetty.servlet.ServletHandler@79e18e38
23:54:07.449 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$4-29a60c27@2f0be93f==org.apache.spark.ui.JettyUtils$$anon$4,jsp=null,order=-1,inst=true
23:54:07.449 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2340ms org.apache.spark.ui.JettyUtils$$anon$4-29a60c27@2f0be93f==org.apache.spark.ui.JettyUtils$$anon$4,jsp=null,order=-1,inst=true
23:54:07.449 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$4@1c32886a for org.apache.spark.ui.JettyUtils$$anon$4-29a60c27
23:54:07.450 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e2822{/,null,AVAILABLE,@Spark}
23:54:07.450 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2340ms o.s.j.s.ServletContextHandler@3e2822{/,null,AVAILABLE,@Spark}
23:54:07.450 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2340ms org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2
23:54:07.450 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d mime types IncludeExclude@10b892d5{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@3d3f761a,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@3546d80f}
23:54:07.450 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d added {o.s.j.s.ServletContextHandler@5fcacc0{/api,null,null,@Spark},MANAGED}
23:54:07.450 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c, org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b, org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722, org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7, org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc, org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2, org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d,UNMANAGED}
23:54:07.450 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - ->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2,[o.s.j.s.ServletContextHandler@3e2822{/,null,AVAILABLE,@Spark}]}]
23:54:07.450 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.450 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - static->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc,[o.s.j.s.ServletContextHandler@452ba1db{/static,null,AVAILABLE,@Spark}]}]
23:54:07.450 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b,[o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7,[o.s.j.s.ServletContextHandler@78aea4b9{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0,[o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,[o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c,[o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - api->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d,[o.s.j.s.ServletContextHandler@5fcacc0{/api,null,null,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722,[o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,AVAILABLE,@Spark}]}]
23:54:07.451 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d
23:54:07.451 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d
23:54:07.451 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@5fcacc0{/api,null,null,@Spark}
23:54:07.451 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@5fcacc0{/api,null,STARTING,@Spark}
23:54:07.451 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@533b266e
23:54:07.451 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/* mapped to servlet=org.glassfish.jersey.servlet.ServletContainer-5f3b9c57 from default=false
23:54:07.451 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.451 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.451 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.451 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/*=org.glassfish.jersey.servlet.ServletContainer-5f3b9c57@cadbfd1a==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=-1,inst=false}
23:54:07.452 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.glassfish.jersey.servlet.ServletContainer-5f3b9c57=org.glassfish.jersey.servlet.ServletContainer-5f3b9c57@cadbfd1a==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=-1,inst=false}
23:54:07.452 [main] DEBUG o.s.jetty.servlet.ServletHandler - Adding Default404Servlet to org.spark_project.jetty.servlet.ServletHandler@533b266e
23:54:07.452 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@533b266e added {org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-452e26d0@fbdd6ac8==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet,jsp=null,order=-1,inst=false,AUTO}
23:54:07.452 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@533b266e added {[/]=>org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-452e26d0,POJO}
23:54:07.452 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/* mapped to servlet=org.glassfish.jersey.servlet.ServletContainer-5f3b9c57 from default=false
23:54:07.452 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-452e26d0 from default=false
23:54:07.452 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.452 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.452 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.452 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/*=org.glassfish.jersey.servlet.ServletContainer-5f3b9c57@cadbfd1a==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=-1,inst=false, /=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-452e26d0@fbdd6ac8==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet,jsp=null,order=-1,inst=false}
23:54:07.452 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.glassfish.jersey.servlet.ServletContainer-5f3b9c57=org.glassfish.jersey.servlet.ServletContainer-5f3b9c57@cadbfd1a==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=-1,inst=false, org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-452e26d0=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-452e26d0@fbdd6ac8==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet,jsp=null,order=-1,inst=false}
23:54:07.452 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@533b266e
23:54:07.452 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2343ms org.spark_project.jetty.servlet.ServletHandler@533b266e
23:54:07.453 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.glassfish.jersey.servlet.ServletContainer-5f3b9c57@cadbfd1a==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=-1,inst=false
23:54:07.453 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2343ms org.glassfish.jersey.servlet.ServletContainer-5f3b9c57@cadbfd1a==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=-1,inst=false
23:54:07.453 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-452e26d0@fbdd6ac8==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet,jsp=null,order=-1,inst=false
23:54:07.453 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2344ms org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-452e26d0@fbdd6ac8==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet,jsp=null,order=-1,inst=false
23:54:07.453 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fcacc0{/api,null,AVAILABLE,@Spark}
23:54:07.453 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2344ms o.s.j.s.ServletContextHandler@5fcacc0{/api,null,AVAILABLE,@Spark}
23:54:07.453 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2344ms org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d
23:54:07.453 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da mime types IncludeExclude@790174f2{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@42257bdd,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@7689ddef}
23:54:07.453 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da added {o.s.j.s.ServletContextHandler@34645867{/jobs/job/kill,null,null,@Spark},MANAGED}
23:54:07.453 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c, org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b, org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722, org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7, org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc, org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2, org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d, org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da,UNMANAGED}
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - ->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2,[o.s.j.s.ServletContextHandler@3e2822{/,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - static->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc,[o.s.j.s.ServletContextHandler@452ba1db{/static,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b,[o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7,[o.s.j.s.ServletContextHandler@78aea4b9{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0,[o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,[o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c,[o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - api->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d,[o.s.j.s.ServletContextHandler@5fcacc0{/api,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/kill->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da,[o.s.j.s.ServletContextHandler@34645867{/jobs/job/kill,null,null,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.454 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722,[o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,AVAILABLE,@Spark}]}]
23:54:07.455 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da
23:54:07.455 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da
23:54:07.455 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@34645867{/jobs/job/kill,null,null,@Spark}
23:54:07.455 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@34645867{/jobs/job/kill,null,STARTING,@Spark}
23:54:07.455 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@2484f433
23:54:07.455 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$4-60b71e8f from default=false
23:54:07.455 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.455 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.455 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.455 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$4-60b71e8f@f43bf45c==org.apache.spark.ui.JettyUtils$$anon$4,jsp=null,order=-1,inst=true}
23:54:07.455 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$4-60b71e8f=org.apache.spark.ui.JettyUtils$$anon$4-60b71e8f@f43bf45c==org.apache.spark.ui.JettyUtils$$anon$4,jsp=null,order=-1,inst=true}
23:54:07.455 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@2484f433
23:54:07.455 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2346ms org.spark_project.jetty.servlet.ServletHandler@2484f433
23:54:07.455 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$4-60b71e8f@f43bf45c==org.apache.spark.ui.JettyUtils$$anon$4,jsp=null,order=-1,inst=true
23:54:07.455 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2346ms org.apache.spark.ui.JettyUtils$$anon$4-60b71e8f@f43bf45c==org.apache.spark.ui.JettyUtils$$anon$4,jsp=null,order=-1,inst=true
23:54:07.455 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$4@687a762c for org.apache.spark.ui.JettyUtils$$anon$4-60b71e8f
23:54:07.455 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34645867{/jobs/job/kill,null,AVAILABLE,@Spark}
23:54:07.455 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2346ms o.s.j.s.ServletContextHandler@34645867{/jobs/job/kill,null,AVAILABLE,@Spark}
23:54:07.455 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2346ms org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da
23:54:07.455 [main] DEBUG o.s.j.s.handler.gzip.GzipHandler - org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935 mime types IncludeExclude@733c423e{i=[],ip=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@4b629f13,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/basic, audio/x-pn-realaudio, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime],ep=org.spark_project.jetty.util.IncludeExcludeSet$SetContainsPredicate@70925b45}
23:54:07.456 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935 added {o.s.j.s.ServletContextHandler@5f59185e{/stages/stage/kill,null,null,@Spark},MANAGED}
23:54:07.456 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c, org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b, org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722, org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7, org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc, org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2, org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d, org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da, org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935] added {org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935,UNMANAGED}
23:54:07.456 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - ->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2,[o.s.j.s.ServletContextHandler@3e2822{/,null,AVAILABLE,@Spark}]}]
23:54:07.456 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.456 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.456 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
23:54:07.456 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - api->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d,[o.s.j.s.ServletContextHandler@5fcacc0{/api,null,AVAILABLE,@Spark}]}]
23:54:07.456 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.456 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.456 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.456 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - static->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc,[o.s.j.s.ServletContextHandler@452ba1db{/static,null,AVAILABLE,@Spark}]}]
23:54:07.456 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b,[o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,AVAILABLE,@Spark}]}]
23:54:07.456 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.456 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7,[o.s.j.s.ServletContextHandler@78aea4b9{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
23:54:07.456 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0,[o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,AVAILABLE,@Spark}]}]
23:54:07.456 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.456 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.457 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.457 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.457 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.457 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/kill->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935,[o.s.j.s.ServletContextHandler@5f59185e{/stages/stage/kill,null,null,@Spark}]}]
23:54:07.457 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.457 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,[o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}]}]
23:54:07.457 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.457 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c,[o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}]}]
23:54:07.457 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/kill->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da,[o.s.j.s.ServletContextHandler@34645867{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
23:54:07.457 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722,[o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,AVAILABLE,@Spark}]}]
23:54:07.457 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935
23:54:07.457 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935
23:54:07.457 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@5f59185e{/stages/stage/kill,null,null,@Spark}
23:54:07.457 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@5f59185e{/stages/stage/kill,null,STARTING,@Spark}
23:54:07.457 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@60bdf15d
23:54:07.457 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$4-47da3952 from default=false
23:54:07.457 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.457 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.457 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.457 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$4-47da3952@9c7f08ce==org.apache.spark.ui.JettyUtils$$anon$4,jsp=null,order=-1,inst=true}
23:54:07.457 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$4-47da3952=org.apache.spark.ui.JettyUtils$$anon$4-47da3952@9c7f08ce==org.apache.spark.ui.JettyUtils$$anon$4,jsp=null,order=-1,inst=true}
23:54:07.457 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@60bdf15d
23:54:07.457 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2348ms org.spark_project.jetty.servlet.ServletHandler@60bdf15d
23:54:07.457 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$4-47da3952@9c7f08ce==org.apache.spark.ui.JettyUtils$$anon$4,jsp=null,order=-1,inst=true
23:54:07.457 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2348ms org.apache.spark.ui.JettyUtils$$anon$4-47da3952@9c7f08ce==org.apache.spark.ui.JettyUtils$$anon$4,jsp=null,order=-1,inst=true
23:54:07.457 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$4@1b9ea3e3 for org.apache.spark.ui.JettyUtils$$anon$4-47da3952
23:54:07.457 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f59185e{/stages/stage/kill,null,AVAILABLE,@Spark}
23:54:07.457 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2348ms o.s.j.s.ServletContextHandler@5f59185e{/stages/stage/kill,null,AVAILABLE,@Spark}
23:54:07.457 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2348ms org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935
23:54:07.459 [main] INFO  org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.67:4040
23:54:07.552 [main] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
23:54:07.595 [main] DEBUG o.a.s.network.server.TransportServer - Shuffle server started on port: 42085
23:54:07.595 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42085.
23:54:07.595 [main] INFO  o.a.s.n.n.NettyBlockTransferService - Server created on 192.168.1.67:42085
23:54:07.596 [main] INFO  o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23:54:07.612 [main] INFO  o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.67, 42085, None)
23:54:07.614 [dispatcher-event-loop-2] DEBUG o.a.s.storage.DefaultTopologyMapper - Got a request for 192.168.1.67
23:54:07.616 [dispatcher-event-loop-2] INFO  o.a.s.s.BlockManagerMasterEndpoint - Registering block manager 192.168.1.67:42085 with 1929.9 MB RAM, BlockManagerId(driver, 192.168.1.67, 42085, None)
23:54:07.618 [main] INFO  o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.67, 42085, None)
23:54:07.618 [main] INFO  o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.67, 42085, None)
23:54:07.725 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@41a6d121
23:54:07.725 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@4f449e8f{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@411291e5,MANAGED}
23:54:07.725 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@411291e5 added {org.apache.spark.ui.JettyUtils$$anon$3-6e28bb87@a1d87c20==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.726 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@411291e5 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-6e28bb87,POJO}
23:54:07.727 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c, org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b, org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722, org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7, org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc, org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2, org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d, org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da, org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935, o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,null,@Spark}] added {o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,null,@Spark},UNMANAGED}
23:54:07.728 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - ->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2,[o.s.j.s.ServletContextHandler@3e2822{/,null,AVAILABLE,@Spark}]}]
23:54:07.728 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.728 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.728 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
23:54:07.728 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - api->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d,[o.s.j.s.ServletContextHandler@5fcacc0{/api,null,AVAILABLE,@Spark}]}]
23:54:07.728 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.728 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.728 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.728 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - static->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc,[o.s.j.s.ServletContextHandler@452ba1db{/static,null,AVAILABLE,@Spark}]}]
23:54:07.728 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b,[o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,AVAILABLE,@Spark}]}]
23:54:07.728 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.728 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7,[o.s.j.s.ServletContextHandler@78aea4b9{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
23:54:07.728 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0,[o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,AVAILABLE,@Spark}]}]
23:54:07.728 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.728 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.728 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.728 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.729 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.729 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/kill->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935,[o.s.j.s.ServletContextHandler@5f59185e{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
23:54:07.729 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.729 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,[o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}]}]
23:54:07.729 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.729 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c,[o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}]}]
23:54:07.729 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/kill->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da,[o.s.j.s.ServletContextHandler@34645867{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
23:54:07.729 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - metrics/json->[{o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,null,@Spark},[o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,null,@Spark}]}]
23:54:07.729 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722,[o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,AVAILABLE,@Spark}]}]
23:54:07.729 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,null,@Spark}
23:54:07.729 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,STARTING,@Spark}
23:54:07.729 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@411291e5
23:54:07.729 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-6e28bb87 from default=false
23:54:07.729 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.729 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.729 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.729 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-6e28bb87@a1d87c20==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.729 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-6e28bb87=org.apache.spark.ui.JettyUtils$$anon$3-6e28bb87@a1d87c20==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.729 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@411291e5
23:54:07.729 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2620ms org.spark_project.jetty.servlet.ServletHandler@411291e5
23:54:07.730 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-6e28bb87@a1d87c20==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.730 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2620ms org.apache.spark.ui.JettyUtils$$anon$3-6e28bb87@a1d87c20==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.730 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@c3c4c1c for org.apache.spark.ui.JettyUtils$$anon$3-6e28bb87
23:54:07.730 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,AVAILABLE,@Spark}
23:54:07.730 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2621ms o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,AVAILABLE,@Spark}
23:54:07.812 [main] DEBUG org.apache.spark.SparkContext - Adding shutdown hook
23:54:07.856 [main] INFO  com.spark.examples.gradle.SparkInit - Csv input file :/home/fabrice/workspace/sparkexamples/gradle.template/build/resources/main/test.csv
23:54:07.856 [main] INFO  com.spark.examples.gradle.SparkInit - Output json :/home/fabrice/Documents/jsonOut
23:54:07.882 [main] INFO  o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/fabrice/workspace/sparkexamples/gradle.template/spark-warehouse').
23:54:07.883 [main] INFO  o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/fabrice/workspace/sparkexamples/gradle.template/spark-warehouse'.
23:54:07.891 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@7c28c1
23:54:07.891 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@baf1bb3{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@67ec8477,MANAGED}
23:54:07.891 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@67ec8477 added {org.apache.spark.ui.JettyUtils$$anon$3-25d958c6@6c7405da==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.891 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@67ec8477 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-25d958c6,POJO}
23:54:07.891 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@4482469c
23:54:07.891 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@3703bf3c{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@34cf5a97,MANAGED}
23:54:07.891 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@34cf5a97 added {org.apache.spark.ui.JettyUtils$$anon$3-5b3f3ba0@9f6c56ca==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.891 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@34cf5a97 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-5b3f3ba0,POJO}
23:54:07.892 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c, org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b, org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722, org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7, org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc, org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2, org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d, org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da, org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935, o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,AVAILABLE,@Spark}, o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,null,@Spark}] added {o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,null,@Spark},UNMANAGED}
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - ->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2,[o.s.j.s.ServletContextHandler@3e2822{/,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - api->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d,[o.s.j.s.ServletContextHandler@5fcacc0{/api,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - static->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc,[o.s.j.s.ServletContextHandler@452ba1db{/static,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b,[o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7,[o.s.j.s.ServletContextHandler@78aea4b9{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0,[o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - SQL->[{o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,null,@Spark},[o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,null,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/kill->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935,[o.s.j.s.ServletContextHandler@5f59185e{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,[o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c,[o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/kill->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da,[o.s.j.s.ServletContextHandler@34645867{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - metrics/json->[{o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722,[o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,AVAILABLE,@Spark}]}]
23:54:07.892 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,null,@Spark}
23:54:07.893 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,STARTING,@Spark}
23:54:07.893 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@67ec8477
23:54:07.893 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-25d958c6 from default=false
23:54:07.893 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.893 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.893 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.893 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-25d958c6@6c7405da==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.893 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-25d958c6=org.apache.spark.ui.JettyUtils$$anon$3-25d958c6@6c7405da==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.893 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@67ec8477
23:54:07.893 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2784ms org.spark_project.jetty.servlet.ServletHandler@67ec8477
23:54:07.893 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-25d958c6@6c7405da==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.893 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2784ms org.apache.spark.ui.JettyUtils$$anon$3-25d958c6@6c7405da==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.893 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@ddf20fd for org.apache.spark.ui.JettyUtils$$anon$3-25d958c6
23:54:07.893 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,AVAILABLE,@Spark}
23:54:07.893 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2784ms o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,AVAILABLE,@Spark}
23:54:07.893 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c, org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b, org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722, org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7, org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc, org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2, org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d, org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da, org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935, o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,AVAILABLE,@Spark}, o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,AVAILABLE,@Spark}, o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,null,@Spark}] added {o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,null,@Spark},UNMANAGED}
23:54:07.893 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - ->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2,[o.s.j.s.ServletContextHandler@3e2822{/,null,AVAILABLE,@Spark}]}]
23:54:07.893 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.893 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.893 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
23:54:07.893 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - api->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d,[o.s.j.s.ServletContextHandler@5fcacc0{/api,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - static->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc,[o.s.j.s.ServletContextHandler@452ba1db{/static,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b,[o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7,[o.s.j.s.ServletContextHandler@78aea4b9{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0,[o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - SQL->[{o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/kill->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935,[o.s.j.s.ServletContextHandler@5f59185e{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,[o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c,[o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - SQL/json->[{o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,null,@Spark},[o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,null,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/kill->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da,[o.s.j.s.ServletContextHandler@34645867{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - metrics/json->[{o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722,[o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,AVAILABLE,@Spark}]}]
23:54:07.894 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,null,@Spark}
23:54:07.894 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,STARTING,@Spark}
23:54:07.894 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@34cf5a97
23:54:07.894 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-5b3f3ba0 from default=false
23:54:07.894 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.894 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.894 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.894 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-5b3f3ba0@9f6c56ca==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.895 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-5b3f3ba0=org.apache.spark.ui.JettyUtils$$anon$3-5b3f3ba0@9f6c56ca==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.895 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@34cf5a97
23:54:07.895 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2785ms org.spark_project.jetty.servlet.ServletHandler@34cf5a97
23:54:07.895 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-5b3f3ba0@9f6c56ca==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.895 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2786ms org.apache.spark.ui.JettyUtils$$anon$3-5b3f3ba0@9f6c56ca==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.895 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@278f8425 for org.apache.spark.ui.JettyUtils$$anon$3-5b3f3ba0
23:54:07.895 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,AVAILABLE,@Spark}
23:54:07.895 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2786ms o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,AVAILABLE,@Spark}
23:54:07.895 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@4b1abd11
23:54:07.895 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@3f36b447{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@6443b128,MANAGED}
23:54:07.895 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@6443b128 added {org.apache.spark.ui.JettyUtils$$anon$3-5eeedb60@93798114==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.895 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@6443b128 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-5eeedb60,POJO}
23:54:07.895 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@40021799
23:54:07.895 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@4a1c0752{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@332f25c8,MANAGED}
23:54:07.895 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@332f25c8 added {org.apache.spark.ui.JettyUtils$$anon$3-1b32cd16@fd20b0a2==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true,AUTO}
23:54:07.896 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@332f25c8 added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-1b32cd16,POJO}
23:54:07.896 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c, org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b, org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722, org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7, org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc, org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2, org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d, org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da, org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935, o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,AVAILABLE,@Spark}, o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,AVAILABLE,@Spark}, o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,AVAILABLE,@Spark}, o.s.j.s.ServletContextHandler@3f36b447{/SQL/execution,null,null,@Spark}] added {o.s.j.s.ServletContextHandler@3f36b447{/SQL/execution,null,null,@Spark},UNMANAGED}
23:54:07.896 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - ->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2,[o.s.j.s.ServletContextHandler@3e2822{/,null,AVAILABLE,@Spark}]}]
23:54:07.896 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.896 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.896 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
23:54:07.896 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - api->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d,[o.s.j.s.ServletContextHandler@5fcacc0{/api,null,AVAILABLE,@Spark}]}]
23:54:07.896 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.896 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - static->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc,[o.s.j.s.ServletContextHandler@452ba1db{/static,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b,[o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7,[o.s.j.s.ServletContextHandler@78aea4b9{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0,[o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - SQL->[{o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/kill->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935,[o.s.j.s.ServletContextHandler@5f59185e{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,[o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c,[o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - SQL/json->[{o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/kill->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da,[o.s.j.s.ServletContextHandler@34645867{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - metrics/json->[{o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - SQL/execution->[{o.s.j.s.ServletContextHandler@3f36b447{/SQL/execution,null,null,@Spark},[o.s.j.s.ServletContextHandler@3f36b447{/SQL/execution,null,null,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722,[o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,AVAILABLE,@Spark}]}]
23:54:07.897 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@3f36b447{/SQL/execution,null,null,@Spark}
23:54:07.897 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@3f36b447{/SQL/execution,null,STARTING,@Spark}
23:54:07.897 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@6443b128
23:54:07.897 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-5eeedb60 from default=false
23:54:07.897 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.897 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.898 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.898 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-5eeedb60@93798114==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.898 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-5eeedb60=org.apache.spark.ui.JettyUtils$$anon$3-5eeedb60@93798114==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.898 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@6443b128
23:54:07.898 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2788ms org.spark_project.jetty.servlet.ServletHandler@6443b128
23:54:07.898 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-5eeedb60@93798114==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.898 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2789ms org.apache.spark.ui.JettyUtils$$anon$3-5eeedb60@93798114==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.898 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@4bdcaf36 for org.apache.spark.ui.JettyUtils$$anon$3-5eeedb60
23:54:07.898 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f36b447{/SQL/execution,null,AVAILABLE,@Spark}
23:54:07.898 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2789ms o.s.j.s.ServletContextHandler@3f36b447{/SQL/execution,null,AVAILABLE,@Spark}
23:54:07.898 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c, org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b, org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722, org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7, org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc, org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2, org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d, org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da, org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935, o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,AVAILABLE,@Spark}, o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,AVAILABLE,@Spark}, o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,AVAILABLE,@Spark}, o.s.j.s.ServletContextHandler@3f36b447{/SQL/execution,null,AVAILABLE,@Spark}, o.s.j.s.ServletContextHandler@4a1c0752{/SQL/execution/json,null,null,@Spark}] added {o.s.j.s.ServletContextHandler@4a1c0752{/SQL/execution/json,null,null,@Spark},UNMANAGED}
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - ->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2,[o.s.j.s.ServletContextHandler@3e2822{/,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - SQL/execution/json->[{o.s.j.s.ServletContextHandler@4a1c0752{/SQL/execution/json,null,null,@Spark},[o.s.j.s.ServletContextHandler@4a1c0752{/SQL/execution/json,null,null,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - api->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d,[o.s.j.s.ServletContextHandler@5fcacc0{/api,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - static->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc,[o.s.j.s.ServletContextHandler@452ba1db{/static,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b,[o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7,[o.s.j.s.ServletContextHandler@78aea4b9{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0,[o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - SQL->[{o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/kill->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935,[o.s.j.s.ServletContextHandler@5f59185e{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,[o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c,[o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - SQL/json->[{o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/kill->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da,[o.s.j.s.ServletContextHandler@34645867{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - metrics/json->[{o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - SQL/execution->[{o.s.j.s.ServletContextHandler@3f36b447{/SQL/execution,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@3f36b447{/SQL/execution,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722,[o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,AVAILABLE,@Spark}]}]
23:54:07.899 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@4a1c0752{/SQL/execution/json,null,null,@Spark}
23:54:07.899 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@4a1c0752{/SQL/execution/json,null,STARTING,@Spark}
23:54:07.900 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@332f25c8
23:54:07.900 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-1b32cd16 from default=false
23:54:07.900 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.900 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.900 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.900 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-1b32cd16@fd20b0a2==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.900 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-1b32cd16=org.apache.spark.ui.JettyUtils$$anon$3-1b32cd16@fd20b0a2==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true}
23:54:07.900 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@332f25c8
23:54:07.900 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2791ms org.spark_project.jetty.servlet.ServletHandler@332f25c8
23:54:07.900 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$3-1b32cd16@fd20b0a2==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.900 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2791ms org.apache.spark.ui.JettyUtils$$anon$3-1b32cd16@fd20b0a2==org.apache.spark.ui.JettyUtils$$anon$3,jsp=null,order=-1,inst=true
23:54:07.900 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$3@61d01788 for org.apache.spark.ui.JettyUtils$$anon$3-1b32cd16
23:54:07.900 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a1c0752{/SQL/execution/json,null,AVAILABLE,@Spark}
23:54:07.900 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2791ms o.s.j.s.ServletContextHandler@4a1c0752{/SQL/execution/json,null,AVAILABLE,@Spark}
23:54:07.901 [main] DEBUG o.s.j.util.DecoratedObjectFactory - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@69cac930
23:54:07.901 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@19593091{/,null,null} added {org.spark_project.jetty.servlet.ServletHandler@5d39f2d8,MANAGED}
23:54:07.901 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@5d39f2d8 added {org.spark_project.jetty.servlet.DefaultServlet-6ad6fa53@bed212a5==org.spark_project.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true,AUTO}
23:54:07.901 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.servlet.ServletHandler@5d39f2d8 added {[/]=>org.spark_project.jetty.servlet.DefaultServlet-6ad6fa53,POJO}
23:54:07.901 [main] DEBUG o.s.j.u.component.ContainerLifeCycle - org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c, org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b, org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722, org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7, org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc, org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2, org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d, org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da, org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935, o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,AVAILABLE,@Spark}, o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,AVAILABLE,@Spark}, o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,AVAILABLE,@Spark}, o.s.j.s.ServletContextHandler@3f36b447{/SQL/execution,null,AVAILABLE,@Spark}, o.s.j.s.ServletContextHandler@4a1c0752{/SQL/execution/json,null,AVAILABLE,@Spark}, o.s.j.s.ServletContextHandler@19593091{/static/sql,null,null,@Spark}] added {o.s.j.s.ServletContextHandler@19593091{/static/sql,null,null,@Spark},UNMANAGED}
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - ->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2,[o.s.j.s.ServletContextHandler@3e2822{/,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03,[o.s.j.s.ServletContextHandler@77192705{/storage/rdd,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740,[o.s.j.s.ServletContextHandler@52b56a3e{/storage,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/rdd/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff,[o.s.j.s.ServletContextHandler@625e134e{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - SQL/execution/json->[{o.s.j.s.ServletContextHandler@4a1c0752{/SQL/execution/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@4a1c0752{/SQL/execution/json,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - api->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d,[o.s.j.s.ServletContextHandler@5fcacc0{/api,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a,[o.s.j.s.ServletContextHandler@7fcbe147{/stages/pool/json,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/pool->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef,[o.s.j.s.ServletContextHandler@4ebea12c{/stages/pool,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5,[o.s.j.s.ServletContextHandler@acb0951{/jobs/json,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - static->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc,[o.s.j.s.ServletContextHandler@452ba1db{/static,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b,[o.s.j.s.ServletContextHandler@5f2f577{/executors/json,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94,[o.s.j.s.ServletContextHandler@423e4cbb{/stages/stage/json,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7,[o.s.j.s.ServletContextHandler@78aea4b9{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0,[o.s.j.s.ServletContextHandler@165b8a71{/environment/json,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a,[o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/json,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905,[o.s.j.s.ServletContextHandler@18245eb0{/jobs,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd,[o.s.j.s.ServletContextHandler@2a39fe6a{/stages/json,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf,[o.s.j.s.ServletContextHandler@7e242b4d{/stages/stage,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - storage/json->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c,[o.s.j.s.ServletContextHandler@4fad94a7{/storage/json,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - SQL->[{o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - static/sql->[{o.s.j.s.ServletContextHandler@19593091{/static/sql,null,null,@Spark},[o.s.j.s.ServletContextHandler@19593091{/static/sql,null,null,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages/stage/kill->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935,[o.s.j.s.ServletContextHandler@5f59185e{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b,[o.s.j.s.ServletContextHandler@68d6972f{/jobs/job,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - environment->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb,[o.s.j.s.ServletContextHandler@119f1f2a{/environment,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - stages->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e,[o.s.j.s.ServletContextHandler@7c351808{/stages,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c,[o.s.j.s.ServletContextHandler@30ea8c23{/executors,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - SQL/json->[{o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - jobs/job/kill->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da,[o.s.j.s.ServletContextHandler@34645867{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - metrics/json->[{o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - SQL/execution->[{o.s.j.s.ServletContextHandler@3f36b447{/SQL/execution,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@3f36b447{/SQL/execution,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.s.h.ContextHandlerCollection - executors/threadDump->[{org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722,[o.s.j.s.ServletContextHandler@3d8bbcdc{/executors/threadDump,null,AVAILABLE,@Spark}]}]
23:54:07.902 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@19593091{/static/sql,null,null,@Spark}
23:54:07.902 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@19593091{/static/sql,null,STARTING,@Spark}
23:54:07.902 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.ServletHandler@5d39f2d8
23:54:07.903 [main] DEBUG o.s.jetty.servlet.ServletHandler - Chose path=/ mapped to servlet=org.spark_project.jetty.servlet.DefaultServlet-6ad6fa53 from default=false
23:54:07.903 [main] DEBUG o.s.jetty.servlet.ServletHandler - filterNameMap={}
23:54:07.903 [main] DEBUG o.s.jetty.servlet.ServletHandler - pathFilters=null
23:54:07.903 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletFilterMap=null
23:54:07.903 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletPathMap={/=org.spark_project.jetty.servlet.DefaultServlet-6ad6fa53@bed212a5==org.spark_project.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true}
23:54:07.903 [main] DEBUG o.s.jetty.servlet.ServletHandler - servletNameMap={org.spark_project.jetty.servlet.DefaultServlet-6ad6fa53=org.spark_project.jetty.servlet.DefaultServlet-6ad6fa53@bed212a5==org.spark_project.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true}
23:54:07.903 [main] DEBUG o.s.j.server.handler.AbstractHandler - starting org.spark_project.jetty.servlet.ServletHandler@5d39f2d8
23:54:07.903 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2794ms org.spark_project.jetty.servlet.ServletHandler@5d39f2d8
23:54:07.903 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - starting org.spark_project.jetty.servlet.DefaultServlet-6ad6fa53@bed212a5==org.spark_project.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true
23:54:07.903 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2794ms org.spark_project.jetty.servlet.DefaultServlet-6ad6fa53@bed212a5==org.spark_project.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true
23:54:07.903 [main] DEBUG o.s.jetty.servlet.ServletHolder - Servlet.init org.spark_project.jetty.servlet.DefaultServlet@55ea2d70 for org.spark_project.jetty.servlet.DefaultServlet-6ad6fa53
23:54:07.903 [main] DEBUG o.s.jetty.servlet.DefaultServlet - resource base = jar:file:/home/fabrice/.gradle/caches/modules-2/files-2.1/org.apache.spark/spark-sql_2.12/2.4.0/2f2331e79d815b394d9ad23d4ff0fb20c24e45ae/spark-sql_2.12-2.4.0.jar!/org/apache/spark/sql/execution/ui/static
23:54:07.903 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19593091{/static/sql,null,AVAILABLE,@Spark}
23:54:07.903 [main] DEBUG o.s.j.u.component.AbstractLifeCycle - STARTED @2794ms o.s.j.s.ServletContextHandler@19593091{/static/sql,null,AVAILABLE,@Spark}
23:54:08.411 [main] INFO  o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
23:54:09.827 [main] DEBUG o.a.s.s.c.a.Analyzer$ResolveReferences - Resolving 'value to value#0
23:54:09.859 [main] DEBUG o.a.s.s.i.BaseSessionStateBuilder$$anon$1 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#0]
 +- Relation[value#0] text                  +- Relation[value#0] text
          
23:54:09.961 [main] DEBUG o.a.s.s.i.BaseSessionStateBuilder$$anon$1 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#4: java.lang.String   DeserializeToObject cast(value#0 as string).toString, obj#4: java.lang.String
 +- LocalRelation <empty>, [value#0]                                                                                                                                      +- LocalRelation <empty>, [value#0]
          
23:54:09.997 [main] DEBUG o.a.s.s.c.a.Analyzer$ResolveReferences - Resolving 'value to value#0
23:54:10.016 [main] DEBUG o.a.s.s.i.BaseSessionStateBuilder$$anon$1 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#0, None)) > 0)
 +- Project [value#0]                       +- Project [value#0]
    +- Relation[value#0] text                  +- Relation[value#0] text
          
23:54:10.025 [main] DEBUG o.a.s.s.i.BaseSessionStateBuilder$$anon$1 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#5: java.lang.String   DeserializeToObject cast(value#0 as string).toString, obj#5: java.lang.String
 +- LocalRelation <empty>, [value#0]                                                                                                                                      +- LocalRelation <empty>, [value#0]
          
23:54:10.037 [main] DEBUG o.a.s.s.i.BaseSessionStateBuilder$$anon$1 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#6: java.lang.String   DeserializeToObject cast(value#0 as string).toString, obj#6: java.lang.String
 +- LocalRelation <empty>, [value#0]                                                                                                                                      +- LocalRelation <empty>, [value#0]
          
23:54:10.156 [main] DEBUG o.a.s.s.i.BaseSessionStateBuilder$$anon$2 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                    GlobalLimit 1
 +- LocalLimit 1                                  +- LocalLimit 1
    +- Filter (length(trim(value#0, None)) > 0)      +- Filter (length(trim(value#0, None)) > 0)
!      +- Project [value#0]                             +- Relation[value#0] text
!         +- Relation[value#0] text               
          
23:54:10.199 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Pruning directories with: 
23:54:10.201 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Post-Scan Filters: (length(trim(value#0, None)) > 0)
23:54:10.204 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Output Data Schema: struct<value: string>
23:54:10.209 [main] INFO  o.a.s.s.execution.FileSourceScanExec - Pushed Filters: 
23:54:10.362 [main] DEBUG o.a.s.s.c.e.c.GenerateSafeProjection - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

23:54:10.379 [main] DEBUG o.a.s.s.c.e.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

23:54:10.513 [main] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 149.721136 ms
23:54:10.729 [main] DEBUG o.a.s.s.e.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

23:54:10.732 [main] DEBUG o.a.s.s.c.e.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

23:54:10.748 [main] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.603723 ms
23:54:10.816 [main] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 221.5 KB, free 1929.7 MB)
23:54:10.817 [main] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_0 locally took  46 ms
23:54:10.818 [main] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_0 without replication took  47 ms
23:54:10.868 [main] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1929.7 MB)
23:54:10.870 [dispatcher-event-loop-5] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.1.67:42085 (size: 20.7 KB, free: 1929.9 MB)
23:54:10.870 [main] DEBUG o.a.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
23:54:10.871 [main] DEBUG o.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
23:54:10.871 [main] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_0_piece0 locally took  5 ms
23:54:10.871 [main] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_0_piece0 without replication took  5 ms
23:54:10.872 [main] INFO  org.apache.spark.SparkContext - Created broadcast 0 from csv at SparkInit.java:35
23:54:10.877 [main] INFO  o.a.s.s.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23:54:10.926 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$doExecute$4$adapted
23:54:10.939 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
23:54:10.977 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$executeTake$1
23:54:10.980 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$executeTake$1) is now cleaned +++
23:54:10.984 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$executeTake$2
23:54:10.986 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$executeTake$2) is now cleaned +++
23:54:10.988 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$runJob$5
23:54:10.995 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$runJob$5) is now cleaned +++
23:54:10.997 [main] INFO  org.apache.spark.SparkContext - Starting job: csv at SparkInit.java:35
23:54:11.020 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Got job 0 (csv at SparkInit.java:35) with 1 output partitions
23:54:11.021 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (csv at SparkInit.java:35)
23:54:11.021 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
23:54:11.023 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Missing parents: List()
23:54:11.026 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - submitStage(ResultStage 0)
23:54:11.027 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - missing: List()
23:54:11.028 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at SparkInit.java:35), which has no missing parents
23:54:11.029 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 0)
23:54:11.105 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 10.2 KB, free 1929.7 MB)
23:54:11.105 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_1 locally took  1 ms
23:54:11.105 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_1 without replication took  1 ms
23:54:11.107 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 1929.6 MB)
23:54:11.108 [dispatcher-event-loop-6] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.1.67:42085 (size: 5.0 KB, free: 1929.9 MB)
23:54:11.108 [dag-scheduler-event-loop] DEBUG o.a.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
23:54:11.108 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
23:54:11.108 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_1_piece0 locally took  2 ms
23:54:11.108 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_1_piece0 without replication took  2 ms
23:54:11.109 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1161
23:54:11.118 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at SparkInit.java:35) (first 15 tasks are for partitions Vector(0))
23:54:11.119 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
23:54:11.134 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.TaskSetManager - Epoch for TaskSet 0.0: 0
23:54:11.137 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
23:54:11.147 [dispatcher-event-loop-7] DEBUG o.a.s.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
23:54:11.147 [dispatcher-event-loop-7] DEBUG o.a.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
23:54:11.172 [dispatcher-event-loop-7] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7847 bytes)
23:54:11.176 [dispatcher-event-loop-7] DEBUG o.a.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
23:54:11.182 [Executor task launch worker for task 0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
23:54:11.210 [Executor task launch worker for task 0] DEBUG o.apache.spark.storage.BlockManager - Getting local block broadcast_1
23:54:11.211 [Executor task launch worker for task 0] DEBUG o.apache.spark.storage.BlockManager - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
23:54:11.456 [Executor task launch worker for task 0] INFO  o.a.s.s.e.datasources.FileScanRDD - Reading File path: file:///home/fabrice/workspace/sparkexamples/gradle.template/build/resources/main/test.csv, range: 0-121306, partition values: [empty row]
23:54:11.462 [Executor task launch worker for task 0] DEBUG o.a.s.s.c.e.c.GenerateUnsafeProjection - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

23:54:11.464 [Executor task launch worker for task 0] DEBUG o.a.s.s.c.e.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

23:54:11.474 [Executor task launch worker for task 0] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 11.898717 ms
23:54:11.475 [Executor task launch worker for task 0] DEBUG o.apache.spark.storage.BlockManager - Getting local block broadcast_0
23:54:11.475 [Executor task launch worker for task 0] DEBUG o.apache.spark.storage.BlockManager - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
23:54:11.501 [Executor task launch worker for task 0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1351 bytes result sent to driver
23:54:11.503 [dispatcher-event-loop-1] DEBUG o.a.s.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
23:54:11.506 [task-result-getter-0] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 354 ms on localhost (executor driver) (1/1)
23:54:11.509 [task-result-getter-0] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
23:54:11.513 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ResultStage 0 (csv at SparkInit.java:35) finished in 0.465 s
23:54:11.515 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - After removal of stage 0, remaining stages = 0
23:54:11.516 [main] INFO  o.a.spark.scheduler.DAGScheduler - Job 0 finished: csv at SparkInit.java:35, took 0.519564 s
23:54:11.548 [main] DEBUG o.a.s.s.i.BaseSessionStateBuilder$$anon$1 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#8: java.lang.String   DeserializeToObject cast(value#0 as string).toString, obj#8: java.lang.String
 +- Project [value#0]                                                                                                                                                     +- Project [value#0]
    +- Relation[value#0] text                                                                                                                                                +- Relation[value#0] text
          
23:54:11.564 [main] DEBUG o.a.s.s.i.BaseSessionStateBuilder$$anon$2 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#0 as string).toString, obj#8: java.lang.String   DeserializeToObject value#0.toString, obj#8: java.lang.String
!+- Project [value#0]                                                            +- Relation[value#0] text
!   +- Relation[value#0] text                                                    
          
23:54:11.575 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Pruning directories with: 
23:54:11.575 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Post-Scan Filters: 
23:54:11.575 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Output Data Schema: struct<value: string>
23:54:11.575 [main] INFO  o.a.s.s.execution.FileSourceScanExec - Pushed Filters: 
23:54:11.580 [main] DEBUG o.a.s.s.e.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

23:54:11.581 [main] DEBUG o.a.s.s.c.e.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

23:54:11.590 [main] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.747604 ms
23:54:11.612 [main] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 221.5 KB, free 1929.4 MB)
23:54:11.612 [main] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_2 locally took  5 ms
23:54:11.612 [main] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_2 without replication took  5 ms
23:54:11.621 [main] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1929.4 MB)
23:54:11.622 [dispatcher-event-loop-2] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.1.67:42085 (size: 20.7 KB, free: 1929.9 MB)
23:54:11.622 [main] DEBUG o.a.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
23:54:11.622 [main] DEBUG o.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
23:54:11.622 [main] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_2_piece0 locally took  1 ms
23:54:11.622 [main] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_2_piece0 without replication took  1 ms
23:54:11.622 [main] INFO  org.apache.spark.SparkContext - Created broadcast 2 from csv at SparkInit.java:35
23:54:11.623 [main] INFO  o.a.s.s.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23:54:11.624 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$doExecute$4$adapted
23:54:11.625 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
23:54:11.629 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$rdd$1
23:54:11.633 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$rdd$1) is now cleaned +++
23:54:11.646 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$inferFromDataset$2
23:54:11.646 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$inferFromDataset$2) is now cleaned +++
23:54:11.683 [main] DEBUG o.a.s.s.i.BaseSessionStateBuilder$$anon$1 - 
=== Result of Batch Cleanup ===
 Aggregate [count(1) AS count#41L]                                                                                                                                            Aggregate [count(1) AS count#41L]
 +- Relation[1#10,Eldon Base for stackable storage shelf, platinum#11,Muhammed MacIntyre#12,3#13,-213.25#14,38.94#15,35#16,Nunavut#17,Storage & Organization#18,0.8#19] csv   +- Relation[1#10,Eldon Base for stackable storage shelf, platinum#11,Muhammed MacIntyre#12,3#13,-213.25#14,38.94#15,35#16,Nunavut#17,Storage & Organization#18,0.8#19] csv
          
23:54:11.714 [main] DEBUG o.a.s.s.i.BaseSessionStateBuilder$$anon$2 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 Aggregate [count(1) AS count#41L]                                                                                                                                            Aggregate [count(1) AS count#41L]
!+- Relation[1#10,Eldon Base for stackable storage shelf, platinum#11,Muhammed MacIntyre#12,3#13,-213.25#14,38.94#15,35#16,Nunavut#17,Storage & Organization#18,0.8#19] csv   +- Project
!                                                                                                                                                                                +- Relation[1#10,Eldon Base for stackable storage shelf, platinum#11,Muhammed MacIntyre#12,3#13,-213.25#14,38.94#15,35#16,Nunavut#17,Storage & Organization#18,0.8#19] csv
          
23:54:11.747 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Pruning directories with: 
23:54:11.747 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Post-Scan Filters: 
23:54:11.747 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Output Data Schema: struct<>
23:54:11.748 [main] INFO  o.a.s.s.execution.FileSourceScanExec - Pushed Filters: 
23:54:11.786 [main] DEBUG o.a.s.s.e.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private scala.collection.Iterator inputadapter_input_0;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     inputadapter_input_0 = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 035 */       long inputadapter_value_0 = inputadapter_row_0.getLong(0);
/* 036 */
/* 037 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0);
/* 038 */       if (shouldStop()) return;
/* 039 */     }
/* 040 */
/* 041 */   }
/* 042 */
/* 043 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, long agg_expr_0_0) throws java.io.IOException {
/* 044 */     // do aggregate
/* 045 */     // common sub-expressions
/* 046 */
/* 047 */     // evaluate aggregate function
/* 048 */     long agg_value_3 = -1L;
/* 049 */     agg_value_3 = agg_bufValue_0 + agg_expr_0_0;
/* 050 */     // update aggregation buffer
/* 051 */     agg_bufIsNull_0 = false;
/* 052 */     agg_bufValue_0 = agg_value_3;
/* 053 */
/* 054 */   }
/* 055 */
/* 056 */   protected void processNext() throws java.io.IOException {
/* 057 */     while (!agg_initAgg_0) {
/* 058 */       agg_initAgg_0 = true;
/* 059 */       long agg_beforeAgg_0 = System.nanoTime();
/* 060 */       agg_doAggregateWithoutKey_0();
/* 061 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 062 */
/* 063 */       // output the result
/* 064 */
/* 065 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 066 */       agg_mutableStateArray_0[0].reset();
/* 067 */
/* 068 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 069 */
/* 070 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 071 */       append((agg_mutableStateArray_0[0].getRow()));
/* 072 */     }
/* 073 */   }
/* 074 */
/* 075 */ }

23:54:11.787 [main] DEBUG o.a.s.s.c.e.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private scala.collection.Iterator inputadapter_input_0;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     inputadapter_input_0 = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 035 */       long inputadapter_value_0 = inputadapter_row_0.getLong(0);
/* 036 */
/* 037 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0);
/* 038 */       if (shouldStop()) return;
/* 039 */     }
/* 040 */
/* 041 */   }
/* 042 */
/* 043 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, long agg_expr_0_0) throws java.io.IOException {
/* 044 */     // do aggregate
/* 045 */     // common sub-expressions
/* 046 */
/* 047 */     // evaluate aggregate function
/* 048 */     long agg_value_3 = -1L;
/* 049 */     agg_value_3 = agg_bufValue_0 + agg_expr_0_0;
/* 050 */     // update aggregation buffer
/* 051 */     agg_bufIsNull_0 = false;
/* 052 */     agg_bufValue_0 = agg_value_3;
/* 053 */
/* 054 */   }
/* 055 */
/* 056 */   protected void processNext() throws java.io.IOException {
/* 057 */     while (!agg_initAgg_0) {
/* 058 */       agg_initAgg_0 = true;
/* 059 */       long agg_beforeAgg_0 = System.nanoTime();
/* 060 */       agg_doAggregateWithoutKey_0();
/* 061 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 062 */
/* 063 */       // output the result
/* 064 */
/* 065 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 066 */       agg_mutableStateArray_0[0].reset();
/* 067 */
/* 068 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 069 */
/* 070 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 071 */       append((agg_mutableStateArray_0[0].getRow()));
/* 072 */     }
/* 073 */   }
/* 074 */
/* 075 */ }

23:54:11.798 [main] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 12.651663 ms
23:54:11.805 [main] DEBUG o.a.s.s.e.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 013 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     scan_mutableStateArray_0[0] = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 034 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 035 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 036 */       agg_doConsume_0(scan_row_0);
/* 037 */       if (shouldStop()) return;
/* 038 */     }
/* 039 */
/* 040 */   }
/* 041 */
/* 042 */   private void agg_doConsume_0(InternalRow scan_row_0) throws java.io.IOException {
/* 043 */     // do aggregate
/* 044 */     // common sub-expressions
/* 045 */
/* 046 */     // evaluate aggregate function
/* 047 */     long agg_value_1 = -1L;
/* 048 */     agg_value_1 = agg_bufValue_0 + 1L;
/* 049 */     // update aggregation buffer
/* 050 */     agg_bufIsNull_0 = false;
/* 051 */     agg_bufValue_0 = agg_value_1;
/* 052 */
/* 053 */   }
/* 054 */
/* 055 */   protected void processNext() throws java.io.IOException {
/* 056 */     while (!agg_initAgg_0) {
/* 057 */       agg_initAgg_0 = true;
/* 058 */       long agg_beforeAgg_0 = System.nanoTime();
/* 059 */       agg_doAggregateWithoutKey_0();
/* 060 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 061 */
/* 062 */       // output the result
/* 063 */
/* 064 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 065 */       agg_mutableStateArray_0[0].reset();
/* 066 */
/* 067 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 068 */
/* 069 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 070 */       append((agg_mutableStateArray_0[0].getRow()));
/* 071 */     }
/* 072 */   }
/* 073 */
/* 074 */ }

23:54:11.806 [main] DEBUG o.a.s.s.c.e.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 013 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     scan_mutableStateArray_0[0] = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 034 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 035 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 036 */       agg_doConsume_0(scan_row_0);
/* 037 */       if (shouldStop()) return;
/* 038 */     }
/* 039 */
/* 040 */   }
/* 041 */
/* 042 */   private void agg_doConsume_0(InternalRow scan_row_0) throws java.io.IOException {
/* 043 */     // do aggregate
/* 044 */     // common sub-expressions
/* 045 */
/* 046 */     // evaluate aggregate function
/* 047 */     long agg_value_1 = -1L;
/* 048 */     agg_value_1 = agg_bufValue_0 + 1L;
/* 049 */     // update aggregation buffer
/* 050 */     agg_bufIsNull_0 = false;
/* 051 */     agg_bufValue_0 = agg_value_1;
/* 052 */
/* 053 */   }
/* 054 */
/* 055 */   protected void processNext() throws java.io.IOException {
/* 056 */     while (!agg_initAgg_0) {
/* 057 */       agg_initAgg_0 = true;
/* 058 */       long agg_beforeAgg_0 = System.nanoTime();
/* 059 */       agg_doAggregateWithoutKey_0();
/* 060 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 061 */
/* 062 */       // output the result
/* 063 */
/* 064 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 065 */       agg_mutableStateArray_0[0].reset();
/* 066 */
/* 067 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 068 */
/* 069 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 070 */       append((agg_mutableStateArray_0[0].getRow()));
/* 071 */     }
/* 072 */   }
/* 073 */
/* 074 */ }

23:54:11.819 [main] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 13.474715 ms
23:54:11.842 [main] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 221.5 KB, free 1929.2 MB)
23:54:11.843 [main] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_3 locally took  6 ms
23:54:11.843 [main] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_3 without replication took  6 ms
23:54:11.854 [main] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1929.2 MB)
23:54:11.855 [dispatcher-event-loop-3] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.1.67:42085 (size: 20.7 KB, free: 1929.8 MB)
23:54:11.855 [main] DEBUG o.a.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
23:54:11.855 [main] DEBUG o.apache.spark.storage.BlockManager - Told master about block broadcast_3_piece0
23:54:11.855 [main] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_3_piece0 locally took  1 ms
23:54:11.855 [main] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_3_piece0 without replication took  1 ms
23:54:11.855 [main] INFO  org.apache.spark.SparkContext - Created broadcast 3 from count at SparkInit.java:36
23:54:11.858 [main] INFO  o.a.s.s.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23:54:11.872 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$doExecute$4$adapted
23:54:11.873 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
23:54:11.880 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$doExecute$4$adapted
23:54:11.881 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
23:54:11.884 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$collect$2
23:54:11.887 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$collect$2) is now cleaned +++
23:54:11.894 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$runJob$5
23:54:11.899 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$runJob$5) is now cleaned +++
23:54:11.901 [main] INFO  org.apache.spark.SparkContext - Starting job: count at SparkInit.java:36
23:54:11.904 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Registering RDD 11 (count at SparkInit.java:36)
23:54:11.906 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Got job 1 (count at SparkInit.java:36) with 1 output partitions
23:54:11.906 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (count at SparkInit.java:36)
23:54:11.906 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 1)
23:54:11.907 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 1)
23:54:11.908 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - submitStage(ResultStage 2)
23:54:11.926 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - missing: List(ShuffleMapStage 1)
23:54:11.927 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - submitStage(ShuffleMapStage 1)
23:54:11.928 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - missing: List()
23:54:11.928 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[11] at count at SparkInit.java:36), which has no missing parents
23:54:11.928 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - submitMissingTasks(ShuffleMapStage 1)
23:54:11.928 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(10)
23:54:11.929 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 10
23:54:11.930 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 10
23:54:11.931 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(26)
23:54:11.931 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 26
23:54:11.931 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 26
23:54:11.931 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(1)
23:54:11.931 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning broadcast 1
23:54:11.932 [Spark Context Cleaner] DEBUG o.a.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 1
23:54:11.946 [block-manager-slave-async-thread-pool-0] DEBUG o.a.s.s.BlockManagerSlaveEndpoint - removing broadcast 1
23:54:11.946 [block-manager-slave-async-thread-pool-0] DEBUG o.apache.spark.storage.BlockManager - Removing broadcast 1
23:54:11.948 [block-manager-slave-async-thread-pool-0] DEBUG o.apache.spark.storage.BlockManager - Removing block broadcast_1
23:54:11.949 [block-manager-slave-async-thread-pool-0] DEBUG o.a.spark.storage.memory.MemoryStore - Block broadcast_1 of size 10464 dropped from memory (free 2022897752)
23:54:11.950 [block-manager-slave-async-thread-pool-0] DEBUG o.apache.spark.storage.BlockManager - Removing block broadcast_1_piece0
23:54:11.951 [block-manager-slave-async-thread-pool-0] DEBUG o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 of size 5148 dropped from memory (free 2022902900)
23:54:11.952 [dispatcher-event-loop-6] INFO  o.a.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.1.67:42085 in memory (size: 5.0 KB, free: 1929.8 MB)
23:54:11.952 [block-manager-slave-async-thread-pool-0] DEBUG o.a.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
23:54:11.952 [block-manager-slave-async-thread-pool-0] DEBUG o.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
23:54:11.955 [block-manager-slave-async-thread-pool-2] DEBUG o.a.s.s.BlockManagerSlaveEndpoint - Done removing broadcast 1, response is 0
23:54:11.956 [block-manager-slave-async-thread-pool-2] DEBUG o.a.s.s.BlockManagerSlaveEndpoint - Sent response: 0 to 192.168.1.67:40325
23:54:11.957 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned broadcast 1
23:54:11.957 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(4)
23:54:11.957 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 4
23:54:11.957 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 4
23:54:11.957 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(34)
23:54:11.957 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 34
23:54:11.957 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 34
23:54:11.957 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(29)
23:54:11.957 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 29
23:54:11.957 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 29
23:54:11.957 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(15)
23:54:11.957 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 15
23:54:11.957 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 15
23:54:11.957 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(20)
23:54:11.957 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 20
23:54:11.957 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 20
23:54:11.957 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(22)
23:54:11.957 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 22
23:54:11.957 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 22
23:54:11.958 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(0)
23:54:11.958 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning broadcast 0
23:54:11.958 [Spark Context Cleaner] DEBUG o.a.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 0
23:54:11.960 [block-manager-slave-async-thread-pool-3] DEBUG o.a.s.s.BlockManagerSlaveEndpoint - removing broadcast 0
23:54:11.960 [block-manager-slave-async-thread-pool-3] DEBUG o.apache.spark.storage.BlockManager - Removing broadcast 0
23:54:11.960 [block-manager-slave-async-thread-pool-3] DEBUG o.apache.spark.storage.BlockManager - Removing block broadcast_0_piece0
23:54:11.960 [block-manager-slave-async-thread-pool-3] DEBUG o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 of size 21158 dropped from memory (free 2022924058)
23:54:11.961 [dispatcher-event-loop-1] INFO  o.a.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.1.67:42085 in memory (size: 20.7 KB, free: 1929.9 MB)
23:54:11.961 [block-manager-slave-async-thread-pool-3] DEBUG o.a.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
23:54:11.961 [block-manager-slave-async-thread-pool-3] DEBUG o.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
23:54:11.961 [block-manager-slave-async-thread-pool-3] DEBUG o.apache.spark.storage.BlockManager - Removing block broadcast_0
23:54:11.961 [block-manager-slave-async-thread-pool-3] DEBUG o.a.spark.storage.memory.MemoryStore - Block broadcast_0 of size 226816 dropped from memory (free 2023136906)
23:54:11.961 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 13.6 KB, free 1929.2 MB)
23:54:11.962 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_4 locally took  1 ms
23:54:11.962 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_4 without replication took  1 ms
23:54:11.964 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.1 KB, free 1929.4 MB)
23:54:11.964 [dispatcher-event-loop-2] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.1.67:42085 (size: 7.1 KB, free: 1929.9 MB)
23:54:11.964 [dag-scheduler-event-loop] DEBUG o.a.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
23:54:11.964 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Told master about block broadcast_4_piece0
23:54:11.964 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_4_piece0 locally took  1 ms
23:54:11.965 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_4_piece0 without replication took  2 ms
23:54:11.965 [block-manager-slave-async-thread-pool-5] DEBUG o.a.s.s.BlockManagerSlaveEndpoint - Done removing broadcast 0, response is 0
23:54:11.965 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1161
23:54:11.965 [block-manager-slave-async-thread-pool-5] DEBUG o.a.s.s.BlockManagerSlaveEndpoint - Sent response: 0 to 192.168.1.67:40325
23:54:11.968 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[11] at count at SparkInit.java:36) (first 15 tasks are for partitions Vector(0))
23:54:11.968 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
23:54:11.968 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned broadcast 0
23:54:11.968 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(12)
23:54:11.968 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 12
23:54:11.968 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.TaskSetManager - Epoch for TaskSet 1.0: 0
23:54:11.969 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 12
23:54:11.969 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(1)
23:54:11.969 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 1.0: NO_PREF, ANY
23:54:11.969 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 1
23:54:11.969 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 1
23:54:11.969 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(8)
23:54:11.969 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 8
23:54:11.969 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 8
23:54:11.969 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(2)
23:54:11.969 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning broadcast 2
23:54:11.969 [Spark Context Cleaner] DEBUG o.a.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 2
23:54:11.969 [dispatcher-event-loop-3] DEBUG o.a.s.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
23:54:11.971 [dispatcher-event-loop-3] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7836 bytes)
23:54:11.971 [dispatcher-event-loop-3] DEBUG o.a.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
23:54:11.971 [Executor task launch worker for task 1] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
23:54:11.972 [block-manager-slave-async-thread-pool-6] DEBUG o.a.s.s.BlockManagerSlaveEndpoint - removing broadcast 2
23:54:11.972 [block-manager-slave-async-thread-pool-6] DEBUG o.apache.spark.storage.BlockManager - Removing broadcast 2
23:54:11.972 [block-manager-slave-async-thread-pool-6] DEBUG o.apache.spark.storage.BlockManager - Removing block broadcast_2
23:54:11.973 [block-manager-slave-async-thread-pool-6] DEBUG o.a.spark.storage.memory.MemoryStore - Block broadcast_2 of size 226816 dropped from memory (free 2023356451)
23:54:11.973 [block-manager-slave-async-thread-pool-6] DEBUG o.apache.spark.storage.BlockManager - Removing block broadcast_2_piece0
23:54:11.973 [block-manager-slave-async-thread-pool-6] DEBUG o.a.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 of size 21158 dropped from memory (free 2023377609)
23:54:11.974 [dispatcher-event-loop-7] INFO  o.a.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.1.67:42085 in memory (size: 20.7 KB, free: 1929.9 MB)
23:54:11.974 [block-manager-slave-async-thread-pool-6] DEBUG o.a.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
23:54:11.974 [block-manager-slave-async-thread-pool-6] DEBUG o.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
23:54:11.974 [Executor task launch worker for task 1] DEBUG o.apache.spark.storage.BlockManager - Getting local block broadcast_4
23:54:11.975 [Executor task launch worker for task 1] DEBUG o.apache.spark.storage.BlockManager - Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
23:54:11.975 [block-manager-slave-async-thread-pool-8] DEBUG o.a.s.s.BlockManagerSlaveEndpoint - Done removing broadcast 2, response is 0
23:54:11.984 [block-manager-slave-async-thread-pool-8] DEBUG o.a.s.s.BlockManagerSlaveEndpoint - Sent response: 0 to 192.168.1.67:40325
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned broadcast 2
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(9)
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 9
23:54:11.985 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 9
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(19)
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 19
23:54:11.985 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 19
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(5)
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 5
23:54:11.985 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 5
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(36)
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 36
23:54:11.985 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 36
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(27)
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 27
23:54:11.985 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 27
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(11)
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 11
23:54:11.985 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 11
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(25)
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 25
23:54:11.985 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 25
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(16)
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 16
23:54:11.985 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 16
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(17)
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 17
23:54:11.985 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 17
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(13)
23:54:11.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 13
23:54:11.986 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 13
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(0)
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 0
23:54:11.986 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 0
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(18)
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 18
23:54:11.986 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 18
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(7)
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 7
23:54:11.986 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 7
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(21)
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 21
23:54:11.986 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 21
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(28)
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 28
23:54:11.986 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 28
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(6)
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 6
23:54:11.986 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 6
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(33)
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 33
23:54:11.986 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 33
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(31)
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 31
23:54:11.986 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 31
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(23)
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 23
23:54:11.986 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 23
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(35)
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 35
23:54:11.986 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 35
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(24)
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 24
23:54:11.986 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 24
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(14)
23:54:11.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 14
23:54:11.987 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 14
23:54:11.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(30)
23:54:11.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 30
23:54:11.987 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 30
23:54:11.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(2)
23:54:11.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 2
23:54:11.987 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 2
23:54:11.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(32)
23:54:11.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 32
23:54:11.987 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 32
23:54:11.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(3)
23:54:11.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 3
23:54:11.987 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 3
23:54:12.004 [Executor task launch worker for task 1] INFO  o.a.s.s.e.datasources.FileScanRDD - Reading File path: file:///home/fabrice/workspace/sparkexamples/gradle.template/build/resources/main/test.csv, range: 0-121306, partition values: [empty row]
23:54:12.006 [Executor task launch worker for task 1] DEBUG o.a.s.s.c.e.c.GenerateUnsafeProjection - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

23:54:12.006 [Executor task launch worker for task 1] DEBUG o.a.s.s.c.e.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

23:54:12.015 [Executor task launch worker for task 1] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.628366 ms
23:54:12.016 [Executor task launch worker for task 1] DEBUG o.apache.spark.storage.BlockManager - Getting local block broadcast_3
23:54:12.016 [Executor task launch worker for task 1] DEBUG o.apache.spark.storage.BlockManager - Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
23:54:12.073 [Executor task launch worker for task 1] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 1598 bytes result sent to driver
23:54:12.073 [dispatcher-event-loop-0] DEBUG o.a.s.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
23:54:12.075 [task-result-getter-1] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 106 ms on localhost (executor driver) (1/1)
23:54:12.075 [task-result-getter-1] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
23:54:12.076 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
23:54:12.077 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (count at SparkInit.java:36) finished in 0.146 s
23:54:12.077 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
23:54:12.077 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - running: Set()
23:54:12.078 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
23:54:12.078 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - failed: Set()
23:54:12.078 [dag-scheduler-event-loop] DEBUG o.a.spark.MapOutputTrackerMaster - Increasing epoch to 1
23:54:12.081 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - submitStage(ResultStage 2)
23:54:12.081 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - missing: List()
23:54:12.081 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[14] at count at SparkInit.java:36), which has no missing parents
23:54:12.081 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 2)
23:54:12.089 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 8.5 KB, free 1929.6 MB)
23:54:12.089 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_5 locally took  1 ms
23:54:12.090 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_5 without replication took  1 ms
23:54:12.091 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.4 KB, free 1929.6 MB)
23:54:12.092 [dispatcher-event-loop-1] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.1.67:42085 (size: 4.4 KB, free: 1929.9 MB)
23:54:12.092 [dag-scheduler-event-loop] DEBUG o.a.spark.storage.BlockManagerMaster - Updated info of block broadcast_5_piece0
23:54:12.092 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Told master about block broadcast_5_piece0
23:54:12.093 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_5_piece0 locally took  2 ms
23:54:12.093 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_5_piece0 without replication took  2 ms
23:54:12.093 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1161
23:54:12.094 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at count at SparkInit.java:36) (first 15 tasks are for partitions Vector(0))
23:54:12.094 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
23:54:12.094 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.TaskSetManager - Epoch for TaskSet 2.0: 1
23:54:12.097 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 2.0: ANY
23:54:12.097 [dispatcher-event-loop-2] DEBUG o.a.s.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
23:54:12.098 [dispatcher-event-loop-2] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7246 bytes)
23:54:12.100 [Executor task launch worker for task 2] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
23:54:12.102 [Executor task launch worker for task 2] DEBUG o.apache.spark.storage.BlockManager - Getting local block broadcast_5
23:54:12.102 [Executor task launch worker for task 2] DEBUG o.apache.spark.storage.BlockManager - Level for block broadcast_5 is StorageLevel(disk, memory, deserialized, 1 replicas)
23:54:12.114 [Executor task launch worker for task 2] DEBUG o.a.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 0, partitions 0-1
23:54:12.122 [Executor task launch worker for task 2] DEBUG o.a.s.s.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
23:54:12.125 [Executor task launch worker for task 2] INFO  o.a.s.s.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
23:54:12.128 [Executor task launch worker for task 2] INFO  o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms
23:54:12.129 [Executor task launch worker for task 2] DEBUG o.a.s.s.ShuffleBlockFetcherIterator - Start fetching local blocks: shuffle_0_0_0
23:54:12.139 [Executor task launch worker for task 2] DEBUG o.a.s.s.ShuffleBlockFetcherIterator - Got local blocks in  18 ms
23:54:12.157 [Executor task launch worker for task 2] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1792 bytes result sent to driver
23:54:12.158 [dispatcher-event-loop-4] DEBUG o.a.s.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
23:54:12.162 [task-result-getter-2] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 65 ms on localhost (executor driver) (1/1)
23:54:12.163 [task-result-getter-2] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
23:54:12.164 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ResultStage 2 (count at SparkInit.java:36) finished in 0.077 s
23:54:12.165 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - After removal of stage 2, remaining stages = 1
23:54:12.165 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - After removal of stage 1, remaining stages = 0
23:54:12.165 [main] INFO  o.a.spark.scheduler.DAGScheduler - Job 1 finished: count at SparkInit.java:36, took 0.264694 s
23:54:12.169 [main] INFO  com.spark.examples.gradle.SparkInit - record load fom csv :999
23:54:12.232 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Pruning directories with: 
23:54:12.232 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Post-Scan Filters: 
23:54:12.233 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Output Data Schema: struct<1: string, Eldon Base for stackable storage shelf, platinum: string, Muhammed MacIntyre: string, 3: string, -213.25: string ... 8 more fields>
23:54:12.233 [main] INFO  o.a.s.s.execution.FileSourceScanExec - Pushed Filters: 
23:54:12.253 [main] DEBUG o.a.s.internal.io.FileCommitProtocol - Creating committer org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol; job a4e9dedc-e3ad-41f8-b5d4-5e8be045ee41; output=file:/home/fabrice/Documents/jsonOut; dynamic=false
23:54:12.255 [main] DEBUG o.a.s.internal.io.FileCommitProtocol - Using (String, String, Boolean) constructor
23:54:12.282 [main] INFO  o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23:54:12.286 [main] DEBUG o.a.s.s.e.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

23:54:12.308 [main] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 221.5 KB, free 1929.4 MB)
23:54:12.309 [main] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_6 locally took  6 ms
23:54:12.309 [main] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_6 without replication took  6 ms
23:54:12.319 [main] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1929.4 MB)
23:54:12.319 [dispatcher-event-loop-6] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.1.67:42085 (size: 20.7 KB, free: 1929.8 MB)
23:54:12.319 [main] DEBUG o.a.spark.storage.BlockManagerMaster - Updated info of block broadcast_6_piece0
23:54:12.319 [main] DEBUG o.apache.spark.storage.BlockManager - Told master about block broadcast_6_piece0
23:54:12.319 [main] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_6_piece0 locally took  1 ms
23:54:12.320 [main] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_6_piece0 without replication took  2 ms
23:54:12.320 [main] INFO  org.apache.spark.SparkContext - Created broadcast 6 from json at SparkInit.java:37
23:54:12.321 [main] INFO  o.a.s.s.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23:54:12.322 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$doExecute$4$adapted
23:54:12.323 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
23:54:12.325 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$write$15
23:54:12.326 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$write$15) is now cleaned +++
23:54:12.348 [main] INFO  org.apache.spark.SparkContext - Starting job: json at SparkInit.java:37
23:54:12.349 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Got job 2 (json at SparkInit.java:37) with 1 output partitions
23:54:12.349 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (json at SparkInit.java:37)
23:54:12.349 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
23:54:12.349 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Missing parents: List()
23:54:12.349 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - submitStage(ResultStage 3)
23:54:12.349 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - missing: List()
23:54:12.349 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[16] at json at SparkInit.java:37), which has no missing parents
23:54:12.349 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 3)
23:54:12.363 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 136.2 KB, free 1929.3 MB)
23:54:12.363 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_7 locally took  1 ms
23:54:12.363 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_7 without replication took  1 ms
23:54:12.365 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 49.6 KB, free 1929.2 MB)
23:54:12.365 [dispatcher-event-loop-3] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.1.67:42085 (size: 49.6 KB, free: 1929.8 MB)
23:54:12.365 [dag-scheduler-event-loop] DEBUG o.a.spark.storage.BlockManagerMaster - Updated info of block broadcast_7_piece0
23:54:12.365 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Told master about block broadcast_7_piece0
23:54:12.366 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_7_piece0 locally took  2 ms
23:54:12.366 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_7_piece0 without replication took  2 ms
23:54:12.366 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1161
23:54:12.366 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[16] at json at SparkInit.java:37) (first 15 tasks are for partitions Vector(0))
23:54:12.366 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
23:54:12.367 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.TaskSetManager - Epoch for TaskSet 3.0: 1
23:54:12.367 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 3.0: NO_PREF, ANY
23:54:12.367 [dispatcher-event-loop-7] DEBUG o.a.s.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_3.0, runningTasks: 0
23:54:12.368 [dispatcher-event-loop-7] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7847 bytes)
23:54:12.368 [dispatcher-event-loop-7] DEBUG o.a.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
23:54:12.368 [Executor task launch worker for task 3] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
23:54:12.369 [Executor task launch worker for task 3] DEBUG o.apache.spark.storage.BlockManager - Getting local block broadcast_7
23:54:12.370 [Executor task launch worker for task 3] DEBUG o.apache.spark.storage.BlockManager - Level for block broadcast_7 is StorageLevel(disk, memory, deserialized, 1 replicas)
23:54:12.406 [Executor task launch worker for task 3] INFO  o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23:54:12.425 [Executor task launch worker for task 3] INFO  o.a.s.s.e.datasources.FileScanRDD - Reading File path: file:///home/fabrice/workspace/sparkexamples/gradle.template/build/resources/main/test.csv, range: 0-121306, partition values: [empty row]
23:54:12.433 [Executor task launch worker for task 3] DEBUG o.a.s.s.c.e.c.GenerateUnsafeProjection - code for input[0, string, true],input[1, string, true],input[2, string, true],input[3, string, true],input[4, string, true],input[5, string, true],input[6, string, true],input[7, string, true],input[8, string, true],input[9, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(10, 320);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_6 = i.isNullAt(6);
/* 048 */     UTF8String value_6 = isNull_6 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_6) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_6);
/* 054 */     }
/* 055 */
/* 056 */     boolean isNull_7 = i.isNullAt(7);
/* 057 */     UTF8String value_7 = isNull_7 ?
/* 058 */     null : (i.getUTF8String(7));
/* 059 */     if (isNull_7) {
/* 060 */       mutableStateArray_0[0].setNullAt(7);
/* 061 */     } else {
/* 062 */       mutableStateArray_0[0].write(7, value_7);
/* 063 */     }
/* 064 */
/* 065 */     boolean isNull_8 = i.isNullAt(8);
/* 066 */     UTF8String value_8 = isNull_8 ?
/* 067 */     null : (i.getUTF8String(8));
/* 068 */     if (isNull_8) {
/* 069 */       mutableStateArray_0[0].setNullAt(8);
/* 070 */     } else {
/* 071 */       mutableStateArray_0[0].write(8, value_8);
/* 072 */     }
/* 073 */
/* 074 */     boolean isNull_9 = i.isNullAt(9);
/* 075 */     UTF8String value_9 = isNull_9 ?
/* 076 */     null : (i.getUTF8String(9));
/* 077 */     if (isNull_9) {
/* 078 */       mutableStateArray_0[0].setNullAt(9);
/* 079 */     } else {
/* 080 */       mutableStateArray_0[0].write(9, value_9);
/* 081 */     }
/* 082 */
/* 083 */   }
/* 084 */
/* 085 */
/* 086 */   private void writeFields_0_0(InternalRow i) {
/* 087 */
/* 088 */     boolean isNull_0 = i.isNullAt(0);
/* 089 */     UTF8String value_0 = isNull_0 ?
/* 090 */     null : (i.getUTF8String(0));
/* 091 */     if (isNull_0) {
/* 092 */       mutableStateArray_0[0].setNullAt(0);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(0, value_0);
/* 095 */     }
/* 096 */
/* 097 */     boolean isNull_1 = i.isNullAt(1);
/* 098 */     UTF8String value_1 = isNull_1 ?
/* 099 */     null : (i.getUTF8String(1));
/* 100 */     if (isNull_1) {
/* 101 */       mutableStateArray_0[0].setNullAt(1);
/* 102 */     } else {
/* 103 */       mutableStateArray_0[0].write(1, value_1);
/* 104 */     }
/* 105 */
/* 106 */     boolean isNull_2 = i.isNullAt(2);
/* 107 */     UTF8String value_2 = isNull_2 ?
/* 108 */     null : (i.getUTF8String(2));
/* 109 */     if (isNull_2) {
/* 110 */       mutableStateArray_0[0].setNullAt(2);
/* 111 */     } else {
/* 112 */       mutableStateArray_0[0].write(2, value_2);
/* 113 */     }
/* 114 */
/* 115 */     boolean isNull_3 = i.isNullAt(3);
/* 116 */     UTF8String value_3 = isNull_3 ?
/* 117 */     null : (i.getUTF8String(3));
/* 118 */     if (isNull_3) {
/* 119 */       mutableStateArray_0[0].setNullAt(3);
/* 120 */     } else {
/* 121 */       mutableStateArray_0[0].write(3, value_3);
/* 122 */     }
/* 123 */
/* 124 */     boolean isNull_4 = i.isNullAt(4);
/* 125 */     UTF8String value_4 = isNull_4 ?
/* 126 */     null : (i.getUTF8String(4));
/* 127 */     if (isNull_4) {
/* 128 */       mutableStateArray_0[0].setNullAt(4);
/* 129 */     } else {
/* 130 */       mutableStateArray_0[0].write(4, value_4);
/* 131 */     }
/* 132 */
/* 133 */   }
/* 134 */
/* 135 */ }

23:54:12.435 [Executor task launch worker for task 3] DEBUG o.a.s.s.c.e.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(10, 320);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_6 = i.isNullAt(6);
/* 048 */     UTF8String value_6 = isNull_6 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_6) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_6);
/* 054 */     }
/* 055 */
/* 056 */     boolean isNull_7 = i.isNullAt(7);
/* 057 */     UTF8String value_7 = isNull_7 ?
/* 058 */     null : (i.getUTF8String(7));
/* 059 */     if (isNull_7) {
/* 060 */       mutableStateArray_0[0].setNullAt(7);
/* 061 */     } else {
/* 062 */       mutableStateArray_0[0].write(7, value_7);
/* 063 */     }
/* 064 */
/* 065 */     boolean isNull_8 = i.isNullAt(8);
/* 066 */     UTF8String value_8 = isNull_8 ?
/* 067 */     null : (i.getUTF8String(8));
/* 068 */     if (isNull_8) {
/* 069 */       mutableStateArray_0[0].setNullAt(8);
/* 070 */     } else {
/* 071 */       mutableStateArray_0[0].write(8, value_8);
/* 072 */     }
/* 073 */
/* 074 */     boolean isNull_9 = i.isNullAt(9);
/* 075 */     UTF8String value_9 = isNull_9 ?
/* 076 */     null : (i.getUTF8String(9));
/* 077 */     if (isNull_9) {
/* 078 */       mutableStateArray_0[0].setNullAt(9);
/* 079 */     } else {
/* 080 */       mutableStateArray_0[0].write(9, value_9);
/* 081 */     }
/* 082 */
/* 083 */   }
/* 084 */
/* 085 */
/* 086 */   private void writeFields_0_0(InternalRow i) {
/* 087 */
/* 088 */     boolean isNull_0 = i.isNullAt(0);
/* 089 */     UTF8String value_0 = isNull_0 ?
/* 090 */     null : (i.getUTF8String(0));
/* 091 */     if (isNull_0) {
/* 092 */       mutableStateArray_0[0].setNullAt(0);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(0, value_0);
/* 095 */     }
/* 096 */
/* 097 */     boolean isNull_1 = i.isNullAt(1);
/* 098 */     UTF8String value_1 = isNull_1 ?
/* 099 */     null : (i.getUTF8String(1));
/* 100 */     if (isNull_1) {
/* 101 */       mutableStateArray_0[0].setNullAt(1);
/* 102 */     } else {
/* 103 */       mutableStateArray_0[0].write(1, value_1);
/* 104 */     }
/* 105 */
/* 106 */     boolean isNull_2 = i.isNullAt(2);
/* 107 */     UTF8String value_2 = isNull_2 ?
/* 108 */     null : (i.getUTF8String(2));
/* 109 */     if (isNull_2) {
/* 110 */       mutableStateArray_0[0].setNullAt(2);
/* 111 */     } else {
/* 112 */       mutableStateArray_0[0].write(2, value_2);
/* 113 */     }
/* 114 */
/* 115 */     boolean isNull_3 = i.isNullAt(3);
/* 116 */     UTF8String value_3 = isNull_3 ?
/* 117 */     null : (i.getUTF8String(3));
/* 118 */     if (isNull_3) {
/* 119 */       mutableStateArray_0[0].setNullAt(3);
/* 120 */     } else {
/* 121 */       mutableStateArray_0[0].write(3, value_3);
/* 122 */     }
/* 123 */
/* 124 */     boolean isNull_4 = i.isNullAt(4);
/* 125 */     UTF8String value_4 = isNull_4 ?
/* 126 */     null : (i.getUTF8String(4));
/* 127 */     if (isNull_4) {
/* 128 */       mutableStateArray_0[0].setNullAt(4);
/* 129 */     } else {
/* 130 */       mutableStateArray_0[0].write(4, value_4);
/* 131 */     }
/* 132 */
/* 133 */   }
/* 134 */
/* 135 */ }

23:54:12.456 [Executor task launch worker for task 3] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 22.780289 ms
23:54:12.457 [Executor task launch worker for task 3] DEBUG o.apache.spark.storage.BlockManager - Getting local block broadcast_6
23:54:12.457 [Executor task launch worker for task 3] DEBUG o.apache.spark.storage.BlockManager - Level for block broadcast_6 is StorageLevel(disk, memory, deserialized, 1 replicas)
23:54:12.534 [dispatcher-event-loop-1] DEBUG o.a.s.s.OutputCommitCoordinator - Commit allowed for stage=3.0, partition=0, task attempt 0
23:54:12.535 [Executor task launch worker for task 3] INFO  o.a.h.m.l.output.FileOutputCommitter - Saved output of task 'attempt_20190312235412_0003_m_000000_0' to file:/home/fabrice/Documents/jsonOut/_temporary/0/task_20190312235412_0003_m_000000
23:54:12.535 [Executor task launch worker for task 3] INFO  o.a.s.mapred.SparkHadoopMapRedUtil - attempt_20190312235412_0003_m_000000_0: Committed
23:54:12.539 [Executor task launch worker for task 3] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2169 bytes result sent to driver
23:54:12.539 [dispatcher-event-loop-2] DEBUG o.a.s.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_3.0, runningTasks: 0
23:54:12.540 [task-result-getter-3] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 173 ms on localhost (executor driver) (1/1)
23:54:12.540 [task-result-getter-3] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
23:54:12.541 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ResultStage 3 (json at SparkInit.java:37) finished in 0.190 s
23:54:12.541 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - After removal of stage 3, remaining stages = 0
23:54:12.541 [main] INFO  o.a.spark.scheduler.DAGScheduler - Job 2 finished: json at SparkInit.java:37, took 0.192941 s
23:54:12.543 [main] DEBUG o.a.h.m.l.output.FileOutputCommitter - Merging data from DeprecatedRawLocalFileStatus{path=file:/home/fabrice/Documents/jsonOut/_temporary/0/task_20190312235412_0003_m_000000; isDirectory=true; modification_time=1552431252000; access_time=0; owner=; group=; permission=rwxrwxrwx; isSymlink=false} to file:/home/fabrice/Documents/jsonOut
23:54:12.543 [main] DEBUG o.a.h.m.l.output.FileOutputCommitter - Merging data from DeprecatedRawLocalFileStatus{path=file:/home/fabrice/Documents/jsonOut/_temporary/0/task_20190312235412_0003_m_000000/part-00000-a4e9dedc-e3ad-41f8-b5d4-5e8be045ee41-c000.json; isDirectory=false; length=285899; replication=1; blocksize=33554432; modification_time=1552431252000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false} to file:/home/fabrice/Documents/jsonOut/part-00000-a4e9dedc-e3ad-41f8-b5d4-5e8be045ee41-c000.json
23:54:12.549 [main] DEBUG o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol - Committing files staged for absolute locations Map()
23:54:12.550 [main] INFO  o.a.s.s.e.d.FileFormatWriter - Write Job be481386-e220-41a7-aad5-3e941ee81cfc committed.
23:54:12.554 [main] INFO  o.a.s.s.e.d.FileFormatWriter - Finished processing stats for write job be481386-e220-41a7-aad5-3e941ee81cfc.
23:54:12.791 [main] DEBUG o.a.s.s.i.BaseSessionStateBuilder$$anon$1 - 
=== Result of Batch Cleanup ===
 Aggregate [count(1) AS count#114L]                                                                                                                                                                              Aggregate [count(1) AS count#114L]
 +- Relation[EMP_TYPE#65,id#66L,ville#67,state#68,rue#69,zip#70,age#71,COMM#72,dob#73,employeeType#74,first_name#75,name#76,PHONE#77,PIC#78,startDate#79,dept_id#80L] JDBCRelation(Employee) [numPartitions=1]   +- Relation[EMP_TYPE#65,id#66L,ville#67,state#68,rue#69,zip#70,age#71,COMM#72,dob#73,employeeType#74,first_name#75,name#76,PHONE#77,PIC#78,startDate#79,dept_id#80L] JDBCRelation(Employee) [numPartitions=1]
          
23:54:12.803 [main] DEBUG o.a.s.s.i.BaseSessionStateBuilder$$anon$2 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 Aggregate [count(1) AS count#114L]                                                                                                                                                                              Aggregate [count(1) AS count#114L]
!+- Relation[EMP_TYPE#65,id#66L,ville#67,state#68,rue#69,zip#70,age#71,COMM#72,dob#73,employeeType#74,first_name#75,name#76,PHONE#77,PIC#78,startDate#79,dept_id#80L] JDBCRelation(Employee) [numPartitions=1]   +- Project
!                                                                                                                                                                                                                   +- Relation[EMP_TYPE#65,id#66L,ville#67,state#68,rue#69,zip#70,age#71,COMM#72,dob#73,employeeType#74,first_name#75,name#76,PHONE#77,PIC#78,startDate#79,dept_id#80L] JDBCRelation(Employee) [numPartitions=1]
          
23:54:12.832 [main] DEBUG o.a.s.s.e.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private scala.collection.Iterator inputadapter_input_0;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     inputadapter_input_0 = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 035 */       long inputadapter_value_0 = inputadapter_row_0.getLong(0);
/* 036 */
/* 037 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0);
/* 038 */       if (shouldStop()) return;
/* 039 */     }
/* 040 */
/* 041 */   }
/* 042 */
/* 043 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, long agg_expr_0_0) throws java.io.IOException {
/* 044 */     // do aggregate
/* 045 */     // common sub-expressions
/* 046 */
/* 047 */     // evaluate aggregate function
/* 048 */     long agg_value_3 = -1L;
/* 049 */     agg_value_3 = agg_bufValue_0 + agg_expr_0_0;
/* 050 */     // update aggregation buffer
/* 051 */     agg_bufIsNull_0 = false;
/* 052 */     agg_bufValue_0 = agg_value_3;
/* 053 */
/* 054 */   }
/* 055 */
/* 056 */   protected void processNext() throws java.io.IOException {
/* 057 */     while (!agg_initAgg_0) {
/* 058 */       agg_initAgg_0 = true;
/* 059 */       long agg_beforeAgg_0 = System.nanoTime();
/* 060 */       agg_doAggregateWithoutKey_0();
/* 061 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 062 */
/* 063 */       // output the result
/* 064 */
/* 065 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 066 */       agg_mutableStateArray_0[0].reset();
/* 067 */
/* 068 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 069 */
/* 070 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 071 */       append((agg_mutableStateArray_0[0].getRow()));
/* 072 */     }
/* 073 */   }
/* 074 */
/* 075 */ }

23:54:12.839 [main] DEBUG o.a.s.s.e.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 013 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     scan_mutableStateArray_0[0] = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 034 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 035 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 036 */       agg_doConsume_0();
/* 037 */       if (shouldStop()) return;
/* 038 */     }
/* 039 */
/* 040 */   }
/* 041 */
/* 042 */   private void agg_doConsume_0() throws java.io.IOException {
/* 043 */     // do aggregate
/* 044 */     // common sub-expressions
/* 045 */
/* 046 */     // evaluate aggregate function
/* 047 */     long agg_value_1 = -1L;
/* 048 */     agg_value_1 = agg_bufValue_0 + 1L;
/* 049 */     // update aggregation buffer
/* 050 */     agg_bufIsNull_0 = false;
/* 051 */     agg_bufValue_0 = agg_value_1;
/* 052 */
/* 053 */   }
/* 054 */
/* 055 */   protected void processNext() throws java.io.IOException {
/* 056 */     while (!agg_initAgg_0) {
/* 057 */       agg_initAgg_0 = true;
/* 058 */       long agg_beforeAgg_0 = System.nanoTime();
/* 059 */       agg_doAggregateWithoutKey_0();
/* 060 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 061 */
/* 062 */       // output the result
/* 063 */
/* 064 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 065 */       agg_mutableStateArray_0[0].reset();
/* 066 */
/* 067 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 068 */
/* 069 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 070 */       append((agg_mutableStateArray_0[0].getRow()));
/* 071 */     }
/* 072 */   }
/* 073 */
/* 074 */ }

23:54:12.839 [main] DEBUG o.a.s.s.c.e.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 013 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     scan_mutableStateArray_0[0] = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 034 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 035 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 036 */       agg_doConsume_0();
/* 037 */       if (shouldStop()) return;
/* 038 */     }
/* 039 */
/* 040 */   }
/* 041 */
/* 042 */   private void agg_doConsume_0() throws java.io.IOException {
/* 043 */     // do aggregate
/* 044 */     // common sub-expressions
/* 045 */
/* 046 */     // evaluate aggregate function
/* 047 */     long agg_value_1 = -1L;
/* 048 */     agg_value_1 = agg_bufValue_0 + 1L;
/* 049 */     // update aggregation buffer
/* 050 */     agg_bufIsNull_0 = false;
/* 051 */     agg_bufValue_0 = agg_value_1;
/* 052 */
/* 053 */   }
/* 054 */
/* 055 */   protected void processNext() throws java.io.IOException {
/* 056 */     while (!agg_initAgg_0) {
/* 057 */       agg_initAgg_0 = true;
/* 058 */       long agg_beforeAgg_0 = System.nanoTime();
/* 059 */       agg_doAggregateWithoutKey_0();
/* 060 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 061 */
/* 062 */       // output the result
/* 063 */
/* 064 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 065 */       agg_mutableStateArray_0[0].reset();
/* 066 */
/* 067 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 068 */
/* 069 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 070 */       append((agg_mutableStateArray_0[0].getRow()));
/* 071 */     }
/* 072 */   }
/* 073 */
/* 074 */ }

23:54:12.848 [main] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.523683 ms
23:54:12.849 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$doExecute$4$adapted
23:54:12.849 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
23:54:12.851 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$doExecute$4$adapted
23:54:12.852 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
23:54:12.853 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$collect$2
23:54:12.855 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$collect$2) is now cleaned +++
23:54:12.856 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$runJob$5
23:54:12.859 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$runJob$5) is now cleaned +++
23:54:12.859 [main] INFO  org.apache.spark.SparkContext - Starting job: count at SparkInit.java:47
23:54:12.860 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Registering RDD 20 (count at SparkInit.java:47)
23:54:12.860 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Got job 3 (count at SparkInit.java:47) with 1 output partitions
23:54:12.860 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (count at SparkInit.java:47)
23:54:12.860 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
23:54:12.860 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 4)
23:54:12.861 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - submitStage(ResultStage 5)
23:54:12.861 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - missing: List(ShuffleMapStage 4)
23:54:12.861 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - submitStage(ShuffleMapStage 4)
23:54:12.861 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - missing: List()
23:54:12.861 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[20] at count at SparkInit.java:47), which has no missing parents
23:54:12.861 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - submitMissingTasks(ShuffleMapStage 4)
23:54:12.866 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 10.5 KB, free 1929.2 MB)
23:54:12.866 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_8 locally took  1 ms
23:54:12.866 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_8 without replication took  1 ms
23:54:12.867 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.5 KB, free 1929.2 MB)
23:54:12.868 [dispatcher-event-loop-5] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 192.168.1.67:42085 (size: 5.5 KB, free: 1929.8 MB)
23:54:12.868 [dag-scheduler-event-loop] DEBUG o.a.spark.storage.BlockManagerMaster - Updated info of block broadcast_8_piece0
23:54:12.868 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Told master about block broadcast_8_piece0
23:54:12.868 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_8_piece0 locally took  1 ms
23:54:12.868 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_8_piece0 without replication took  1 ms
23:54:12.868 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1161
23:54:12.869 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[20] at count at SparkInit.java:47) (first 15 tasks are for partitions Vector(0))
23:54:12.869 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
23:54:12.869 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.TaskSetManager - Epoch for TaskSet 4.0: 1
23:54:12.869 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 4.0: NO_PREF, ANY
23:54:12.870 [dispatcher-event-loop-4] DEBUG o.a.s.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_4.0, runningTasks: 0
23:54:12.870 [dispatcher-event-loop-4] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 7158 bytes)
23:54:12.870 [dispatcher-event-loop-4] DEBUG o.a.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
23:54:12.870 [Executor task launch worker for task 4] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
23:54:12.872 [Executor task launch worker for task 4] DEBUG o.apache.spark.storage.BlockManager - Getting local block broadcast_8
23:54:12.872 [Executor task launch worker for task 4] DEBUG o.apache.spark.storage.BlockManager - Level for block broadcast_8 is StorageLevel(disk, memory, deserialized, 1 replicas)
23:54:12.911 [Executor task launch worker for task 4] INFO  o.a.s.s.e.datasources.jdbc.JDBCRDD - closed connection
23:54:12.914 [Executor task launch worker for task 4] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 1512 bytes result sent to driver
23:54:12.914 [dispatcher-event-loop-3] DEBUG o.a.s.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_4.0, runningTasks: 0
23:54:12.914 [task-result-getter-0] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 44 ms on localhost (executor driver) (1/1)
23:54:12.915 [task-result-getter-0] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
23:54:12.915 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
23:54:12.915 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (count at SparkInit.java:47) finished in 0.053 s
23:54:12.915 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
23:54:12.915 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - running: Set()
23:54:12.915 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5)
23:54:12.915 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - failed: Set()
23:54:12.915 [dag-scheduler-event-loop] DEBUG o.a.spark.MapOutputTrackerMaster - Increasing epoch to 2
23:54:12.916 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - submitStage(ResultStage 5)
23:54:12.916 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - missing: List()
23:54:12.916 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[23] at count at SparkInit.java:47), which has no missing parents
23:54:12.916 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 5)
23:54:12.918 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 8.5 KB, free 1929.2 MB)
23:54:12.918 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_9 locally took  1 ms
23:54:12.918 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_9 without replication took  1 ms
23:54:12.919 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.4 KB, free 1929.2 MB)
23:54:12.919 [dispatcher-event-loop-7] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 192.168.1.67:42085 (size: 4.4 KB, free: 1929.8 MB)
23:54:12.920 [dag-scheduler-event-loop] DEBUG o.a.spark.storage.BlockManagerMaster - Updated info of block broadcast_9_piece0
23:54:12.920 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Told master about block broadcast_9_piece0
23:54:12.920 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_9_piece0 locally took  1 ms
23:54:12.920 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_9_piece0 without replication took  1 ms
23:54:12.920 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1161
23:54:12.920 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at count at SparkInit.java:47) (first 15 tasks are for partitions Vector(0))
23:54:12.920 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
23:54:12.921 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.TaskSetManager - Epoch for TaskSet 5.0: 2
23:54:12.921 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 5.0: ANY
23:54:12.921 [dispatcher-event-loop-0] DEBUG o.a.s.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_5.0, runningTasks: 0
23:54:12.921 [dispatcher-event-loop-0] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 7246 bytes)
23:54:12.922 [Executor task launch worker for task 5] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
23:54:12.923 [Executor task launch worker for task 5] DEBUG o.apache.spark.storage.BlockManager - Getting local block broadcast_9
23:54:12.923 [Executor task launch worker for task 5] DEBUG o.apache.spark.storage.BlockManager - Level for block broadcast_9 is StorageLevel(disk, memory, deserialized, 1 replicas)
23:54:12.924 [Executor task launch worker for task 5] DEBUG o.a.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 1, partitions 0-1
23:54:12.925 [Executor task launch worker for task 5] DEBUG o.a.s.s.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
23:54:12.925 [Executor task launch worker for task 5] INFO  o.a.s.s.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
23:54:12.925 [Executor task launch worker for task 5] INFO  o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
23:54:12.925 [Executor task launch worker for task 5] DEBUG o.a.s.s.ShuffleBlockFetcherIterator - Start fetching local blocks: shuffle_1_0_0
23:54:12.925 [Executor task launch worker for task 5] DEBUG o.a.s.s.ShuffleBlockFetcherIterator - Got local blocks in  0 ms
23:54:12.927 [Executor task launch worker for task 5] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 5). 1749 bytes result sent to driver
23:54:12.928 [dispatcher-event-loop-2] DEBUG o.a.s.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_5.0, runningTasks: 0
23:54:12.928 [task-result-getter-1] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 5) in 7 ms on localhost (executor driver) (1/1)
23:54:12.928 [task-result-getter-1] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
23:54:12.929 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ResultStage 5 (count at SparkInit.java:47) finished in 0.012 s
23:54:12.929 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - After removal of stage 5, remaining stages = 1
23:54:12.929 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - After removal of stage 4, remaining stages = 0
23:54:12.929 [main] INFO  o.a.spark.scheduler.DAGScheduler - Job 3 finished: count at SparkInit.java:47, took 0.070114 s
23:54:12.931 [main] INFO  com.spark.examples.gradle.SparkInit - record load fom jdbc : 4
23:54:12.931 [main] INFO  com.spark.examples.gradle.SparkInit - output csv fom jdbc : /home/fabrice/Documents/csvOut
23:54:12.985 [main] DEBUG o.a.s.internal.io.FileCommitProtocol - Creating committer org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol; job 8322d94f-bd59-4ed1-b832-813af4371c15; output=file:/home/fabrice/Documents/csvOut; dynamic=false
23:54:12.985 [main] DEBUG o.a.s.internal.io.FileCommitProtocol - Using (String, String, Boolean) constructor
23:54:12.989 [main] INFO  o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23:54:12.999 [main] DEBUG o.a.s.s.e.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     scan_mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(16, 352);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 029 */       UTF8String scan_value_0 = scan_isNull_0 ?
/* 030 */       null : (scan_row_0.getUTF8String(0));
/* 031 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 032 */       long scan_value_1 = scan_isNull_1 ?
/* 033 */       -1L : (scan_row_0.getLong(1));
/* 034 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 035 */       UTF8String scan_value_2 = scan_isNull_2 ?
/* 036 */       null : (scan_row_0.getUTF8String(2));
/* 037 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);
/* 038 */       UTF8String scan_value_3 = scan_isNull_3 ?
/* 039 */       null : (scan_row_0.getUTF8String(3));
/* 040 */       boolean scan_isNull_4 = scan_row_0.isNullAt(4);
/* 041 */       UTF8String scan_value_4 = scan_isNull_4 ?
/* 042 */       null : (scan_row_0.getUTF8String(4));
/* 043 */       boolean scan_isNull_5 = scan_row_0.isNullAt(5);
/* 044 */       UTF8String scan_value_5 = scan_isNull_5 ?
/* 045 */       null : (scan_row_0.getUTF8String(5));
/* 046 */       boolean scan_isNull_6 = scan_row_0.isNullAt(6);
/* 047 */       int scan_value_6 = scan_isNull_6 ?
/* 048 */       -1 : (scan_row_0.getInt(6));
/* 049 */       boolean scan_isNull_7 = scan_row_0.isNullAt(7);
/* 050 */       UTF8String scan_value_7 = scan_isNull_7 ?
/* 051 */       null : (scan_row_0.getUTF8String(7));
/* 052 */       boolean scan_isNull_8 = scan_row_0.isNullAt(8);
/* 053 */       int scan_value_8 = scan_isNull_8 ?
/* 054 */       -1 : (scan_row_0.getInt(8));
/* 055 */       boolean scan_isNull_9 = scan_row_0.isNullAt(9);
/* 056 */       UTF8String scan_value_9 = scan_isNull_9 ?
/* 057 */       null : (scan_row_0.getUTF8String(9));
/* 058 */       boolean scan_isNull_10 = scan_row_0.isNullAt(10);
/* 059 */       UTF8String scan_value_10 = scan_isNull_10 ?
/* 060 */       null : (scan_row_0.getUTF8String(10));
/* 061 */       boolean scan_isNull_11 = scan_row_0.isNullAt(11);
/* 062 */       UTF8String scan_value_11 = scan_isNull_11 ?
/* 063 */       null : (scan_row_0.getUTF8String(11));
/* 064 */       boolean scan_isNull_12 = scan_row_0.isNullAt(12);
/* 065 */       UTF8String scan_value_12 = scan_isNull_12 ?
/* 066 */       null : (scan_row_0.getUTF8String(12));
/* 067 */       boolean scan_isNull_13 = scan_row_0.isNullAt(13);
/* 068 */       byte[] scan_value_13 = scan_isNull_13 ?
/* 069 */       null : (scan_row_0.getBinary(13));
/* 070 */       boolean scan_isNull_14 = scan_row_0.isNullAt(14);
/* 071 */       int scan_value_14 = scan_isNull_14 ?
/* 072 */       -1 : (scan_row_0.getInt(14));
/* 073 */       boolean scan_isNull_15 = scan_row_0.isNullAt(15);
/* 074 */       long scan_value_15 = scan_isNull_15 ?
/* 075 */       -1L : (scan_row_0.getLong(15));
/* 076 */       scan_mutableStateArray_1[0].reset();
/* 077 */
/* 078 */       scan_mutableStateArray_1[0].zeroOutNullBytes();
/* 079 */
/* 080 */       if (scan_isNull_0) {
/* 081 */         scan_mutableStateArray_1[0].setNullAt(0);
/* 082 */       } else {
/* 083 */         scan_mutableStateArray_1[0].write(0, scan_value_0);
/* 084 */       }
/* 085 */
/* 086 */       if (scan_isNull_1) {
/* 087 */         scan_mutableStateArray_1[0].setNullAt(1);
/* 088 */       } else {
/* 089 */         scan_mutableStateArray_1[0].write(1, scan_value_1);
/* 090 */       }
/* 091 */
/* 092 */       if (scan_isNull_2) {
/* 093 */         scan_mutableStateArray_1[0].setNullAt(2);
/* 094 */       } else {
/* 095 */         scan_mutableStateArray_1[0].write(2, scan_value_2);
/* 096 */       }
/* 097 */
/* 098 */       if (scan_isNull_3) {
/* 099 */         scan_mutableStateArray_1[0].setNullAt(3);
/* 100 */       } else {
/* 101 */         scan_mutableStateArray_1[0].write(3, scan_value_3);
/* 102 */       }
/* 103 */
/* 104 */       if (scan_isNull_4) {
/* 105 */         scan_mutableStateArray_1[0].setNullAt(4);
/* 106 */       } else {
/* 107 */         scan_mutableStateArray_1[0].write(4, scan_value_4);
/* 108 */       }
/* 109 */
/* 110 */       if (scan_isNull_5) {
/* 111 */         scan_mutableStateArray_1[0].setNullAt(5);
/* 112 */       } else {
/* 113 */         scan_mutableStateArray_1[0].write(5, scan_value_5);
/* 114 */       }
/* 115 */
/* 116 */       if (scan_isNull_6) {
/* 117 */         scan_mutableStateArray_1[0].setNullAt(6);
/* 118 */       } else {
/* 119 */         scan_mutableStateArray_1[0].write(6, scan_value_6);
/* 120 */       }
/* 121 */
/* 122 */       if (scan_isNull_7) {
/* 123 */         scan_mutableStateArray_1[0].setNullAt(7);
/* 124 */       } else {
/* 125 */         scan_mutableStateArray_1[0].write(7, scan_value_7);
/* 126 */       }
/* 127 */
/* 128 */       if (scan_isNull_8) {
/* 129 */         scan_mutableStateArray_1[0].setNullAt(8);
/* 130 */       } else {
/* 131 */         scan_mutableStateArray_1[0].write(8, scan_value_8);
/* 132 */       }
/* 133 */
/* 134 */       if (scan_isNull_9) {
/* 135 */         scan_mutableStateArray_1[0].setNullAt(9);
/* 136 */       } else {
/* 137 */         scan_mutableStateArray_1[0].write(9, scan_value_9);
/* 138 */       }
/* 139 */
/* 140 */       if (scan_isNull_10) {
/* 141 */         scan_mutableStateArray_1[0].setNullAt(10);
/* 142 */       } else {
/* 143 */         scan_mutableStateArray_1[0].write(10, scan_value_10);
/* 144 */       }
/* 145 */
/* 146 */       if (scan_isNull_11) {
/* 147 */         scan_mutableStateArray_1[0].setNullAt(11);
/* 148 */       } else {
/* 149 */         scan_mutableStateArray_1[0].write(11, scan_value_11);
/* 150 */       }
/* 151 */
/* 152 */       if (scan_isNull_12) {
/* 153 */         scan_mutableStateArray_1[0].setNullAt(12);
/* 154 */       } else {
/* 155 */         scan_mutableStateArray_1[0].write(12, scan_value_12);
/* 156 */       }
/* 157 */
/* 158 */       if (scan_isNull_13) {
/* 159 */         scan_mutableStateArray_1[0].setNullAt(13);
/* 160 */       } else {
/* 161 */         scan_mutableStateArray_1[0].write(13, scan_value_13);
/* 162 */       }
/* 163 */
/* 164 */       if (scan_isNull_14) {
/* 165 */         scan_mutableStateArray_1[0].setNullAt(14);
/* 166 */       } else {
/* 167 */         scan_mutableStateArray_1[0].write(14, scan_value_14);
/* 168 */       }
/* 169 */
/* 170 */       if (scan_isNull_15) {
/* 171 */         scan_mutableStateArray_1[0].setNullAt(15);
/* 172 */       } else {
/* 173 */         scan_mutableStateArray_1[0].write(15, scan_value_15);
/* 174 */       }
/* 175 */       append((scan_mutableStateArray_1[0].getRow()));
/* 176 */       if (shouldStop()) return;
/* 177 */     }
/* 178 */   }
/* 179 */
/* 180 */ }

23:54:13.001 [main] DEBUG o.a.s.s.c.e.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     scan_mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(16, 352);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 029 */       UTF8String scan_value_0 = scan_isNull_0 ?
/* 030 */       null : (scan_row_0.getUTF8String(0));
/* 031 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 032 */       long scan_value_1 = scan_isNull_1 ?
/* 033 */       -1L : (scan_row_0.getLong(1));
/* 034 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 035 */       UTF8String scan_value_2 = scan_isNull_2 ?
/* 036 */       null : (scan_row_0.getUTF8String(2));
/* 037 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);
/* 038 */       UTF8String scan_value_3 = scan_isNull_3 ?
/* 039 */       null : (scan_row_0.getUTF8String(3));
/* 040 */       boolean scan_isNull_4 = scan_row_0.isNullAt(4);
/* 041 */       UTF8String scan_value_4 = scan_isNull_4 ?
/* 042 */       null : (scan_row_0.getUTF8String(4));
/* 043 */       boolean scan_isNull_5 = scan_row_0.isNullAt(5);
/* 044 */       UTF8String scan_value_5 = scan_isNull_5 ?
/* 045 */       null : (scan_row_0.getUTF8String(5));
/* 046 */       boolean scan_isNull_6 = scan_row_0.isNullAt(6);
/* 047 */       int scan_value_6 = scan_isNull_6 ?
/* 048 */       -1 : (scan_row_0.getInt(6));
/* 049 */       boolean scan_isNull_7 = scan_row_0.isNullAt(7);
/* 050 */       UTF8String scan_value_7 = scan_isNull_7 ?
/* 051 */       null : (scan_row_0.getUTF8String(7));
/* 052 */       boolean scan_isNull_8 = scan_row_0.isNullAt(8);
/* 053 */       int scan_value_8 = scan_isNull_8 ?
/* 054 */       -1 : (scan_row_0.getInt(8));
/* 055 */       boolean scan_isNull_9 = scan_row_0.isNullAt(9);
/* 056 */       UTF8String scan_value_9 = scan_isNull_9 ?
/* 057 */       null : (scan_row_0.getUTF8String(9));
/* 058 */       boolean scan_isNull_10 = scan_row_0.isNullAt(10);
/* 059 */       UTF8String scan_value_10 = scan_isNull_10 ?
/* 060 */       null : (scan_row_0.getUTF8String(10));
/* 061 */       boolean scan_isNull_11 = scan_row_0.isNullAt(11);
/* 062 */       UTF8String scan_value_11 = scan_isNull_11 ?
/* 063 */       null : (scan_row_0.getUTF8String(11));
/* 064 */       boolean scan_isNull_12 = scan_row_0.isNullAt(12);
/* 065 */       UTF8String scan_value_12 = scan_isNull_12 ?
/* 066 */       null : (scan_row_0.getUTF8String(12));
/* 067 */       boolean scan_isNull_13 = scan_row_0.isNullAt(13);
/* 068 */       byte[] scan_value_13 = scan_isNull_13 ?
/* 069 */       null : (scan_row_0.getBinary(13));
/* 070 */       boolean scan_isNull_14 = scan_row_0.isNullAt(14);
/* 071 */       int scan_value_14 = scan_isNull_14 ?
/* 072 */       -1 : (scan_row_0.getInt(14));
/* 073 */       boolean scan_isNull_15 = scan_row_0.isNullAt(15);
/* 074 */       long scan_value_15 = scan_isNull_15 ?
/* 075 */       -1L : (scan_row_0.getLong(15));
/* 076 */       scan_mutableStateArray_1[0].reset();
/* 077 */
/* 078 */       scan_mutableStateArray_1[0].zeroOutNullBytes();
/* 079 */
/* 080 */       if (scan_isNull_0) {
/* 081 */         scan_mutableStateArray_1[0].setNullAt(0);
/* 082 */       } else {
/* 083 */         scan_mutableStateArray_1[0].write(0, scan_value_0);
/* 084 */       }
/* 085 */
/* 086 */       if (scan_isNull_1) {
/* 087 */         scan_mutableStateArray_1[0].setNullAt(1);
/* 088 */       } else {
/* 089 */         scan_mutableStateArray_1[0].write(1, scan_value_1);
/* 090 */       }
/* 091 */
/* 092 */       if (scan_isNull_2) {
/* 093 */         scan_mutableStateArray_1[0].setNullAt(2);
/* 094 */       } else {
/* 095 */         scan_mutableStateArray_1[0].write(2, scan_value_2);
/* 096 */       }
/* 097 */
/* 098 */       if (scan_isNull_3) {
/* 099 */         scan_mutableStateArray_1[0].setNullAt(3);
/* 100 */       } else {
/* 101 */         scan_mutableStateArray_1[0].write(3, scan_value_3);
/* 102 */       }
/* 103 */
/* 104 */       if (scan_isNull_4) {
/* 105 */         scan_mutableStateArray_1[0].setNullAt(4);
/* 106 */       } else {
/* 107 */         scan_mutableStateArray_1[0].write(4, scan_value_4);
/* 108 */       }
/* 109 */
/* 110 */       if (scan_isNull_5) {
/* 111 */         scan_mutableStateArray_1[0].setNullAt(5);
/* 112 */       } else {
/* 113 */         scan_mutableStateArray_1[0].write(5, scan_value_5);
/* 114 */       }
/* 115 */
/* 116 */       if (scan_isNull_6) {
/* 117 */         scan_mutableStateArray_1[0].setNullAt(6);
/* 118 */       } else {
/* 119 */         scan_mutableStateArray_1[0].write(6, scan_value_6);
/* 120 */       }
/* 121 */
/* 122 */       if (scan_isNull_7) {
/* 123 */         scan_mutableStateArray_1[0].setNullAt(7);
/* 124 */       } else {
/* 125 */         scan_mutableStateArray_1[0].write(7, scan_value_7);
/* 126 */       }
/* 127 */
/* 128 */       if (scan_isNull_8) {
/* 129 */         scan_mutableStateArray_1[0].setNullAt(8);
/* 130 */       } else {
/* 131 */         scan_mutableStateArray_1[0].write(8, scan_value_8);
/* 132 */       }
/* 133 */
/* 134 */       if (scan_isNull_9) {
/* 135 */         scan_mutableStateArray_1[0].setNullAt(9);
/* 136 */       } else {
/* 137 */         scan_mutableStateArray_1[0].write(9, scan_value_9);
/* 138 */       }
/* 139 */
/* 140 */       if (scan_isNull_10) {
/* 141 */         scan_mutableStateArray_1[0].setNullAt(10);
/* 142 */       } else {
/* 143 */         scan_mutableStateArray_1[0].write(10, scan_value_10);
/* 144 */       }
/* 145 */
/* 146 */       if (scan_isNull_11) {
/* 147 */         scan_mutableStateArray_1[0].setNullAt(11);
/* 148 */       } else {
/* 149 */         scan_mutableStateArray_1[0].write(11, scan_value_11);
/* 150 */       }
/* 151 */
/* 152 */       if (scan_isNull_12) {
/* 153 */         scan_mutableStateArray_1[0].setNullAt(12);
/* 154 */       } else {
/* 155 */         scan_mutableStateArray_1[0].write(12, scan_value_12);
/* 156 */       }
/* 157 */
/* 158 */       if (scan_isNull_13) {
/* 159 */         scan_mutableStateArray_1[0].setNullAt(13);
/* 160 */       } else {
/* 161 */         scan_mutableStateArray_1[0].write(13, scan_value_13);
/* 162 */       }
/* 163 */
/* 164 */       if (scan_isNull_14) {
/* 165 */         scan_mutableStateArray_1[0].setNullAt(14);
/* 166 */       } else {
/* 167 */         scan_mutableStateArray_1[0].write(14, scan_value_14);
/* 168 */       }
/* 169 */
/* 170 */       if (scan_isNull_15) {
/* 171 */         scan_mutableStateArray_1[0].setNullAt(15);
/* 172 */       } else {
/* 173 */         scan_mutableStateArray_1[0].write(15, scan_value_15);
/* 174 */       }
/* 175 */       append((scan_mutableStateArray_1[0].getRow()));
/* 176 */       if (shouldStop()) return;
/* 177 */     }
/* 178 */   }
/* 179 */
/* 180 */ }

23:54:13.022 [main] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 23.226379 ms
23:54:13.023 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$doExecute$4$adapted
23:54:13.023 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
23:54:13.025 [main] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning lambda: $anonfun$write$15
23:54:13.026 [main] DEBUG org.apache.spark.util.ClosureCleaner -  +++ Lambda closure ($anonfun$write$15) is now cleaned +++
23:54:13.044 [main] INFO  org.apache.spark.SparkContext - Starting job: csv at SparkInit.java:50
23:54:13.045 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Got job 4 (csv at SparkInit.java:50) with 1 output partitions
23:54:13.045 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (csv at SparkInit.java:50)
23:54:13.045 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
23:54:13.045 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Missing parents: List()
23:54:13.045 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - submitStage(ResultStage 6)
23:54:13.045 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - missing: List()
23:54:13.045 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[25] at csv at SparkInit.java:50), which has no missing parents
23:54:13.045 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 6)
23:54:13.062 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 139.5 KB, free 1929.0 MB)
23:54:13.062 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_10 locally took  0 ms
23:54:13.063 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_10 without replication took  1 ms
23:54:13.064 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 50.5 KB, free 1929.0 MB)
23:54:13.065 [dispatcher-event-loop-5] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 192.168.1.67:42085 (size: 50.5 KB, free: 1929.7 MB)
23:54:13.065 [dag-scheduler-event-loop] DEBUG o.a.spark.storage.BlockManagerMaster - Updated info of block broadcast_10_piece0
23:54:13.065 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Told master about block broadcast_10_piece0
23:54:13.065 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Put block broadcast_10_piece0 locally took  1 ms
23:54:13.065 [dag-scheduler-event-loop] DEBUG o.apache.spark.storage.BlockManager - Putting block broadcast_10_piece0 without replication took  1 ms
23:54:13.066 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1161
23:54:13.066 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[25] at csv at SparkInit.java:50) (first 15 tasks are for partitions Vector(0))
23:54:13.066 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks
23:54:13.066 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.TaskSetManager - Epoch for TaskSet 6.0: 2
23:54:13.067 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 6.0: NO_PREF, ANY
23:54:13.067 [dispatcher-event-loop-4] DEBUG o.a.s.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_6.0, runningTasks: 0
23:54:13.067 [dispatcher-event-loop-4] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7169 bytes)
23:54:13.067 [dispatcher-event-loop-4] DEBUG o.a.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
23:54:13.068 [Executor task launch worker for task 6] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 6)
23:54:13.068 [Executor task launch worker for task 6] DEBUG o.apache.spark.storage.BlockManager - Getting local block broadcast_10
23:54:13.069 [Executor task launch worker for task 6] DEBUG o.apache.spark.storage.BlockManager - Level for block broadcast_10 is StorageLevel(disk, memory, deserialized, 1 replicas)
23:54:13.092 [Executor task launch worker for task 6] INFO  o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23:54:13.109 [Executor task launch worker for task 6] INFO  o.a.s.s.e.datasources.jdbc.JDBCRDD - closed connection
23:54:13.110 [dispatcher-event-loop-3] DEBUG o.a.s.s.OutputCommitCoordinator - Commit allowed for stage=6.0, partition=0, task attempt 0
23:54:13.112 [Executor task launch worker for task 6] INFO  o.a.h.m.l.output.FileOutputCommitter - Saved output of task 'attempt_20190312235413_0006_m_000000_0' to file:/home/fabrice/Documents/csvOut/_temporary/0/task_20190312235413_0006_m_000000
23:54:13.112 [Executor task launch worker for task 6] INFO  o.a.s.mapred.SparkHadoopMapRedUtil - attempt_20190312235413_0006_m_000000_0: Committed
23:54:13.114 [Executor task launch worker for task 6] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 6). 2083 bytes result sent to driver
23:54:13.114 [dispatcher-event-loop-7] DEBUG o.a.s.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_6.0, runningTasks: 0
23:54:13.115 [task-result-getter-2] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 6) in 48 ms on localhost (executor driver) (1/1)
23:54:13.115 [task-result-getter-2] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
23:54:13.115 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ResultStage 6 (csv at SparkInit.java:50) finished in 0.069 s
23:54:13.116 [dag-scheduler-event-loop] DEBUG o.a.spark.scheduler.DAGScheduler - After removal of stage 6, remaining stages = 0
23:54:13.116 [main] INFO  o.a.spark.scheduler.DAGScheduler - Job 4 finished: csv at SparkInit.java:50, took 0.072284 s
23:54:13.117 [main] DEBUG o.a.h.m.l.output.FileOutputCommitter - Merging data from DeprecatedRawLocalFileStatus{path=file:/home/fabrice/Documents/csvOut/_temporary/0/task_20190312235413_0006_m_000000; isDirectory=true; modification_time=1552431253000; access_time=0; owner=; group=; permission=rwxrwxrwx; isSymlink=false} to file:/home/fabrice/Documents/csvOut
23:54:13.117 [main] DEBUG o.a.h.m.l.output.FileOutputCommitter - Merging data from DeprecatedRawLocalFileStatus{path=file:/home/fabrice/Documents/csvOut/_temporary/0/task_20190312235413_0006_m_000000/part-00000-8322d94f-bd59-4ed1-b832-813af4371c15-c000.csv; isDirectory=false; length=247; replication=1; blocksize=33554432; modification_time=1552431253000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false} to file:/home/fabrice/Documents/csvOut/part-00000-8322d94f-bd59-4ed1-b832-813af4371c15-c000.csv
23:54:13.133 [main] DEBUG o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol - Committing files staged for absolute locations Map()
23:54:13.133 [main] INFO  o.a.s.s.e.d.FileFormatWriter - Write Job 4a203223-57ef-4dcc-96e4-3fd64ffa993e committed.
23:54:13.133 [main] INFO  o.a.s.s.e.d.FileFormatWriter - Finished processing stats for write job 4a203223-57ef-4dcc-96e4-3fd64ffa993e.
23:54:13.154 [Thread-2] INFO  org.apache.spark.SparkContext - Invoking stop() from shutdown hook
23:54:13.156 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - stopping org.spark_project.jetty.server.Server@4ef27d66
23:54:13.156 [Thread-2] DEBUG o.spark_project.jetty.server.Server - doStop org.spark_project.jetty.server.Server@4ef27d66
23:54:13.156 [SparkUI-38] DEBUG o.s.j.util.thread.QueuedThreadPool - ran SparkUI-38-acceptor-0@5990e6c5-ServerConnector@a77614d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
23:54:13.157 [Thread-2] DEBUG o.spark_project.jetty.server.Server - Graceful shutdown org.spark_project.jetty.server.Server@4ef27d66 by 
23:54:13.158 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - stopping Spark@a77614d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
23:54:13.158 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - stopping org.spark_project.jetty.server.ServerConnector$ServerConnectorManager@4cc76301
23:54:13.158 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - stopping org.spark_project.jetty.io.ManagedSelector@517bd097 id=3 keys=0 selected=0
23:54:13.158 [Thread-2] DEBUG o.s.jetty.io.ManagedSelector - Stopping org.spark_project.jetty.io.ManagedSelector@517bd097 id=3 keys=0 selected=0
23:54:13.158 [Thread-2] DEBUG o.s.jetty.io.ManagedSelector - Queued change org.spark_project.jetty.io.ManagedSelector$CloseEndPoints@51e34298 on org.spark_project.jetty.io.ManagedSelector@517bd097 id=3 keys=0 selected=0
23:54:13.158 [SparkUI-37] DEBUG o.s.jetty.io.ManagedSelector - Selector loop woken up from select, 0/0 selected
23:54:13.159 [SparkUI-37] DEBUG o.s.jetty.io.ManagedSelector - Running change org.spark_project.jetty.io.ManagedSelector$CloseEndPoints@51e34298
23:54:13.159 [SparkUI-37] DEBUG o.s.jetty.io.ManagedSelector - Closing 0 endPoints on org.spark_project.jetty.io.ManagedSelector@517bd097 id=3 keys=0 selected=0
23:54:13.159 [SparkUI-37] DEBUG o.s.jetty.io.ManagedSelector - Closed 0 endPoints on org.spark_project.jetty.io.ManagedSelector@517bd097 id=3 keys=0 selected=0
23:54:13.159 [SparkUI-37] DEBUG o.s.jetty.io.ManagedSelector - Selector loop waiting on select
23:54:13.159 [Thread-2] DEBUG o.s.jetty.io.ManagedSelector - Queued change org.spark_project.jetty.io.ManagedSelector$CloseSelector@5202cdb9 on org.spark_project.jetty.io.ManagedSelector@517bd097 id=3 keys=0 selected=0
23:54:13.159 [SparkUI-37] DEBUG o.s.jetty.io.ManagedSelector - Selector loop woken up from select, 0/0 selected
23:54:13.159 [SparkUI-37] DEBUG o.s.jetty.io.ManagedSelector - Running change org.spark_project.jetty.io.ManagedSelector$CloseSelector@5202cdb9
23:54:13.159 [SparkUI-37] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Prod/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@7b8d954b produced null
23:54:13.160 [Thread-2] DEBUG o.s.jetty.io.ManagedSelector - Stopped org.spark_project.jetty.io.ManagedSelector@517bd097 id=3 keys=-1 selected=-1
23:54:13.160 [SparkUI-37] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Idle/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@7b8d954b produce exit
23:54:13.160 [SparkUI-37] DEBUG o.s.j.util.thread.QueuedThreadPool - ran org.spark_project.jetty.io.ManagedSelector@517bd097 id=3 keys=-1 selected=-1
23:54:13.160 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - STOPPED org.spark_project.jetty.io.ManagedSelector@517bd097 id=3 keys=-1 selected=-1
23:54:13.160 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - stopping org.spark_project.jetty.io.ManagedSelector@50687efb id=2 keys=0 selected=0
23:54:13.160 [Thread-2] DEBUG o.s.jetty.io.ManagedSelector - Stopping org.spark_project.jetty.io.ManagedSelector@50687efb id=2 keys=0 selected=0
23:54:13.160 [Thread-2] DEBUG o.s.jetty.io.ManagedSelector - Queued change org.spark_project.jetty.io.ManagedSelector$CloseEndPoints@19bea2e3 on org.spark_project.jetty.io.ManagedSelector@50687efb id=2 keys=0 selected=0
23:54:13.160 [SparkUI-36] DEBUG o.s.jetty.io.ManagedSelector - Selector loop woken up from select, 0/0 selected
23:54:13.160 [SparkUI-36] DEBUG o.s.jetty.io.ManagedSelector - Running change org.spark_project.jetty.io.ManagedSelector$CloseEndPoints@19bea2e3
23:54:13.160 [SparkUI-36] DEBUG o.s.jetty.io.ManagedSelector - Closing 0 endPoints on org.spark_project.jetty.io.ManagedSelector@50687efb id=2 keys=0 selected=0
23:54:13.160 [SparkUI-36] DEBUG o.s.jetty.io.ManagedSelector - Closed 0 endPoints on org.spark_project.jetty.io.ManagedSelector@50687efb id=2 keys=0 selected=0
23:54:13.160 [SparkUI-36] DEBUG o.s.jetty.io.ManagedSelector - Selector loop waiting on select
23:54:13.160 [Thread-2] DEBUG o.s.jetty.io.ManagedSelector - Queued change org.spark_project.jetty.io.ManagedSelector$CloseSelector@158cb002 on org.spark_project.jetty.io.ManagedSelector@50687efb id=2 keys=0 selected=0
23:54:13.160 [SparkUI-36] DEBUG o.s.jetty.io.ManagedSelector - Selector loop woken up from select, 0/0 selected
23:54:13.160 [SparkUI-36] DEBUG o.s.jetty.io.ManagedSelector - Running change org.spark_project.jetty.io.ManagedSelector$CloseSelector@158cb002
23:54:13.160 [SparkUI-36] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Prod/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@4f6583b6 produced null
23:54:13.160 [SparkUI-36] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Idle/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@4f6583b6 produce exit
23:54:13.160 [Thread-2] DEBUG o.s.jetty.io.ManagedSelector - Stopped org.spark_project.jetty.io.ManagedSelector@50687efb id=2 keys=-1 selected=-1
23:54:13.160 [SparkUI-36] DEBUG o.s.j.util.thread.QueuedThreadPool - ran org.spark_project.jetty.io.ManagedSelector@50687efb id=2 keys=-1 selected=-1
23:54:13.160 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - STOPPED org.spark_project.jetty.io.ManagedSelector@50687efb id=2 keys=-1 selected=-1
23:54:13.160 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - stopping org.spark_project.jetty.io.ManagedSelector@66971f6b id=1 keys=0 selected=0
23:54:13.160 [Thread-2] DEBUG o.s.jetty.io.ManagedSelector - Stopping org.spark_project.jetty.io.ManagedSelector@66971f6b id=1 keys=0 selected=0
23:54:13.160 [Thread-2] DEBUG o.s.jetty.io.ManagedSelector - Queued change org.spark_project.jetty.io.ManagedSelector$CloseEndPoints@38b76a5d on org.spark_project.jetty.io.ManagedSelector@66971f6b id=1 keys=0 selected=0
23:54:13.160 [SparkUI-35] DEBUG o.s.jetty.io.ManagedSelector - Selector loop woken up from select, 0/0 selected
23:54:13.160 [SparkUI-35] DEBUG o.s.jetty.io.ManagedSelector - Running change org.spark_project.jetty.io.ManagedSelector$CloseEndPoints@38b76a5d
23:54:13.160 [SparkUI-35] DEBUG o.s.jetty.io.ManagedSelector - Closing 0 endPoints on org.spark_project.jetty.io.ManagedSelector@66971f6b id=1 keys=0 selected=0
23:54:13.160 [SparkUI-35] DEBUG o.s.jetty.io.ManagedSelector - Closed 0 endPoints on org.spark_project.jetty.io.ManagedSelector@66971f6b id=1 keys=0 selected=0
23:54:13.160 [SparkUI-35] DEBUG o.s.jetty.io.ManagedSelector - Selector loop waiting on select
23:54:13.160 [Thread-2] DEBUG o.s.jetty.io.ManagedSelector - Queued change org.spark_project.jetty.io.ManagedSelector$CloseSelector@7c1c17f7 on org.spark_project.jetty.io.ManagedSelector@66971f6b id=1 keys=0 selected=0
23:54:13.160 [SparkUI-35] DEBUG o.s.jetty.io.ManagedSelector - Selector loop woken up from select, 0/0 selected
23:54:13.160 [SparkUI-35] DEBUG o.s.jetty.io.ManagedSelector - Running change org.spark_project.jetty.io.ManagedSelector$CloseSelector@7c1c17f7
23:54:13.160 [SparkUI-35] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Prod/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@58693520 produced null
23:54:13.160 [Thread-2] DEBUG o.s.jetty.io.ManagedSelector - Stopped org.spark_project.jetty.io.ManagedSelector@66971f6b id=1 keys=-1 selected=-1
23:54:13.160 [SparkUI-35] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Idle/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@58693520 produce exit
23:54:13.160 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - STOPPED org.spark_project.jetty.io.ManagedSelector@66971f6b id=1 keys=-1 selected=-1
23:54:13.160 [SparkUI-35] DEBUG o.s.j.util.thread.QueuedThreadPool - ran org.spark_project.jetty.io.ManagedSelector@66971f6b id=1 keys=-1 selected=-1
23:54:13.161 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - stopping org.spark_project.jetty.io.ManagedSelector@3bffddff id=0 keys=0 selected=0
23:54:13.161 [Thread-2] DEBUG o.s.jetty.io.ManagedSelector - Stopping org.spark_project.jetty.io.ManagedSelector@3bffddff id=0 keys=0 selected=0
23:54:13.161 [Thread-2] DEBUG o.s.jetty.io.ManagedSelector - Queued change org.spark_project.jetty.io.ManagedSelector$CloseEndPoints@10d90b2d on org.spark_project.jetty.io.ManagedSelector@3bffddff id=0 keys=0 selected=0
23:54:13.161 [SparkUI-34] DEBUG o.s.jetty.io.ManagedSelector - Selector loop woken up from select, 0/0 selected
23:54:13.161 [SparkUI-34] DEBUG o.s.jetty.io.ManagedSelector - Running change org.spark_project.jetty.io.ManagedSelector$CloseEndPoints@10d90b2d
23:54:13.161 [SparkUI-34] DEBUG o.s.jetty.io.ManagedSelector - Closing 0 endPoints on org.spark_project.jetty.io.ManagedSelector@3bffddff id=0 keys=0 selected=0
23:54:13.161 [SparkUI-34] DEBUG o.s.jetty.io.ManagedSelector - Closed 0 endPoints on org.spark_project.jetty.io.ManagedSelector@3bffddff id=0 keys=0 selected=0
23:54:13.161 [SparkUI-34] DEBUG o.s.jetty.io.ManagedSelector - Selector loop waiting on select
23:54:13.161 [Thread-2] DEBUG o.s.jetty.io.ManagedSelector - Queued change org.spark_project.jetty.io.ManagedSelector$CloseSelector@6025a594 on org.spark_project.jetty.io.ManagedSelector@3bffddff id=0 keys=0 selected=0
23:54:13.161 [SparkUI-34] DEBUG o.s.jetty.io.ManagedSelector - Selector loop woken up from select, 0/0 selected
23:54:13.161 [SparkUI-34] DEBUG o.s.jetty.io.ManagedSelector - Running change org.spark_project.jetty.io.ManagedSelector$CloseSelector@6025a594
23:54:13.161 [SparkUI-34] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Prod/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@3e4a1453 produced null
23:54:13.161 [SparkUI-34] DEBUG o.s.j.u.t.s.ExecuteProduceConsume - EPC Idle/org.spark_project.jetty.io.ManagedSelector$SelectorProducer@3e4a1453 produce exit
23:54:13.161 [Thread-2] DEBUG o.s.jetty.io.ManagedSelector - Stopped org.spark_project.jetty.io.ManagedSelector@3bffddff id=0 keys=-1 selected=-1
23:54:13.161 [SparkUI-34] DEBUG o.s.j.util.thread.QueuedThreadPool - ran org.spark_project.jetty.io.ManagedSelector@3bffddff id=0 keys=-1 selected=-1
23:54:13.161 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - STOPPED org.spark_project.jetty.io.ManagedSelector@3bffddff id=0 keys=-1 selected=-1
23:54:13.161 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - STOPPED org.spark_project.jetty.server.ServerConnector$ServerConnectorManager@4cc76301
23:54:13.161 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - stopping HttpConnectionFactory@11dee337[HTTP/1.1]
23:54:13.161 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - STOPPED HttpConnectionFactory@11dee337[HTTP/1.1]
23:54:13.161 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - stopping org.spark_project.jetty.util.thread.ScheduledExecutorScheduler@4fd4cae3
23:54:13.161 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - STOPPED org.spark_project.jetty.util.thread.ScheduledExecutorScheduler@4fd4cae3
23:54:13.161 [Thread-2] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@a77614d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
23:54:13.161 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - STOPPED Spark@a77614d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
23:54:13.161 [Thread-2] DEBUG o.s.j.server.handler.AbstractHandler - stopping org.spark_project.jetty.server.Server@4ef27d66
23:54:13.161 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - stopping org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c, org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b, org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722, org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7, org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc, org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2, org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d, org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da, org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935, o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,SHUTDOWN,@Spark}, o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,SHUTDOWN,@Spark}, o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,SHUTDOWN,@Spark}, o.s.j.s.ServletContextHandler@3f36b447{/SQL/execution,null,SHUTDOWN,@Spark}, o.s.j.s.ServletContextHandler@4a1c0752{/SQL/execution/json,null,SHUTDOWN,@Spark}, o.s.j.s.ServletContextHandler@19593091{/static/sql,null,SHUTDOWN,@Spark}]
23:54:13.162 [Thread-2] DEBUG o.s.j.server.handler.AbstractHandler - stopping org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c, org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b, org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722, org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7, org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc, org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2, org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d, org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da, org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935, o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,SHUTDOWN,@Spark}, o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,SHUTDOWN,@Spark}, o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,SHUTDOWN,@Spark}, o.s.j.s.ServletContextHandler@3f36b447{/SQL/execution,null,SHUTDOWN,@Spark}, o.s.j.s.ServletContextHandler@4a1c0752{/SQL/execution/json,null,SHUTDOWN,@Spark}, o.s.j.s.ServletContextHandler@19593091{/static/sql,null,SHUTDOWN,@Spark}]
23:54:13.162 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - STOPPED org.spark_project.jetty.server.handler.ContextHandlerCollection@439a8f59[org.spark_project.jetty.server.handler.gzip.GzipHandler@2a2da905, org.spark_project.jetty.server.handler.gzip.GzipHandler@7a138fc5, org.spark_project.jetty.server.handler.gzip.GzipHandler@4e4efc1b, org.spark_project.jetty.server.handler.gzip.GzipHandler@2a76b80a, org.spark_project.jetty.server.handler.gzip.GzipHandler@e6516e, org.spark_project.jetty.server.handler.gzip.GzipHandler@4da855dd, org.spark_project.jetty.server.handler.gzip.GzipHandler@4de025bf, org.spark_project.jetty.server.handler.gzip.GzipHandler@9f46d94, org.spark_project.jetty.server.handler.gzip.GzipHandler@7df587ef, org.spark_project.jetty.server.handler.gzip.GzipHandler@560cbf1a, org.spark_project.jetty.server.handler.gzip.GzipHandler@7e094740, org.spark_project.jetty.server.handler.gzip.GzipHandler@3591009c, org.spark_project.jetty.server.handler.gzip.GzipHandler@758f4f03, org.spark_project.jetty.server.handler.gzip.GzipHandler@9cd25ff, org.spark_project.jetty.server.handler.gzip.GzipHandler@2de366bb, org.spark_project.jetty.server.handler.gzip.GzipHandler@e24ddd0, org.spark_project.jetty.server.handler.gzip.GzipHandler@1ac85b0c, org.spark_project.jetty.server.handler.gzip.GzipHandler@5003041b, org.spark_project.jetty.server.handler.gzip.GzipHandler@ca27722, org.spark_project.jetty.server.handler.gzip.GzipHandler@544630b7, org.spark_project.jetty.server.handler.gzip.GzipHandler@1b822fcc, org.spark_project.jetty.server.handler.gzip.GzipHandler@6815c5f2, org.spark_project.jetty.server.handler.gzip.GzipHandler@3a4b0e5d, org.spark_project.jetty.server.handler.gzip.GzipHandler@46ab18da, org.spark_project.jetty.server.handler.gzip.GzipHandler@1a2e2935, o.s.j.s.ServletContextHandler@4f449e8f{/metrics/json,null,SHUTDOWN,@Spark}, o.s.j.s.ServletContextHandler@baf1bb3{/SQL,null,SHUTDOWN,@Spark}, o.s.j.s.ServletContextHandler@3703bf3c{/SQL/json,null,SHUTDOWN,@Spark}, o.s.j.s.ServletContextHandler@3f36b447{/SQL/execution,null,SHUTDOWN,@Spark}, o.s.j.s.ServletContextHandler@4a1c0752{/SQL/execution/json,null,SHUTDOWN,@Spark}, o.s.j.s.ServletContextHandler@19593091{/static/sql,null,SHUTDOWN,@Spark}]
23:54:13.162 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - stopping org.spark_project.jetty.server.handler.ErrorHandler@5c48c0c0
23:54:13.162 [Thread-2] DEBUG o.s.j.server.handler.AbstractHandler - stopping org.spark_project.jetty.server.handler.ErrorHandler@5c48c0c0
23:54:13.162 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - STOPPED org.spark_project.jetty.server.handler.ErrorHandler@5c48c0c0
23:54:13.162 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - stopping SparkUI{STARTED,8<=8<=200,i=8,q=0}
23:54:13.163 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - STOPPED SparkUI{STOPPED,8<=8<=200,i=0,q=0}
23:54:13.163 [Thread-2] DEBUG o.s.j.u.component.AbstractLifeCycle - STOPPED org.spark_project.jetty.server.Server@4ef27d66
23:54:13.163 [Thread-2] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.67:4040
23:54:13.194 [dispatcher-event-loop-2] INFO  o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
23:54:13.216 [Thread-2] INFO  o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
23:54:13.216 [Thread-2] INFO  o.apache.spark.storage.BlockManager - BlockManager stopped
23:54:13.218 [Thread-2] INFO  o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
23:54:13.221 [dispatcher-event-loop-7] INFO  o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
23:54:13.246 [Thread-2] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext
23:54:13.247 [Thread-2] INFO  o.a.spark.util.ShutdownHookManager - Shutdown hook called
23:54:13.247 [Thread-2] INFO  o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-60a4bdf0-ee86-4d43-9cfa-3d403a8c251a
23:54:54.360 [main] WARN  org.apache.spark.util.Utils - Your hostname, fabrice-GL552VW resolves to a loopback address: 127.0.1.1; using 192.168.1.67 instead (on interface wlp2s0)
23:54:54.364 [main] WARN  org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
23:54:54.450 [main] INFO  org.apache.spark.SparkContext - Running Spark version 2.4.0
23:54:54.627 [main] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23:54:54.733 [main] INFO  org.apache.spark.SparkContext - Submitted application: spark session example
23:54:54.803 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: fabrice
23:54:54.804 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: fabrice
23:54:54.805 [main] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
23:54:54.805 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
23:54:54.806 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fabrice); groups with view permissions: Set(); users  with modify permissions: Set(fabrice); groups with modify permissions: Set()
23:54:55.121 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43323.
23:54:55.142 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
23:54:55.159 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
23:54:55.163 [main] INFO  o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23:54:55.163 [main] INFO  o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
23:54:55.178 [main] INFO  o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-320d345b-c044-4696-94e7-48323d6860ab
23:54:55.200 [main] INFO  o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1929.9 MB
23:54:55.213 [main] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
23:54:55.283 [main] INFO  org.spark_project.jetty.util.log - Logging initialized @2113ms
23:54:55.333 [main] INFO  o.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
23:54:55.344 [main] INFO  o.spark_project.jetty.server.Server - Started @2174ms
23:54:55.362 [main] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@1bbfc27d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
23:54:55.363 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
23:54:55.383 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11dee337{/jobs,null,AVAILABLE,@Spark}
23:54:55.384 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fd4cae3{/jobs/json,null,AVAILABLE,@Spark}
23:54:55.384 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a067c25{/jobs/job,null,AVAILABLE,@Spark}
23:54:55.385 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3bde62ff{/jobs/job/json,null,AVAILABLE,@Spark}
23:54:55.385 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@523424b5{/stages,null,AVAILABLE,@Spark}
23:54:55.386 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2baa8d82{/stages/json,null,AVAILABLE,@Spark}
23:54:55.386 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@319dead1{/stages/stage,null,AVAILABLE,@Spark}
23:54:55.388 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@754777cd{/stages/stage/json,null,AVAILABLE,@Spark}
23:54:55.388 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b52c0d6{/stages/pool,null,AVAILABLE,@Spark}
23:54:55.389 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@372ea2bc{/stages/pool/json,null,AVAILABLE,@Spark}
23:54:55.389 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4cc76301{/storage,null,AVAILABLE,@Spark}
23:54:55.390 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f08c4b{/storage/json,null,AVAILABLE,@Spark}
23:54:55.390 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f19b8b3{/storage/rdd,null,AVAILABLE,@Spark}
23:54:55.391 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7de0c6ae{/storage/rdd/json,null,AVAILABLE,@Spark}
23:54:55.391 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a486d78{/environment,null,AVAILABLE,@Spark}
23:54:55.392 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cdc3aae{/environment/json,null,AVAILABLE,@Spark}
23:54:55.392 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ef2d7a6{/executors,null,AVAILABLE,@Spark}
23:54:55.393 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dcbb60{/executors/json,null,AVAILABLE,@Spark}
23:54:55.394 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c36250e{/executors/threadDump,null,AVAILABLE,@Spark}
23:54:55.394 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21526f6c{/executors/threadDump/json,null,AVAILABLE,@Spark}
23:54:55.400 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49f5c307{/static,null,AVAILABLE,@Spark}
23:54:55.400 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cf65451{/,null,AVAILABLE,@Spark}
23:54:55.401 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@724f138e{/api,null,AVAILABLE,@Spark}
23:54:55.402 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64da2a7{/jobs/job/kill,null,AVAILABLE,@Spark}
23:54:55.402 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46074492{/stages/stage/kill,null,AVAILABLE,@Spark}
23:54:55.404 [main] INFO  org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.67:4040
23:54:55.492 [main] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
23:54:55.539 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32945.
23:54:55.540 [main] INFO  o.a.s.n.n.NettyBlockTransferService - Server created on 192.168.1.67:32945
23:54:55.541 [main] INFO  o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23:54:55.558 [main] INFO  o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.67, 32945, None)
23:54:55.562 [dispatcher-event-loop-2] INFO  o.a.s.s.BlockManagerMasterEndpoint - Registering block manager 192.168.1.67:32945 with 1929.9 MB RAM, BlockManagerId(driver, 192.168.1.67, 32945, None)
23:54:55.564 [main] INFO  o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.67, 32945, None)
23:54:55.565 [main] INFO  o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.67, 32945, None)
23:54:55.682 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36453307{/metrics/json,null,AVAILABLE,@Spark}
23:54:55.820 [main] INFO  com.spark.examples.gradle.SparkInit - Csv input file :/home/fabrice/workspace/sparkexamples/gradle.template/build/resources/main/test.csv
23:54:55.820 [main] INFO  com.spark.examples.gradle.SparkInit - Output json :/home/fabrice/Documents/jsonOut
23:54:55.853 [main] INFO  o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/fabrice/workspace/sparkexamples/gradle.template/spark-warehouse').
23:54:55.853 [main] INFO  o.a.spark.sql.internal.SharedState - Warehouse path is 'file:/home/fabrice/workspace/sparkexamples/gradle.template/spark-warehouse'.
23:54:55.861 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@757529a4{/SQL,null,AVAILABLE,@Spark}
23:54:55.862 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@779de014{/SQL/json,null,AVAILABLE,@Spark}
23:54:55.862 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4642b71d{/SQL/execution,null,AVAILABLE,@Spark}
23:54:55.863 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1450078a{/SQL/execution/json,null,AVAILABLE,@Spark}
23:54:55.864 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6719a5b8{/static/sql,null,AVAILABLE,@Spark}
23:54:56.322 [main] INFO  o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
23:54:58.077 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Pruning directories with: 
23:54:58.080 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Post-Scan Filters: (length(trim(value#0, None)) > 0)
23:54:58.083 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Output Data Schema: struct<value: string>
23:54:58.089 [main] INFO  o.a.s.s.execution.FileSourceScanExec - Pushed Filters: 
23:54:58.380 [main] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 151.341849 ms
23:54:58.603 [main] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.043751 ms
23:54:58.669 [main] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 221.5 KB, free 1929.7 MB)
23:54:58.714 [main] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1929.7 MB)
23:54:58.716 [dispatcher-event-loop-5] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.1.67:32945 (size: 20.7 KB, free: 1929.9 MB)
23:54:58.718 [main] INFO  org.apache.spark.SparkContext - Created broadcast 0 from csv at SparkInit.java:35
23:54:58.723 [main] INFO  o.a.s.s.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23:54:58.847 [main] INFO  org.apache.spark.SparkContext - Starting job: csv at SparkInit.java:35
23:54:58.868 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Got job 0 (csv at SparkInit.java:35) with 1 output partitions
23:54:58.869 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (csv at SparkInit.java:35)
23:54:58.869 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
23:54:58.870 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Missing parents: List()
23:54:58.874 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at SparkInit.java:35), which has no missing parents
23:54:58.938 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 10.2 KB, free 1929.7 MB)
23:54:58.942 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 1929.6 MB)
23:54:58.942 [dispatcher-event-loop-6] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.1.67:32945 (size: 5.0 KB, free: 1929.9 MB)
23:54:58.943 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1161
23:54:58.954 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at SparkInit.java:35) (first 15 tasks are for partitions Vector(0))
23:54:58.955 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
23:54:59.012 [dispatcher-event-loop-7] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7847 bytes)
23:54:59.022 [Executor task launch worker for task 0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
23:54:59.321 [Executor task launch worker for task 0] INFO  o.a.s.s.e.datasources.FileScanRDD - Reading File path: file:///home/fabrice/workspace/sparkexamples/gradle.template/build/resources/main/test.csv, range: 0-121306, partition values: [empty row]
23:54:59.337 [Executor task launch worker for task 0] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 10.594645 ms
23:54:59.369 [Executor task launch worker for task 0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1394 bytes result sent to driver
23:54:59.375 [task-result-getter-0] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 383 ms on localhost (executor driver) (1/1)
23:54:59.377 [task-result-getter-0] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
23:54:59.381 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ResultStage 0 (csv at SparkInit.java:35) finished in 0.489 s
23:54:59.385 [main] INFO  o.a.spark.scheduler.DAGScheduler - Job 0 finished: csv at SparkInit.java:35, took 0.537744 s
23:54:59.445 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Pruning directories with: 
23:54:59.445 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Post-Scan Filters: 
23:54:59.446 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Output Data Schema: struct<value: string>
23:54:59.446 [main] INFO  o.a.s.s.execution.FileSourceScanExec - Pushed Filters: 
23:54:59.457 [main] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 7.037326 ms
23:54:59.481 [main] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 221.5 KB, free 1929.4 MB)
23:54:59.490 [main] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1929.4 MB)
23:54:59.491 [dispatcher-event-loop-2] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.1.67:32945 (size: 20.7 KB, free: 1929.9 MB)
23:54:59.492 [main] INFO  org.apache.spark.SparkContext - Created broadcast 2 from csv at SparkInit.java:35
23:54:59.492 [main] INFO  o.a.s.s.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23:54:59.597 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Pruning directories with: 
23:54:59.598 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Post-Scan Filters: 
23:54:59.598 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Output Data Schema: struct<>
23:54:59.598 [main] INFO  o.a.s.s.execution.FileSourceScanExec - Pushed Filters: 
23:54:59.646 [main] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 11.051324 ms
23:54:59.661 [main] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 9.90451 ms
23:54:59.677 [main] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 221.5 KB, free 1929.2 MB)
23:54:59.684 [main] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1929.2 MB)
23:54:59.685 [dispatcher-event-loop-3] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.1.67:32945 (size: 20.7 KB, free: 1929.8 MB)
23:54:59.686 [main] INFO  org.apache.spark.SparkContext - Created broadcast 3 from count at SparkInit.java:36
23:54:59.688 [main] INFO  o.a.s.s.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23:54:59.727 [main] INFO  org.apache.spark.SparkContext - Starting job: count at SparkInit.java:36
23:54:59.730 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Registering RDD 11 (count at SparkInit.java:36)
23:54:59.732 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Got job 1 (count at SparkInit.java:36) with 1 output partitions
23:54:59.732 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (count at SparkInit.java:36)
23:54:59.733 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 1)
23:54:59.734 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 1)
23:54:59.735 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[11] at count at SparkInit.java:36), which has no missing parents
23:54:59.753 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 13.6 KB, free 1929.2 MB)
23:54:59.755 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.1 KB, free 1929.2 MB)
23:54:59.756 [dispatcher-event-loop-4] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.1.67:32945 (size: 7.1 KB, free: 1929.8 MB)
23:54:59.757 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1161
23:54:59.759 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[11] at count at SparkInit.java:36) (first 15 tasks are for partitions Vector(0))
23:54:59.759 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
23:54:59.761 [dispatcher-event-loop-5] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7836 bytes)
23:54:59.762 [Executor task launch worker for task 1] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
23:54:59.785 [Executor task launch worker for task 1] INFO  o.a.s.s.e.datasources.FileScanRDD - Reading File path: file:///home/fabrice/workspace/sparkexamples/gradle.template/build/resources/main/test.csv, range: 0-121306, partition values: [empty row]
23:54:59.795 [Executor task launch worker for task 1] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.879161 ms
23:54:59.850 [Executor task launch worker for task 1] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 1598 bytes result sent to driver
23:54:59.852 [task-result-getter-1] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 92 ms on localhost (executor driver) (1/1)
23:54:59.852 [task-result-getter-1] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
23:54:59.853 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (count at SparkInit.java:36) finished in 0.115 s
23:54:59.854 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
23:54:59.854 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - running: Set()
23:54:59.854 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
23:54:59.855 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - failed: Set()
23:54:59.858 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[14] at count at SparkInit.java:36), which has no missing parents
23:54:59.866 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 8.5 KB, free 1929.1 MB)
23:54:59.868 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.4 KB, free 1929.1 MB)
23:54:59.868 [dispatcher-event-loop-0] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.1.67:32945 (size: 4.4 KB, free: 1929.8 MB)
23:54:59.869 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1161
23:54:59.870 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at count at SparkInit.java:36) (first 15 tasks are for partitions Vector(0))
23:54:59.870 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
23:54:59.873 [dispatcher-event-loop-1] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7246 bytes)
23:54:59.874 [Executor task launch worker for task 2] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
23:54:59.893 [Executor task launch worker for task 2] INFO  o.a.s.s.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
23:54:59.895 [Executor task launch worker for task 2] INFO  o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
23:54:59.915 [Executor task launch worker for task 2] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1792 bytes result sent to driver
23:54:59.916 [task-result-getter-2] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 43 ms on localhost (executor driver) (1/1)
23:54:59.916 [task-result-getter-2] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
23:54:59.917 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ResultStage 2 (count at SparkInit.java:36) finished in 0.054 s
23:54:59.918 [main] INFO  o.a.spark.scheduler.DAGScheduler - Job 1 finished: count at SparkInit.java:36, took 0.190441 s
23:54:59.921 [main] INFO  com.spark.examples.gradle.SparkInit - record load fom csv :999
23:54:59.961 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Pruning directories with: 
23:54:59.962 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Post-Scan Filters: 
23:54:59.962 [main] INFO  o.a.s.s.e.d.FileSourceStrategy - Output Data Schema: struct<1: string, Eldon Base for stackable storage shelf, platinum: string, Muhammed MacIntyre: string, 3: string, -213.25: string ... 8 more fields>
23:54:59.962 [main] INFO  o.a.s.s.execution.FileSourceScanExec - Pushed Filters: 
23:55:00.012 [main] INFO  o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23:55:00.031 [main] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 221.5 KB, free 1928.9 MB)
23:55:00.038 [main] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1928.9 MB)
23:55:00.039 [dispatcher-event-loop-4] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.1.67:32945 (size: 20.7 KB, free: 1929.8 MB)
23:55:00.039 [main] INFO  org.apache.spark.SparkContext - Created broadcast 6 from json at SparkInit.java:37
23:55:00.040 [main] INFO  o.a.s.s.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23:55:00.064 [main] INFO  org.apache.spark.SparkContext - Starting job: json at SparkInit.java:37
23:55:00.065 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Got job 2 (json at SparkInit.java:37) with 1 output partitions
23:55:00.065 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (json at SparkInit.java:37)
23:55:00.065 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
23:55:00.066 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Missing parents: List()
23:55:00.066 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[16] at json at SparkInit.java:37), which has no missing parents
23:55:00.079 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 136.2 KB, free 1928.8 MB)
23:55:00.081 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 49.6 KB, free 1928.7 MB)
23:55:00.081 [dispatcher-event-loop-5] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.1.67:32945 (size: 49.6 KB, free: 1929.8 MB)
23:55:00.082 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1161
23:55:00.082 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[16] at json at SparkInit.java:37) (first 15 tasks are for partitions Vector(0))
23:55:00.082 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
23:55:00.083 [dispatcher-event-loop-6] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7847 bytes)
23:55:00.084 [Executor task launch worker for task 3] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
23:55:00.127 [Executor task launch worker for task 3] INFO  o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23:55:00.148 [Executor task launch worker for task 3] INFO  o.a.s.s.e.datasources.FileScanRDD - Reading File path: file:///home/fabrice/workspace/sparkexamples/gradle.template/build/resources/main/test.csv, range: 0-121306, partition values: [empty row]
23:55:00.173 [Executor task launch worker for task 3] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 19.916604 ms
23:55:00.265 [Executor task launch worker for task 3] INFO  o.a.h.m.l.output.FileOutputCommitter - Saved output of task 'attempt_20190312235500_0003_m_000000_0' to file:/home/fabrice/Documents/jsonOut/_temporary/0/task_20190312235500_0003_m_000000
23:55:00.265 [Executor task launch worker for task 3] INFO  o.a.s.mapred.SparkHadoopMapRedUtil - attempt_20190312235500_0003_m_000000_0: Committed
23:55:00.268 [Executor task launch worker for task 3] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2169 bytes result sent to driver
23:55:00.270 [task-result-getter-3] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 187 ms on localhost (executor driver) (1/1)
23:55:00.270 [task-result-getter-3] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
23:55:00.271 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ResultStage 3 (json at SparkInit.java:37) finished in 0.204 s
23:55:00.271 [main] INFO  o.a.spark.scheduler.DAGScheduler - Job 2 finished: json at SparkInit.java:37, took 0.207231 s
23:55:00.280 [main] INFO  o.a.s.s.e.d.FileFormatWriter - Write Job 5a66cef2-770e-4d9b-9f55-19189e1ffc6c committed.
23:55:00.282 [main] INFO  o.a.s.s.e.d.FileFormatWriter - Finished processing stats for write job 5a66cef2-770e-4d9b-9f55-19189e1ffc6c.
23:55:00.550 [main] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 8.514053 ms
23:55:00.561 [main] INFO  org.apache.spark.SparkContext - Starting job: count at SparkInit.java:47
23:55:00.562 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Registering RDD 20 (count at SparkInit.java:47)
23:55:00.562 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Got job 3 (count at SparkInit.java:47) with 1 output partitions
23:55:00.563 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (count at SparkInit.java:47)
23:55:00.563 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
23:55:00.563 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 4)
23:55:00.564 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[20] at count at SparkInit.java:47), which has no missing parents
23:55:00.570 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 10.5 KB, free 1928.7 MB)
23:55:00.587 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.5 KB, free 1928.7 MB)
23:55:00.587 [dispatcher-event-loop-2] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 192.168.1.67:32945 (size: 5.5 KB, free: 1929.7 MB)
23:55:00.588 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1161
23:55:00.589 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[20] at count at SparkInit.java:47) (first 15 tasks are for partitions Vector(0))
23:55:00.589 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
23:55:00.590 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 6
23:55:00.590 [dispatcher-event-loop-3] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 7158 bytes)
23:55:00.590 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 85
23:55:00.591 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 13
23:55:00.591 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 99
23:55:00.591 [Executor task launch worker for task 4] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
23:55:00.591 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 138
23:55:00.591 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 52
23:55:00.591 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 117
23:55:00.591 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 72
23:55:00.592 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 35
23:55:00.605 [dispatcher-event-loop-7] INFO  o.a.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.1.67:32945 in memory (size: 20.7 KB, free: 1929.8 MB)
23:55:00.613 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 55
23:55:00.613 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 16
23:55:00.613 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 18
23:55:00.613 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 100
23:55:00.613 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 49
23:55:00.613 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 63
23:55:00.613 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 26
23:55:00.613 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 12
23:55:00.617 [dispatcher-event-loop-2] INFO  o.a.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.1.67:32945 in memory (size: 20.7 KB, free: 1929.8 MB)
23:55:00.621 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 118
23:55:00.621 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 131
23:55:00.621 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 132
23:55:00.621 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 61
23:55:00.621 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 11
23:55:00.621 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 135
23:55:00.624 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned shuffle 0
23:55:00.624 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 94
23:55:00.625 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 123
23:55:00.625 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 80
23:55:00.625 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 66
23:55:00.625 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 33
23:55:00.625 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 27
23:55:00.625 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 4
23:55:00.625 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 128
23:55:00.625 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 69
23:55:00.629 [dispatcher-event-loop-7] INFO  o.a.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.1.67:32945 in memory (size: 5.0 KB, free: 1929.8 MB)
23:55:00.637 [Executor task launch worker for task 4] INFO  o.a.s.s.e.datasources.jdbc.JDBCRDD - closed connection
23:55:00.638 [Executor task launch worker for task 4] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 1512 bytes result sent to driver
23:55:00.641 [task-result-getter-0] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 51 ms on localhost (executor driver) (1/1)
23:55:00.641 [task-result-getter-0] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
23:55:00.641 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 67
23:55:00.642 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (count at SparkInit.java:47) finished in 0.076 s
23:55:00.642 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
23:55:00.642 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - running: Set()
23:55:00.642 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5)
23:55:00.642 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - failed: Set()
23:55:00.643 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[23] at count at SparkInit.java:47), which has no missing parents
23:55:00.643 [dispatcher-event-loop-3] INFO  o.a.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.1.67:32945 in memory (size: 7.1 KB, free: 1929.8 MB)
23:55:00.645 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 8.5 KB, free 1929.2 MB)
23:55:00.646 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.4 KB, free 1929.2 MB)
23:55:00.647 [dispatcher-event-loop-4] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 192.168.1.67:32945 (size: 4.4 KB, free: 1929.8 MB)
23:55:00.647 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1161
23:55:00.648 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at count at SparkInit.java:47) (first 15 tasks are for partitions Vector(0))
23:55:00.648 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
23:55:00.649 [dispatcher-event-loop-5] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 7246 bytes)
23:55:00.649 [Executor task launch worker for task 5] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
23:55:00.652 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 101
23:55:00.652 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 39
23:55:00.652 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 34
23:55:00.652 [Executor task launch worker for task 5] INFO  o.a.s.s.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
23:55:00.652 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 103
23:55:00.652 [Executor task launch worker for task 5] INFO  o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
23:55:00.652 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 29
23:55:00.652 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 40
23:55:00.652 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 2
23:55:00.653 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 121
23:55:00.653 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 64
23:55:00.653 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 77
23:55:00.653 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 84
23:55:00.653 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 90
23:55:00.653 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 41
23:55:00.653 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 122
23:55:00.653 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 1
23:55:00.653 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 87
23:55:00.653 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 20
23:55:00.653 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 3
23:55:00.653 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 88
23:55:00.653 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 126
23:55:00.653 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 115
23:55:00.653 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 19
23:55:00.654 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 127
23:55:00.654 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 21
23:55:00.654 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 57
23:55:00.654 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 104
23:55:00.654 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 7
23:55:00.654 [Executor task launch worker for task 5] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 5). 1749 bytes result sent to driver
23:55:00.654 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 120
23:55:00.654 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 42
23:55:00.654 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 71
23:55:00.654 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 28
23:55:00.654 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 83
23:55:00.654 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 113
23:55:00.654 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 119
23:55:00.654 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 31
23:55:00.654 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 75
23:55:00.654 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 37
23:55:00.655 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 73
23:55:00.655 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 78
23:55:00.655 [task-result-getter-1] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 5) in 6 ms on localhost (executor driver) (1/1)
23:55:00.655 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 98
23:55:00.655 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 5
23:55:00.655 [task-result-getter-1] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
23:55:00.655 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 130
23:55:00.655 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 9
23:55:00.655 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 17
23:55:00.655 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 60
23:55:00.656 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ResultStage 5 (count at SparkInit.java:47) finished in 0.011 s
23:55:00.656 [main] INFO  o.a.spark.scheduler.DAGScheduler - Job 3 finished: count at SparkInit.java:47, took 0.095057 s
23:55:00.657 [main] INFO  com.spark.examples.gradle.SparkInit - record load fom jdbc : 4
23:55:00.657 [dispatcher-event-loop-2] INFO  o.a.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 192.168.1.67:32945 in memory (size: 49.6 KB, free: 1929.8 MB)
23:55:00.657 [main] INFO  com.spark.examples.gradle.SparkInit - output csv fom jdbc : /home/fabrice/Documents/csvOut
23:55:00.667 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 105
23:55:00.667 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 82
23:55:00.667 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 50
23:55:00.667 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 129
23:55:00.667 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 92
23:55:00.667 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 46
23:55:00.667 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 15
23:55:00.668 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 32
23:55:00.668 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 110
23:55:00.668 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 124
23:55:00.668 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 53
23:55:00.668 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 45
23:55:00.668 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 109
23:55:00.668 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 76
23:55:00.668 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 102
23:55:00.668 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 86
23:55:00.669 [dispatcher-event-loop-5] INFO  o.a.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 192.168.1.67:32945 in memory (size: 20.7 KB, free: 1929.9 MB)
23:55:00.672 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 22
23:55:00.672 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 65
23:55:00.672 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 95
23:55:00.672 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 10
23:55:00.672 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 106
23:55:00.676 [dispatcher-event-loop-0] INFO  o.a.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 192.168.1.67:32945 in memory (size: 4.4 KB, free: 1929.9 MB)
23:55:00.684 [dispatcher-event-loop-3] INFO  o.a.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 192.168.1.67:32945 in memory (size: 20.7 KB, free: 1929.9 MB)
23:55:00.689 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 47
23:55:00.689 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 125
23:55:00.689 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 70
23:55:00.689 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 97
23:55:00.689 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 43
23:55:00.689 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 23
23:55:00.689 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 51
23:55:00.689 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 114
23:55:00.689 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 24
23:55:00.689 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 74
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 91
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 111
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 79
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 68
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 54
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 62
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 89
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 108
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 96
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 112
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 0
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 58
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 81
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 59
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 44
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 136
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 14
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 30
23:55:00.690 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 116
23:55:00.691 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 56
23:55:00.691 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 107
23:55:00.691 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 36
23:55:00.691 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 133
23:55:00.691 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 8
23:55:00.691 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 137
23:55:00.691 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 38
23:55:00.691 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 48
23:55:00.691 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 93
23:55:00.691 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 134
23:55:00.691 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 25
23:55:00.707 [main] INFO  o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23:55:00.729 [main] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 15.733843 ms
23:55:00.744 [main] INFO  org.apache.spark.SparkContext - Starting job: csv at SparkInit.java:50
23:55:00.745 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Got job 4 (csv at SparkInit.java:50) with 1 output partitions
23:55:00.745 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (csv at SparkInit.java:50)
23:55:00.745 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
23:55:00.746 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Missing parents: List()
23:55:00.746 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[25] at csv at SparkInit.java:50), which has no missing parents
23:55:00.762 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 139.5 KB, free 1929.7 MB)
23:55:00.764 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 50.5 KB, free 1929.7 MB)
23:55:00.765 [dispatcher-event-loop-4] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 192.168.1.67:32945 (size: 50.5 KB, free: 1929.8 MB)
23:55:00.765 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1161
23:55:00.766 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[25] at csv at SparkInit.java:50) (first 15 tasks are for partitions Vector(0))
23:55:00.766 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks
23:55:00.767 [dispatcher-event-loop-5] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7169 bytes)
23:55:00.767 [Executor task launch worker for task 6] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 6)
23:55:00.792 [Executor task launch worker for task 6] INFO  o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23:55:00.817 [Executor task launch worker for task 6] INFO  o.a.s.s.e.datasources.jdbc.JDBCRDD - closed connection
23:55:00.819 [Executor task launch worker for task 6] INFO  o.a.h.m.l.output.FileOutputCommitter - Saved output of task 'attempt_20190312235500_0006_m_000000_0' to file:/home/fabrice/Documents/csvOut/_temporary/0/task_20190312235500_0006_m_000000
23:55:00.819 [Executor task launch worker for task 6] INFO  o.a.s.mapred.SparkHadoopMapRedUtil - attempt_20190312235500_0006_m_000000_0: Committed
23:55:00.820 [Executor task launch worker for task 6] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 6). 2126 bytes result sent to driver
23:55:00.821 [task-result-getter-2] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 6) in 54 ms on localhost (executor driver) (1/1)
23:55:00.822 [task-result-getter-2] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
23:55:00.822 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ResultStage 6 (csv at SparkInit.java:50) finished in 0.075 s
23:55:00.823 [main] INFO  o.a.spark.scheduler.DAGScheduler - Job 4 finished: csv at SparkInit.java:50, took 0.078238 s
23:55:00.837 [main] INFO  o.a.s.s.e.d.FileFormatWriter - Write Job a2e2967e-cbac-4d04-959f-d3e7bffc0614 committed.
23:55:00.838 [main] INFO  o.a.s.s.e.d.FileFormatWriter - Finished processing stats for write job a2e2967e-cbac-4d04-959f-d3e7bffc0614.
23:55:00.855 [Thread-2] INFO  org.apache.spark.SparkContext - Invoking stop() from shutdown hook
23:55:00.859 [Thread-2] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@1bbfc27d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
23:55:00.860 [Thread-2] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.67:4040
23:55:00.870 [dispatcher-event-loop-3] INFO  o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
23:55:00.879 [Thread-2] INFO  o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
23:55:00.880 [Thread-2] INFO  o.apache.spark.storage.BlockManager - BlockManager stopped
23:55:00.882 [Thread-2] INFO  o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
23:55:00.884 [dispatcher-event-loop-0] INFO  o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
23:55:00.899 [Thread-2] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext
23:55:00.900 [Thread-2] INFO  o.a.spark.util.ShutdownHookManager - Shutdown hook called
23:55:00.900 [Thread-2] INFO  o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-2fc37648-954c-4f77-bbdc-e7f07a6a7e6d
